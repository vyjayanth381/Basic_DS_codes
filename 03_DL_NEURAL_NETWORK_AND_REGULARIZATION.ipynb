{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9oVfDdhiJM7"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43XlK_TmfNaT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "ID4w6cb8iZdw",
    "outputId": "5ec81e73-0f03-45f9-948e-54dc7514d407"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttrFkUKgik-6"
   },
   "outputs": [],
   "source": [
    "def random_regression_data(n):\n",
    "    # n = number of samples\n",
    "    np.random.seed(n)\n",
    "    x = np.linspace(-3.14,6.28,n)\n",
    "    y = np.sin(x) + np.random.random(n) * 1.5\n",
    "    return x,y\n",
    "\n",
    "X,y = random_regression_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "pRpR30v1ndMf",
    "outputId": "20a3f8ac-b002-4556-b2f2-fb070c1a004f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (80,), (20,), (20,))"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ClN_MHlQjG9l",
    "outputId": "26ec5ccd-c7a7-4700-997c-6ad0fede0eef"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGSKkAQAAZIiQBgAAkKH/Dz8sjsWWLvJPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotting():\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(x_train,y_train)\n",
    "    plt.scatter(x_test,y_test)\n",
    "    plt.xlabel('X'),plt.ylabel('y')\n",
    "    plt.title('Scatter Plot')\n",
    "plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCDaCF2Yjw95"
   },
   "source": [
    "# Base Line Model (1 estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mysf5kTjmRg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "768WsPeSktxk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0W3n4jdAkJmV",
    "outputId": "f2176dd9-1003-4b8f-ea84-e43f7b929ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 0s 1ms/sample - loss: 17.8150 - val_loss: 24.0196\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 17.7299 - val_loss: 23.8973\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 17.6394 - val_loss: 23.7760\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 17.5542 - val_loss: 23.6551\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 17.4712 - val_loss: 23.5363\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 17.3839 - val_loss: 23.4195\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 17.2950 - val_loss: 23.3028\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 17.2088 - val_loss: 23.1853\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 17.1257 - val_loss: 23.0668\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 17.0421 - val_loss: 22.9484\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 16.9552 - val_loss: 22.8305\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 16.8720 - val_loss: 22.7131\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 16.7848 - val_loss: 22.5967\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 16.6966 - val_loss: 22.4809\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 16.6134 - val_loss: 22.3637\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 79us/sample - loss: 16.5304 - val_loss: 22.2462\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 16.4466 - val_loss: 22.1295\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 16.3631 - val_loss: 22.0137\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 16.2814 - val_loss: 21.8988\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 78us/sample - loss: 16.2006 - val_loss: 21.7849\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 16.1158 - val_loss: 21.6727\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 16.0368 - val_loss: 21.5605\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 15.9510 - val_loss: 21.4499\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 15.8754 - val_loss: 21.3371\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 15.7947 - val_loss: 21.2254\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 15.7102 - val_loss: 21.1153\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 15.6274 - val_loss: 21.0058\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 15.5549 - val_loss: 20.8946\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 15.4740 - val_loss: 20.7853\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 15.3976 - val_loss: 20.6763\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 15.3179 - val_loss: 20.5695\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 15.2400 - val_loss: 20.4638\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 15.1638 - val_loss: 20.3585\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 15.0903 - val_loss: 20.2536\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 15.0089 - val_loss: 20.1517\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 14.9383 - val_loss: 20.0482\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 14.8641 - val_loss: 19.9446\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 14.7901 - val_loss: 19.8414\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 14.7150 - val_loss: 19.7390\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 14.6405 - val_loss: 19.6371\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 14.5703 - val_loss: 19.5343\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 78us/sample - loss: 14.4946 - val_loss: 19.4338\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 14.4208 - val_loss: 19.3345\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 14.3479 - val_loss: 19.2359\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 14.2750 - val_loss: 19.1378\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 14.2050 - val_loss: 19.0383\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 14.1330 - val_loss: 18.9387\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 14.0599 - val_loss: 18.8390\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 13.9902 - val_loss: 18.7379\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 13.9181 - val_loss: 18.6374\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 13.8429 - val_loss: 18.5386\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 13.7696 - val_loss: 18.4390\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 13.7028 - val_loss: 18.3379\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 13.6285 - val_loss: 18.2398\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 13.5588 - val_loss: 18.1420\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 13.4857 - val_loss: 18.0454\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 13.4196 - val_loss: 17.9477\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 13.3455 - val_loss: 17.8518\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 13.2777 - val_loss: 17.7561\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 13.2080 - val_loss: 17.6618\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 13.1406 - val_loss: 17.5680\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 13.0741 - val_loss: 17.4742\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 13.0069 - val_loss: 17.3808\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 12.9370 - val_loss: 17.2884\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 12.8728 - val_loss: 17.1951\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 12.8014 - val_loss: 17.1037\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 12.7409 - val_loss: 17.0100\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 12.6690 - val_loss: 16.9187\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 12.6056 - val_loss: 16.8265\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 12.5390 - val_loss: 16.7351\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 12.4710 - val_loss: 16.6452\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 12.4082 - val_loss: 16.5554\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 12.3421 - val_loss: 16.4660\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 12.2761 - val_loss: 16.3769\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 12.2158 - val_loss: 16.2873\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 12.1492 - val_loss: 16.1994\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 12.0834 - val_loss: 16.1122\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 12.0209 - val_loss: 16.0245\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 11.9571 - val_loss: 15.9363\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 11.8916 - val_loss: 15.8480\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 11.8296 - val_loss: 15.7591\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 11.7676 - val_loss: 15.6705\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 11.7037 - val_loss: 15.5834\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 11.6392 - val_loss: 15.4973\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 11.5753 - val_loss: 15.4111\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 11.5170 - val_loss: 15.3238\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 11.4507 - val_loss: 15.2382\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 11.3875 - val_loss: 15.1521\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 11.3269 - val_loss: 15.0651\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 11.2663 - val_loss: 14.9790\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 11.2045 - val_loss: 14.8945\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 11.1401 - val_loss: 14.8117\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 11.0804 - val_loss: 14.7279\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 11.0198 - val_loss: 14.6441\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 10.9610 - val_loss: 14.5602\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 10.8999 - val_loss: 14.4774\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 10.8394 - val_loss: 14.3953\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 10.7798 - val_loss: 14.3139\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 10.7207 - val_loss: 14.2333\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 10.6593 - val_loss: 14.1537\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 10.6071 - val_loss: 14.0715\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 10.5465 - val_loss: 13.9908\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 10.4910 - val_loss: 13.9100\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 10.4318 - val_loss: 13.8317\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 10.3758 - val_loss: 13.7547\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 10.3194 - val_loss: 13.6784\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 10.2659 - val_loss: 13.6024\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 10.2089 - val_loss: 13.5281\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 10.1530 - val_loss: 13.4535\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 10.1014 - val_loss: 13.3769\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 10.0450 - val_loss: 13.3009\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 9.9878 - val_loss: 13.2252\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 9.9334 - val_loss: 13.1487\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 9.8788 - val_loss: 13.0721\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 9.8257 - val_loss: 12.9956\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 9.7697 - val_loss: 12.9208\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 9.7148 - val_loss: 12.8471\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 9.6627 - val_loss: 12.7729\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 9.6069 - val_loss: 12.7002\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 9.5564 - val_loss: 12.6267\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 9.5023 - val_loss: 12.5538\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 9.4504 - val_loss: 12.4811\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 9.3978 - val_loss: 12.4093\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 9.3455 - val_loss: 12.3382\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 9.2953 - val_loss: 12.2678\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 9.2402 - val_loss: 12.1991\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 9.1930 - val_loss: 12.1287\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 9.1431 - val_loss: 12.0590\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 9.0911 - val_loss: 11.9903\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 9.0416 - val_loss: 11.9209\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 8.9910 - val_loss: 11.8516\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 8.9417 - val_loss: 11.7822\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 8.8924 - val_loss: 11.7133\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 8.8415 - val_loss: 11.6453\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 8.7952 - val_loss: 11.5770\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 8.7441 - val_loss: 11.5109\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 8.6942 - val_loss: 11.4457\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 8.6466 - val_loss: 11.3797\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 8.6000 - val_loss: 11.3130\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 8.5530 - val_loss: 11.2466\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 8.5054 - val_loss: 11.1812\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 8.4578 - val_loss: 11.1166\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 8.4098 - val_loss: 11.0524\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 8.3610 - val_loss: 10.9878\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 8.3189 - val_loss: 10.9215\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 8.2694 - val_loss: 10.8570\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 8.2233 - val_loss: 10.7926\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 8.1738 - val_loss: 10.7295\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 8.1296 - val_loss: 10.6657\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 8.0838 - val_loss: 10.6023\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 8.0372 - val_loss: 10.5395\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 7.9912 - val_loss: 10.4771\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 7.9459 - val_loss: 10.4147\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 7.9001 - val_loss: 10.3525\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 7.8551 - val_loss: 10.2904\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 7.8102 - val_loss: 10.2283\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 7.7671 - val_loss: 10.1662\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 7.7212 - val_loss: 10.1055\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 7.6775 - val_loss: 10.0447\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 7.6324 - val_loss: 9.9836\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 7.5878 - val_loss: 9.9224\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 7.5426 - val_loss: 9.8619\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 7.4988 - val_loss: 9.8011\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 7.4570 - val_loss: 9.7399\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 7.4110 - val_loss: 9.6802\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 7.3666 - val_loss: 9.6209\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 7.3292 - val_loss: 9.5601\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 7.2805 - val_loss: 9.5022\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 7.2379 - val_loss: 9.4442\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 7.1961 - val_loss: 9.3858\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 7.1527 - val_loss: 9.3274\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 7.1136 - val_loss: 9.2679\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 7.0680 - val_loss: 9.2104\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 7.0293 - val_loss: 9.1524\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 6.9858 - val_loss: 9.0960\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 6.9462 - val_loss: 9.0394\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 6.9048 - val_loss: 8.9841\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 6.8648 - val_loss: 8.9293\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 6.8237 - val_loss: 8.8758\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 6.7849 - val_loss: 8.8220\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 6.7464 - val_loss: 8.7680\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 6.7069 - val_loss: 8.7140\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 6.6676 - val_loss: 8.6600\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 6.6267 - val_loss: 8.6064\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 6.5902 - val_loss: 8.5520\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 6.5511 - val_loss: 8.4981\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 6.5121 - val_loss: 8.4447\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 6.4716 - val_loss: 8.3924\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 6.4348 - val_loss: 8.3398\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 6.3962 - val_loss: 8.2882\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 6.3609 - val_loss: 8.2366\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 6.3218 - val_loss: 8.1861\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 6.2845 - val_loss: 8.1357\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 6.2483 - val_loss: 8.0847\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 6.2134 - val_loss: 8.0331\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 6.1733 - val_loss: 7.9831\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 6.1375 - val_loss: 7.9332\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 6.1028 - val_loss: 7.8833\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 6.0655 - val_loss: 7.8346\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 6.0305 - val_loss: 7.7857\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 5.9966 - val_loss: 7.7370\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 5.9594 - val_loss: 7.6898\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 5.9262 - val_loss: 7.6427\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 5.8912 - val_loss: 7.5962\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 5.8570 - val_loss: 7.5497\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 5.8247 - val_loss: 7.5027\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 5.7908 - val_loss: 7.4564\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 5.7566 - val_loss: 7.4109\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 5.7244 - val_loss: 7.3653\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 5.6892 - val_loss: 7.3206\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 5.6593 - val_loss: 7.2748\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 5.6235 - val_loss: 7.2304\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 5.5920 - val_loss: 7.1855\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 5.5582 - val_loss: 7.1409\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 5.5286 - val_loss: 7.0954\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 5.4953 - val_loss: 7.0509\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 5.4631 - val_loss: 7.0070\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 5.4296 - val_loss: 6.9643\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 5.3985 - val_loss: 6.9216\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 5.3665 - val_loss: 6.8792\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 5.3366 - val_loss: 6.8360\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 5.3038 - val_loss: 6.7932\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 5.2740 - val_loss: 6.7501\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 5.2441 - val_loss: 6.7072\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 5.2119 - val_loss: 6.6653\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 5.1793 - val_loss: 6.6242\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 5.1509 - val_loss: 6.5821\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 5.1217 - val_loss: 6.5402\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 5.0895 - val_loss: 6.4991\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 5.0585 - val_loss: 6.4583\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 5.0302 - val_loss: 6.4174\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 5.0013 - val_loss: 6.3770\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 4.9717 - val_loss: 6.3370\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 4.9423 - val_loss: 6.2977\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 4.9140 - val_loss: 6.2586\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 4.8845 - val_loss: 6.2200\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 4.8568 - val_loss: 6.1810\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 4.8289 - val_loss: 6.1422\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 4.8016 - val_loss: 6.1034\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 4.7722 - val_loss: 6.0656\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 4.7444 - val_loss: 6.0278\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 4.7166 - val_loss: 5.9902\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 4.6895 - val_loss: 5.9526\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 4.6622 - val_loss: 5.9150\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 4.6353 - val_loss: 5.8773\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 4.6076 - val_loss: 5.8401\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 4.5814 - val_loss: 5.8031\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 4.5527 - val_loss: 5.7672\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 4.5261 - val_loss: 5.7306\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 4.5004 - val_loss: 5.6933\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 4.4721 - val_loss: 5.6565\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 4.4465 - val_loss: 5.6194\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 4.4197 - val_loss: 5.5829\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 4.3942 - val_loss: 5.5469\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 4.3671 - val_loss: 5.5118\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 4.3411 - val_loss: 5.4770\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 4.3152 - val_loss: 5.4423\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 4.2905 - val_loss: 5.4076\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 4.2650 - val_loss: 5.3735\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 4.2382 - val_loss: 5.3398\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 4.2160 - val_loss: 5.3050\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 4.1903 - val_loss: 5.2709\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 4.1644 - val_loss: 5.2369\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 4.1399 - val_loss: 5.2027\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 4.1155 - val_loss: 5.1686\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 4.0898 - val_loss: 5.1350\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 4.0657 - val_loss: 5.1013\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 4.0404 - val_loss: 5.0678\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 4.0170 - val_loss: 5.0340\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 3.9911 - val_loss: 5.0011\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 3.9684 - val_loss: 4.9678\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 3.9430 - val_loss: 4.9354\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 3.9206 - val_loss: 4.9027\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 3.8969 - val_loss: 4.8708\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 3.8732 - val_loss: 4.8393\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 3.8507 - val_loss: 4.8080\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 3.8276 - val_loss: 4.7770\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 3.8044 - val_loss: 4.7464\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 3.7814 - val_loss: 4.7159\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 3.7597 - val_loss: 4.6849\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 3.7376 - val_loss: 4.6540\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 3.7150 - val_loss: 4.6234\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 3.6921 - val_loss: 4.5933\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 3.6709 - val_loss: 4.5631\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 3.6474 - val_loss: 4.5337\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 3.6258 - val_loss: 4.5039\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 3.6042 - val_loss: 4.4739\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 3.5816 - val_loss: 4.4440\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 3.5607 - val_loss: 4.4142\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 3.5388 - val_loss: 4.3849\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 3.5178 - val_loss: 4.3561\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 3.4968 - val_loss: 4.3275\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 3.4746 - val_loss: 4.2992\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 3.4547 - val_loss: 4.2705\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 3.4331 - val_loss: 4.2424\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 76us/sample - loss: 3.4132 - val_loss: 4.2141\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 79us/sample - loss: 3.3919 - val_loss: 4.1866\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 3.3725 - val_loss: 4.1590\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 3.3503 - val_loss: 4.1318\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 3.3317 - val_loss: 4.1035\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 3.3114 - val_loss: 4.0755\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 3.2908 - val_loss: 4.0480\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 3.2702 - val_loss: 4.0210\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 3.2505 - val_loss: 3.9940\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 3.2317 - val_loss: 3.9668\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 3.2112 - val_loss: 3.9405\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 3.1926 - val_loss: 3.9145\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 3.1733 - val_loss: 3.8887\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 3.1533 - val_loss: 3.8635\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 3.1356 - val_loss: 3.8380\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 3.1161 - val_loss: 3.8130\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 3.0987 - val_loss: 3.7877\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 3.0793 - val_loss: 3.7631\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 3.0625 - val_loss: 3.7381\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 3.0434 - val_loss: 3.7137\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 3.0256 - val_loss: 3.6898\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 3.0084 - val_loss: 3.6659\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 2.9907 - val_loss: 3.6423\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 2.9726 - val_loss: 3.6190\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 2.9566 - val_loss: 3.5951\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 2.9385 - val_loss: 3.5719\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 2.9219 - val_loss: 3.5487\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 2.9037 - val_loss: 3.5259\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 2.8878 - val_loss: 3.5027\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 2.8700 - val_loss: 3.4799\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 78us/sample - loss: 2.8530 - val_loss: 3.4570\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 2.8373 - val_loss: 3.4340\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 2.8198 - val_loss: 3.4115\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.8039 - val_loss: 3.3892\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 2.7862 - val_loss: 3.3674\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 2.7710 - val_loss: 3.3452\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 2.7554 - val_loss: 3.3230\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 2.7380 - val_loss: 3.3016\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 2.7226 - val_loss: 3.2800\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.7068 - val_loss: 3.2587\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.6906 - val_loss: 3.2375\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 2.6750 - val_loss: 3.2161\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 2.6597 - val_loss: 3.1946\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 2.6434 - val_loss: 3.1737\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 2.6284 - val_loss: 3.1527\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 2.6127 - val_loss: 3.1321\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.5979 - val_loss: 3.1117\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 2.5829 - val_loss: 3.0915\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 2.5674 - val_loss: 3.0715\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 2.5535 - val_loss: 3.0517\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 2.5388 - val_loss: 3.0323\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 2.5244 - val_loss: 3.0131\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 2.5099 - val_loss: 2.9940\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 2.4956 - val_loss: 2.9749\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 2.4817 - val_loss: 2.9556\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 2.4672 - val_loss: 2.9364\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 2.4532 - val_loss: 2.9171\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.4387 - val_loss: 2.8982\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 2.4250 - val_loss: 2.8794\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 2.4110 - val_loss: 2.8608\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 2.3970 - val_loss: 2.8424\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 2.3831 - val_loss: 2.8240\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 2.3700 - val_loss: 2.8054\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 2.3564 - val_loss: 2.7872\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 2.3428 - val_loss: 2.7693\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 2.3294 - val_loss: 2.7516\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 2.3163 - val_loss: 2.7341\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 2.3038 - val_loss: 2.7166\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 2.2905 - val_loss: 2.6996\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 2.2778 - val_loss: 2.6826\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 2.2651 - val_loss: 2.6655\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 2.2531 - val_loss: 2.6484\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 2.2405 - val_loss: 2.6317\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 2.2270 - val_loss: 2.6154\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 2.2158 - val_loss: 2.5987\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 2.2026 - val_loss: 2.5823\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 2.1907 - val_loss: 2.5657\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 2.1779 - val_loss: 2.5493\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 2.1661 - val_loss: 2.5325\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 2.1538 - val_loss: 2.5160\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 2.1419 - val_loss: 2.4997\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 2.1292 - val_loss: 2.4838\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 2.1179 - val_loss: 2.4678\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 2.1063 - val_loss: 2.4521\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 2.0943 - val_loss: 2.4368\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 2.0829 - val_loss: 2.4216\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 2.0720 - val_loss: 2.4066\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 2.0602 - val_loss: 2.3920\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 75us/sample - loss: 2.0495 - val_loss: 2.3773\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 2.0383 - val_loss: 2.3627\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 2.0272 - val_loss: 2.3482\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 2.0171 - val_loss: 2.3334\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 2.0052 - val_loss: 2.3190\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.9954 - val_loss: 2.3043\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 1.9845 - val_loss: 2.2900\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.9731 - val_loss: 2.2761\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.9634 - val_loss: 2.2620\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 1.9524 - val_loss: 2.2482\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.9425 - val_loss: 2.2343\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.9322 - val_loss: 2.2206\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.9217 - val_loss: 2.2071\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.9117 - val_loss: 2.1938\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.9014 - val_loss: 2.1807\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 1.8916 - val_loss: 2.1675\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 1.8818 - val_loss: 2.1544\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 1.8723 - val_loss: 2.1413\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.8621 - val_loss: 2.1284\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 1.8527 - val_loss: 2.1155\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.8430 - val_loss: 2.1027\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 1.8332 - val_loss: 2.0901\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.8239 - val_loss: 2.0775\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 1.8137 - val_loss: 2.0650\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.8054 - val_loss: 2.0521\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.7957 - val_loss: 2.0395\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 1.7864 - val_loss: 2.0272\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.7766 - val_loss: 2.0153\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 1.7675 - val_loss: 2.0033\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 1.7587 - val_loss: 1.9912\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.7498 - val_loss: 1.9792\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 1.7409 - val_loss: 1.9675\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 1.7320 - val_loss: 1.9561\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.7237 - val_loss: 1.9446\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.7150 - val_loss: 1.9332\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 1.7066 - val_loss: 1.9220\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 1.6978 - val_loss: 1.9109\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 1.6889 - val_loss: 1.9000\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 1.6808 - val_loss: 1.8888\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 1.6721 - val_loss: 1.8776\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 1.6640 - val_loss: 1.8662\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 1.6559 - val_loss: 1.8550\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 1.6472 - val_loss: 1.8442\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 1.6392 - val_loss: 1.8334\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.6308 - val_loss: 1.8228\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 1.6232 - val_loss: 1.8122\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 1.6150 - val_loss: 1.8018\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 1.6071 - val_loss: 1.7915\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.5995 - val_loss: 1.7813\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 1.5915 - val_loss: 1.7714\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.5838 - val_loss: 1.7614\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 1.5763 - val_loss: 1.7514\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 1.5690 - val_loss: 1.7413\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.5616 - val_loss: 1.7313\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 1.5540 - val_loss: 1.7215\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 1.5461 - val_loss: 1.7120\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 1.5389 - val_loss: 1.7023\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 1.5317 - val_loss: 1.6927\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 1.5245 - val_loss: 1.6832\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 1.5170 - val_loss: 1.6739\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.5098 - val_loss: 1.6646\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.5030 - val_loss: 1.6551\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 1.4959 - val_loss: 1.6457\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 1.4891 - val_loss: 1.6366\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 1.4818 - val_loss: 1.6278\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 1.4752 - val_loss: 1.6190\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.4685 - val_loss: 1.6103\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 1.4618 - val_loss: 1.6017\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 1.4555 - val_loss: 1.5932\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 1.4485 - val_loss: 1.5849\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 1.4423 - val_loss: 1.5764\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 1.4358 - val_loss: 1.5680\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 1.4294 - val_loss: 1.5597\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.4232 - val_loss: 1.5514\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 1.4169 - val_loss: 1.5432\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 1.4105 - val_loss: 1.5352\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 1.4048 - val_loss: 1.5272\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 1.3983 - val_loss: 1.5196\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 1.3924 - val_loss: 1.5120\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.3868 - val_loss: 1.5043\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.3806 - val_loss: 1.4968\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.3748 - val_loss: 1.4894\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.3691 - val_loss: 1.4819\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 1.3635 - val_loss: 1.4745\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 1.3577 - val_loss: 1.4670\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 1.3520 - val_loss: 1.4596\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.3460 - val_loss: 1.4523\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.3407 - val_loss: 1.4449\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 1.3350 - val_loss: 1.4376\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.3295 - val_loss: 1.4305\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.3238 - val_loss: 1.4236\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 1.3184 - val_loss: 1.4167\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.3130 - val_loss: 1.4098\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.3080 - val_loss: 1.4029\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 1.3022 - val_loss: 1.3962\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.2972 - val_loss: 1.3893\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 1.2918 - val_loss: 1.3824\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.2864 - val_loss: 1.3755\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.2812 - val_loss: 1.3686\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 1.2759 - val_loss: 1.3620\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.2708 - val_loss: 1.3553\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 1.2656 - val_loss: 1.3489\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.2606 - val_loss: 1.3425\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.2555 - val_loss: 1.3363\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.2508 - val_loss: 1.3299\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.2460 - val_loss: 1.3236\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 1.2410 - val_loss: 1.3175\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 1.2360 - val_loss: 1.3115\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.2315 - val_loss: 1.3053\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 1.2267 - val_loss: 1.2993\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 1.2220 - val_loss: 1.2933\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 1.2174 - val_loss: 1.2874\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.2129 - val_loss: 1.2815\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.2084 - val_loss: 1.2757\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 1.2038 - val_loss: 1.2700\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.1992 - val_loss: 1.2644\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.1948 - val_loss: 1.2589\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 1.1907 - val_loss: 1.2534\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.1862 - val_loss: 1.2480\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.1818 - val_loss: 1.2426\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 1.1777 - val_loss: 1.2372\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 1.1733 - val_loss: 1.2318\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 1.1689 - val_loss: 1.2265\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.1650 - val_loss: 1.2211\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.1609 - val_loss: 1.2157\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 1.1563 - val_loss: 1.2105\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 1.1522 - val_loss: 1.2053\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 1.1483 - val_loss: 1.2002\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.1441 - val_loss: 1.1952\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 1.1400 - val_loss: 1.1902\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.1362 - val_loss: 1.1852\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.1323 - val_loss: 1.1801\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 1.1282 - val_loss: 1.1752\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 1.1243 - val_loss: 1.1703\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 1.1206 - val_loss: 1.1654\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 1.1166 - val_loss: 1.1606\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 1.1130 - val_loss: 1.1558\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.1091 - val_loss: 1.1511\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.1053 - val_loss: 1.1465\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 1.1017 - val_loss: 1.1420\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 1.0982 - val_loss: 1.1376\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 1.0945 - val_loss: 1.1333\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.0908 - val_loss: 1.1289\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 1.0874 - val_loss: 1.1244\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 1.0840 - val_loss: 1.1199\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 1.0802 - val_loss: 1.1154\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 1.0768 - val_loss: 1.1110\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 1.0734 - val_loss: 1.1067\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 1.0699 - val_loss: 1.1025\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 1.0663 - val_loss: 1.0984\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 1.0631 - val_loss: 1.0942\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 1.0595 - val_loss: 1.0900\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.0563 - val_loss: 1.0858\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.0529 - val_loss: 1.0816\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 1.0494 - val_loss: 1.0775\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 1.0464 - val_loss: 1.0734\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 1.0430 - val_loss: 1.0695\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.0399 - val_loss: 1.0656\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 1.0367 - val_loss: 1.0618\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 1.0336 - val_loss: 1.0580\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 1.0304 - val_loss: 1.0543\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 1.0276 - val_loss: 1.0506\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 1.0245 - val_loss: 1.0469\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 1.0216 - val_loss: 1.0432\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 1.0186 - val_loss: 1.0397\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 1.0156 - val_loss: 1.0362\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 1.0130 - val_loss: 1.0326\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 1.0099 - val_loss: 1.0292\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 1.0071 - val_loss: 1.0257\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 1.0043 - val_loss: 1.0223\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 1.0013 - val_loss: 1.0189\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.9987 - val_loss: 1.0154\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.9959 - val_loss: 1.0120\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.9931 - val_loss: 1.0087\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.9901 - val_loss: 1.0055\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.9875 - val_loss: 1.0022\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.9849 - val_loss: 0.9989\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.9823 - val_loss: 0.9957\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.9798 - val_loss: 0.9926\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.9771 - val_loss: 0.9896\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.9746 - val_loss: 0.9866\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.9720 - val_loss: 0.9836\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.9694 - val_loss: 0.9806\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.9671 - val_loss: 0.9776\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.9645 - val_loss: 0.9746\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.9620 - val_loss: 0.9717\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.9596 - val_loss: 0.9688\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.9571 - val_loss: 0.9659\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.9548 - val_loss: 0.9630\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.9525 - val_loss: 0.9601\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.9501 - val_loss: 0.9573\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.9476 - val_loss: 0.9545\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.9452 - val_loss: 0.9518\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.9430 - val_loss: 0.9490\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.9408 - val_loss: 0.9463\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.9382 - val_loss: 0.9436\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.9362 - val_loss: 0.9409\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.9339 - val_loss: 0.9383\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.9318 - val_loss: 0.9356\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.9294 - val_loss: 0.9331\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.9272 - val_loss: 0.9306\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.9250 - val_loss: 0.9281\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.9229 - val_loss: 0.9256\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.9207 - val_loss: 0.9231\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.9186 - val_loss: 0.9205\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.9164 - val_loss: 0.9180\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.9145 - val_loss: 0.9155\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.9122 - val_loss: 0.9131\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.9102 - val_loss: 0.9107\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.9081 - val_loss: 0.9083\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.9061 - val_loss: 0.9060\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.9040 - val_loss: 0.9036\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.9021 - val_loss: 0.9013\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.9002 - val_loss: 0.8990\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.8981 - val_loss: 0.8967\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.8962 - val_loss: 0.8946\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.8943 - val_loss: 0.8924\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.8924 - val_loss: 0.8902\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.8906 - val_loss: 0.8880\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.8887 - val_loss: 0.8858\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.8867 - val_loss: 0.8837\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.8850 - val_loss: 0.8816\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.8832 - val_loss: 0.8795\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.8814 - val_loss: 0.8774\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.8794 - val_loss: 0.8754\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.8778 - val_loss: 0.8734\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.8760 - val_loss: 0.8715\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.8745 - val_loss: 0.8695\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.8727 - val_loss: 0.8676\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.8710 - val_loss: 0.8658\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.8693 - val_loss: 0.8639\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.8678 - val_loss: 0.8620\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.8661 - val_loss: 0.8602\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.8645 - val_loss: 0.8583\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.8628 - val_loss: 0.8566\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.8614 - val_loss: 0.8548\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.8598 - val_loss: 0.8531\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.8582 - val_loss: 0.8514\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.8568 - val_loss: 0.8497\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.8552 - val_loss: 0.8480\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.8536 - val_loss: 0.8463\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.8522 - val_loss: 0.8446\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.8506 - val_loss: 0.8429\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.8492 - val_loss: 0.8412\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.8477 - val_loss: 0.8395\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.8461 - val_loss: 0.8380\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.8447 - val_loss: 0.8364\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.8434 - val_loss: 0.8348\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.8420 - val_loss: 0.8333\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.8406 - val_loss: 0.8318\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.8391 - val_loss: 0.8303\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.8378 - val_loss: 0.8287\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.8365 - val_loss: 0.8272\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.8351 - val_loss: 0.8257\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.8337 - val_loss: 0.8242\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.8323 - val_loss: 0.8227\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.8310 - val_loss: 0.8211\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.8296 - val_loss: 0.8197\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.8283 - val_loss: 0.8182\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.8270 - val_loss: 0.8168\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 0.8258 - val_loss: 0.8155\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.8244 - val_loss: 0.8141\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.8231 - val_loss: 0.8128\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.8219 - val_loss: 0.8115\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.8208 - val_loss: 0.8101\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.8195 - val_loss: 0.8088\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.8182 - val_loss: 0.8075\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.8170 - val_loss: 0.8062\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.8158 - val_loss: 0.8048\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.8145 - val_loss: 0.8035\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.8134 - val_loss: 0.8022\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.8122 - val_loss: 0.8010\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.8109 - val_loss: 0.7997\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.8098 - val_loss: 0.7985\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.8087 - val_loss: 0.7973\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.8075 - val_loss: 0.7961\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.8064 - val_loss: 0.7949\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.8053 - val_loss: 0.7937\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.8042 - val_loss: 0.7925\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.8031 - val_loss: 0.7914\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.8021 - val_loss: 0.7902\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.8009 - val_loss: 0.7891\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7999 - val_loss: 0.7880\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7987 - val_loss: 0.7870\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7978 - val_loss: 0.7858\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7967 - val_loss: 0.7848\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7957 - val_loss: 0.7837\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7946 - val_loss: 0.7826\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7937 - val_loss: 0.7816\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7926 - val_loss: 0.7806\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7918 - val_loss: 0.7795\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7907 - val_loss: 0.7785\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.7897 - val_loss: 0.7775\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7887 - val_loss: 0.7765\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.7877 - val_loss: 0.7755\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7868 - val_loss: 0.7745\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7859 - val_loss: 0.7735\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7849 - val_loss: 0.7725\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7839 - val_loss: 0.7716\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7830 - val_loss: 0.7707\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7822 - val_loss: 0.7697\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7812 - val_loss: 0.7688\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.7803 - val_loss: 0.7679\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7794 - val_loss: 0.7670\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7785 - val_loss: 0.7661\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.7776 - val_loss: 0.7652\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.7767 - val_loss: 0.7643\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7759 - val_loss: 0.7634\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.7749 - val_loss: 0.7625\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7742 - val_loss: 0.7617\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7733 - val_loss: 0.7608\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.7725 - val_loss: 0.7600\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.7716 - val_loss: 0.7592\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.7709 - val_loss: 0.7584\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.7701 - val_loss: 0.7577\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.7692 - val_loss: 0.7569\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.7685 - val_loss: 0.7561\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7676 - val_loss: 0.7553\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7668 - val_loss: 0.7544\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7660 - val_loss: 0.7536\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7651 - val_loss: 0.7528\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.7644 - val_loss: 0.7520\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7636 - val_loss: 0.7512\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7627 - val_loss: 0.7504\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.7620 - val_loss: 0.7497\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.7612 - val_loss: 0.7489\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7604 - val_loss: 0.7482\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.7597 - val_loss: 0.7474\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.7589 - val_loss: 0.7467\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7582 - val_loss: 0.7460\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7574 - val_loss: 0.7453\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7568 - val_loss: 0.7446\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7560 - val_loss: 0.7440\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.7554 - val_loss: 0.7433\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7547 - val_loss: 0.7426\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7540 - val_loss: 0.7420\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 0.7534 - val_loss: 0.7413\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.7526 - val_loss: 0.7407\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7520 - val_loss: 0.7400\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 0.7512 - val_loss: 0.7394\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7506 - val_loss: 0.7387\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7499 - val_loss: 0.7381\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7493 - val_loss: 0.7374\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7485 - val_loss: 0.7368\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7479 - val_loss: 0.7362\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.7473 - val_loss: 0.7355\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7466 - val_loss: 0.7349\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7459 - val_loss: 0.7343\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7453 - val_loss: 0.7336\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7445 - val_loss: 0.7330\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7439 - val_loss: 0.7323\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7432 - val_loss: 0.7316\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.7424 - val_loss: 0.7310\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7419 - val_loss: 0.7304\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7412 - val_loss: 0.7298\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.7405 - val_loss: 0.7292\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7399 - val_loss: 0.7286\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.7393 - val_loss: 0.7281\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7387 - val_loss: 0.7275\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.7381 - val_loss: 0.7270\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7375 - val_loss: 0.7264\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.7370 - val_loss: 0.7258\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.7364 - val_loss: 0.7253\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7357 - val_loss: 0.7248\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7352 - val_loss: 0.7242\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7346 - val_loss: 0.7237\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7341 - val_loss: 0.7232\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.7334 - val_loss: 0.7227\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7329 - val_loss: 0.7221\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7324 - val_loss: 0.7216\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7317 - val_loss: 0.7211\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7312 - val_loss: 0.7206\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.7306 - val_loss: 0.7200\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7300 - val_loss: 0.7195\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7295 - val_loss: 0.7190\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.7290 - val_loss: 0.7185\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7284 - val_loss: 0.7180\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.7278 - val_loss: 0.7175\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7273 - val_loss: 0.7170\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.7268 - val_loss: 0.7166\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.7263 - val_loss: 0.7161\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7258 - val_loss: 0.7157\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7252 - val_loss: 0.7153\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.7249 - val_loss: 0.7148\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7243 - val_loss: 0.7143\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7238 - val_loss: 0.7139\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7233 - val_loss: 0.7134\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7229 - val_loss: 0.7130\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7223 - val_loss: 0.7126\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7219 - val_loss: 0.7122\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 80us/sample - loss: 0.7215 - val_loss: 0.7118\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.7210 - val_loss: 0.7113\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.7204 - val_loss: 0.7110\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.7201 - val_loss: 0.7105\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7196 - val_loss: 0.7101\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7191 - val_loss: 0.7097\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7187 - val_loss: 0.7093\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.7182 - val_loss: 0.7089\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7178 - val_loss: 0.7085\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.7173 - val_loss: 0.7081\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.7169 - val_loss: 0.7077\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.7164 - val_loss: 0.7073\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7160 - val_loss: 0.7069\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.7155 - val_loss: 0.7065\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.7151 - val_loss: 0.7061\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7146 - val_loss: 0.7057\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.7142 - val_loss: 0.7054\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7138 - val_loss: 0.7050\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7133 - val_loss: 0.7046\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.7129 - val_loss: 0.7043\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7126 - val_loss: 0.7039\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7122 - val_loss: 0.7035\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7117 - val_loss: 0.7032\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7113 - val_loss: 0.7028\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.7109 - val_loss: 0.7025\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7105 - val_loss: 0.7021\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.7101 - val_loss: 0.7018\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.7097 - val_loss: 0.7014\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.7093 - val_loss: 0.7011\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 0.7089 - val_loss: 0.7007\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.7085 - val_loss: 0.7003\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7080 - val_loss: 0.7000\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.7077 - val_loss: 0.6996\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.7072 - val_loss: 0.6992\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.7069 - val_loss: 0.6989\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.7064 - val_loss: 0.6986\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7060 - val_loss: 0.6983\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.7057 - val_loss: 0.6979\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.7053 - val_loss: 0.6976\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.7049 - val_loss: 0.6972\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.7045 - val_loss: 0.6969\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.7042 - val_loss: 0.6965\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7038 - val_loss: 0.6962\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7034 - val_loss: 0.6958\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.7030 - val_loss: 0.6955\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.7026 - val_loss: 0.6952\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.7023 - val_loss: 0.6949\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.7020 - val_loss: 0.6947\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.7016 - val_loss: 0.6944\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7013 - val_loss: 0.6941\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.7009 - val_loss: 0.6938\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.7006 - val_loss: 0.6935\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.7003 - val_loss: 0.6932\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.7000 - val_loss: 0.6929\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 0.6997 - val_loss: 0.6926\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6993 - val_loss: 0.6924\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.6989 - val_loss: 0.6921\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.6987 - val_loss: 0.6918\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6984 - val_loss: 0.6916\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 0.6981 - val_loss: 0.6913\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.6977 - val_loss: 0.6910\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.6974 - val_loss: 0.6908\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.6972 - val_loss: 0.6905\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.6969 - val_loss: 0.6903\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6966 - val_loss: 0.6900\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6963 - val_loss: 0.6898\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.6960 - val_loss: 0.6896\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.6957 - val_loss: 0.6894\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.6954 - val_loss: 0.6891\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6952 - val_loss: 0.6889\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.6949 - val_loss: 0.6886\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.6947 - val_loss: 0.6884\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.6943 - val_loss: 0.6882\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6940 - val_loss: 0.6880\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.6938 - val_loss: 0.6877\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6935 - val_loss: 0.6875\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.6932 - val_loss: 0.6873\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.6929 - val_loss: 0.6871\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.6927 - val_loss: 0.6869\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.6924 - val_loss: 0.6867\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.6922 - val_loss: 0.6864\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.6919 - val_loss: 0.6862\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.6916 - val_loss: 0.6860\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6914 - val_loss: 0.6857\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.6911 - val_loss: 0.6855\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.6908 - val_loss: 0.6852\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.6905 - val_loss: 0.6850\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.6903 - val_loss: 0.6848\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.6900 - val_loss: 0.6845\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.6897 - val_loss: 0.6843\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.6894 - val_loss: 0.6840\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6891 - val_loss: 0.6838\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 0.6889 - val_loss: 0.6836\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6886 - val_loss: 0.6833\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6884 - val_loss: 0.6831\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6882 - val_loss: 0.6829\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.6878 - val_loss: 0.6827\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.6876 - val_loss: 0.6825\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6873 - val_loss: 0.6823\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6871 - val_loss: 0.6821\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6869 - val_loss: 0.6818\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.6867 - val_loss: 0.6816\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.6864 - val_loss: 0.6814\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.6861 - val_loss: 0.6812\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.6858 - val_loss: 0.6810\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.6857 - val_loss: 0.6808\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.6854 - val_loss: 0.6806\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.6851 - val_loss: 0.6803\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.6849 - val_loss: 0.6801\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6846 - val_loss: 0.6799\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.6844 - val_loss: 0.6797\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.6842 - val_loss: 0.6795\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.6839 - val_loss: 0.6793\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6837 - val_loss: 0.6791\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6835 - val_loss: 0.6790\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6833 - val_loss: 0.6788\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6831 - val_loss: 0.6787\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.6829 - val_loss: 0.6785\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6827 - val_loss: 0.6783\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.6825 - val_loss: 0.6782\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.6823 - val_loss: 0.6780\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.6821 - val_loss: 0.6778\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6819 - val_loss: 0.6776\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.6816 - val_loss: 0.6775\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.6815 - val_loss: 0.6772\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6812 - val_loss: 0.6771\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6810 - val_loss: 0.6769\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6808 - val_loss: 0.6767\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6806 - val_loss: 0.6766\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6804 - val_loss: 0.6764\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6802 - val_loss: 0.6762\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6800 - val_loss: 0.6761\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6798 - val_loss: 0.6759\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6796 - val_loss: 0.6757\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6794 - val_loss: 0.6755\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6792 - val_loss: 0.6754\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6790 - val_loss: 0.6752\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6788 - val_loss: 0.6750\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6786 - val_loss: 0.6748\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6784 - val_loss: 0.6746\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6782 - val_loss: 0.6744\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6780 - val_loss: 0.6743\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6778 - val_loss: 0.6741\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6777 - val_loss: 0.6739\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6775 - val_loss: 0.6738\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.6773 - val_loss: 0.6736\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.6771 - val_loss: 0.6735\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.6769 - val_loss: 0.6733\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6768 - val_loss: 0.6732\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6766 - val_loss: 0.6730\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.6764 - val_loss: 0.6729\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6762 - val_loss: 0.6727\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6760 - val_loss: 0.6725\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.6759 - val_loss: 0.6723\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.6756 - val_loss: 0.6721\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 0.6754 - val_loss: 0.6719\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6753 - val_loss: 0.6717\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6751 - val_loss: 0.6716\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.6749 - val_loss: 0.6715\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6748 - val_loss: 0.6713\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6746 - val_loss: 0.6712\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6744 - val_loss: 0.6711\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.6743 - val_loss: 0.6709\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6741 - val_loss: 0.6708\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6740 - val_loss: 0.6707\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6738 - val_loss: 0.6706\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6737 - val_loss: 0.6704\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6735 - val_loss: 0.6703\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6734 - val_loss: 0.6701\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.6733 - val_loss: 0.6700\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.6731 - val_loss: 0.6698\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.6730 - val_loss: 0.6697\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6728 - val_loss: 0.6695\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.6727 - val_loss: 0.6694\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.6726 - val_loss: 0.6693\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6724 - val_loss: 0.6691\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6723 - val_loss: 0.6690\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6721 - val_loss: 0.6689\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6720 - val_loss: 0.6688\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6719 - val_loss: 0.6686\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6718 - val_loss: 0.6685\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6716 - val_loss: 0.6684\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 82us/sample - loss: 0.6714 - val_loss: 0.6683\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6713 - val_loss: 0.6682\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6712 - val_loss: 0.6681\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6710 - val_loss: 0.6680\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6709 - val_loss: 0.6678\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6707 - val_loss: 0.6678\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6706 - val_loss: 0.6677\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6705 - val_loss: 0.6675\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 94us/sample - loss: 0.6703 - val_loss: 0.6674\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6702 - val_loss: 0.6673\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6700 - val_loss: 0.6671\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6699 - val_loss: 0.6670\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 90us/sample - loss: 0.6698 - val_loss: 0.6669\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6697 - val_loss: 0.6668\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6695 - val_loss: 0.6667\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 92us/sample - loss: 0.6694 - val_loss: 0.6666\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6693 - val_loss: 0.6664\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6692 - val_loss: 0.6663\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6690 - val_loss: 0.6662\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6690 - val_loss: 0.6661\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.6689 - val_loss: 0.6659\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.6687 - val_loss: 0.6658\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 96us/sample - loss: 0.6686 - val_loss: 0.6657\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6684 - val_loss: 0.6656\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6684 - val_loss: 0.6655\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.6682 - val_loss: 0.6654\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6681 - val_loss: 0.6653\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.6680 - val_loss: 0.6652\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.6679 - val_loss: 0.6651\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6678 - val_loss: 0.6650\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6677 - val_loss: 0.6648\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.6676 - val_loss: 0.6647\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.6675 - val_loss: 0.6646\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.6674 - val_loss: 0.6645\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.6672 - val_loss: 0.6644\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.6672 - val_loss: 0.6643\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 0.6670 - val_loss: 0.6642\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.6669 - val_loss: 0.6641\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 99us/sample - loss: 0.6668 - val_loss: 0.6639\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 88us/sample - loss: 0.6667 - val_loss: 0.6638\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.6666 - val_loss: 0.6637\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 85us/sample - loss: 0.6664 - val_loss: 0.6636\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.6664 - val_loss: 0.6634\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 97us/sample - loss: 0.6662 - val_loss: 0.6633\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.6661 - val_loss: 0.6632\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eee74caadb17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# loading into data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdata_loss_base_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdata_loss_base_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# visualizing losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "def base_line():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=1,input_shape=(1,)))# input connected to output\n",
    "    # y = a + b * x\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model\n",
    "\n",
    "# calling baseline model\n",
    "baseline = base_line()\n",
    "print(baseline.summary())\n",
    "# initializing tensorboard\n",
    "tfb = TensorBoard('baseline')\n",
    "# Training Model \n",
    "history = baseline.fit(x=x_train,y=y_train,batch_size=None,epochs=1000,callbacks=[tfb],validation_data=[x_test,y_test])\n",
    "# loading into data\n",
    "data_loss_base_line = pd.DataFrame(history.history)\n",
    "data_loss_base_line.plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "wO5xy-Erp0TH",
    "outputId": "8cec7f85-ad5a-4a98-993a-f0454e093fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f43a37ec7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD7CAYAAABzGc+QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loss_base_line = pd.DataFrame(history.history)\n",
    "data_loss_base_line.plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "IiBQYVC3kj93",
    "outputId": "66a565be-0af9-4fab-cfb3-f9f95ecc6282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f439fdbfe80>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting results\n",
    "y_pred_base= baseline.predict(X)\n",
    "plotting()\n",
    "plt.plot(X,y_pred_base,'r') # plot regression line\n",
    "plt.legend(['baseline model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeQ_ksacphl-"
   },
   "source": [
    "# Neural Network\n",
    ">  No. of hidden layer is more than 2 and number of neuron is more than 100 then this kind of network is Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z3MITvbWpyuZ",
    "outputId": "22f7a140-ca20-44a6-bc3a-a3d06ab55abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 0s 2ms/sample - loss: 0.8493 - val_loss: 0.7430\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.8165 - val_loss: 0.7271\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.7992 - val_loss: 0.7270\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.7761 - val_loss: 0.7004\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.7532 - val_loss: 0.6785\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.7380 - val_loss: 0.6598\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.7114 - val_loss: 0.6430\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.7046 - val_loss: 0.6376\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.6758 - val_loss: 0.6094\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.6629 - val_loss: 0.5973\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.6430 - val_loss: 0.5786\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.6215 - val_loss: 0.5666\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.6022 - val_loss: 0.5529\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.5869 - val_loss: 0.5373\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.5671 - val_loss: 0.5220\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.5505 - val_loss: 0.5076\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.5302 - val_loss: 0.4934\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.5142 - val_loss: 0.4800\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.4971 - val_loss: 0.4596\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.4769 - val_loss: 0.4426\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.4572 - val_loss: 0.4292\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.4448 - val_loss: 0.4123\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.4244 - val_loss: 0.4026\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.4143 - val_loss: 0.3854\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.3941 - val_loss: 0.3743\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.3839 - val_loss: 0.3680\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.3697 - val_loss: 0.3523\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.3593 - val_loss: 0.3554\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.3580 - val_loss: 0.3366\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.3371 - val_loss: 0.3315\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 0.3309 - val_loss: 0.3296\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.3271 - val_loss: 0.3185\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.3136 - val_loss: 0.3239\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.3282 - val_loss: 0.3175\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.3125 - val_loss: 0.3186\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.3113 - val_loss: 0.3233\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.3063 - val_loss: 0.3043\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.3043 - val_loss: 0.3085\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.3013 - val_loss: 0.3019\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2998 - val_loss: 0.3309\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.3079 - val_loss: 0.3048\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2910 - val_loss: 0.2986\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2907 - val_loss: 0.2958\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2871 - val_loss: 0.2975\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2864 - val_loss: 0.3010\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2866 - val_loss: 0.2983\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2864 - val_loss: 0.2919\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2829 - val_loss: 0.2920\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2840 - val_loss: 0.2913\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2797 - val_loss: 0.2960\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2837 - val_loss: 0.3014\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2865 - val_loss: 0.2971\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2808 - val_loss: 0.2901\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2825 - val_loss: 0.2922\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2830 - val_loss: 0.2942\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2784 - val_loss: 0.2929\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2846 - val_loss: 0.2907\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2837 - val_loss: 0.2862\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2812 - val_loss: 0.2868\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2808 - val_loss: 0.2886\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2832 - val_loss: 0.2979\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2793 - val_loss: 0.2891\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2824 - val_loss: 0.2887\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2829 - val_loss: 0.2883\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2796 - val_loss: 0.2958\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2782 - val_loss: 0.2867\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2773 - val_loss: 0.2993\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2931 - val_loss: 0.2971\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2892 - val_loss: 0.2843\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2747 - val_loss: 0.2964\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.2828 - val_loss: 0.3114\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2894 - val_loss: 0.2910\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.2878 - val_loss: 0.2842\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2834 - val_loss: 0.2834\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.2780 - val_loss: 0.2897\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2829 - val_loss: 0.2865\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2757 - val_loss: 0.2969\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2849 - val_loss: 0.2914\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2823 - val_loss: 0.2978\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2903 - val_loss: 0.2895\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2764 - val_loss: 0.2902\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2782 - val_loss: 0.3031\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2821 - val_loss: 0.2885\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2797 - val_loss: 0.2816\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.2798 - val_loss: 0.2807\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2803 - val_loss: 0.2869\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2772 - val_loss: 0.2898\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2743 - val_loss: 0.2862\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2770 - val_loss: 0.2864\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.2772 - val_loss: 0.2855\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2763 - val_loss: 0.2828\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2776 - val_loss: 0.2827\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2749 - val_loss: 0.2849\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2746 - val_loss: 0.2877\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2742 - val_loss: 0.2848\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2757 - val_loss: 0.2852\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.2783 - val_loss: 0.2871\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2738 - val_loss: 0.2910\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2811 - val_loss: 0.3054\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2841 - val_loss: 0.2843\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2770 - val_loss: 0.2806\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2775 - val_loss: 0.2816\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2766 - val_loss: 0.2880\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2781 - val_loss: 0.2912\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2771 - val_loss: 0.2851\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2804 - val_loss: 0.2814\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2748 - val_loss: 0.2836\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2736 - val_loss: 0.2858\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2794 - val_loss: 0.2865\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2757 - val_loss: 0.2798\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2784 - val_loss: 0.2810\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2794 - val_loss: 0.2836\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2775 - val_loss: 0.2878\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2744 - val_loss: 0.2883\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2738 - val_loss: 0.2836\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2736 - val_loss: 0.2824\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2766 - val_loss: 0.2809\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2771 - val_loss: 0.2841\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2765 - val_loss: 0.2846\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2732 - val_loss: 0.2823\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2738 - val_loss: 0.2808\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2739 - val_loss: 0.2838\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2733 - val_loss: 0.2814\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2745 - val_loss: 0.2809\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2758 - val_loss: 0.2814\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.2752 - val_loss: 0.2825\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2727 - val_loss: 0.2829\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2741 - val_loss: 0.2825\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2771 - val_loss: 0.2817\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2766 - val_loss: 0.2829\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2841 - val_loss: 0.3063\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2837 - val_loss: 0.2925\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2748 - val_loss: 0.2805\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2795 - val_loss: 0.2891\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2834 - val_loss: 0.2783\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2913 - val_loss: 0.3126\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2842 - val_loss: 0.2821\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2697 - val_loss: 0.2862\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2869 - val_loss: 0.2904\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2795 - val_loss: 0.2909\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2732 - val_loss: 0.3009\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2814 - val_loss: 0.2967\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.2751 - val_loss: 0.2806\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2715 - val_loss: 0.2858\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2809 - val_loss: 0.2820\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.2753 - val_loss: 0.2786\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2774 - val_loss: 0.2842\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2817 - val_loss: 0.2766\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2745 - val_loss: 0.2792\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2721 - val_loss: 0.2821\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2742 - val_loss: 0.2815\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2777 - val_loss: 0.2798\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2792 - val_loss: 0.2785\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2700 - val_loss: 0.2933\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2777 - val_loss: 0.2954\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.2802 - val_loss: 0.2829\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2786 - val_loss: 0.2811\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.2735 - val_loss: 0.2796\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2727 - val_loss: 0.2789\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2755 - val_loss: 0.2827\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.2732 - val_loss: 0.3125\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2889 - val_loss: 0.2946\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2753 - val_loss: 0.2846\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2868 - val_loss: 0.2885\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2819 - val_loss: 0.2869\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2894 - val_loss: 0.2975\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2755 - val_loss: 0.2793\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.2865 - val_loss: 0.2860\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.2815 - val_loss: 0.2801\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2712 - val_loss: 0.2880\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2762 - val_loss: 0.2888\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2744 - val_loss: 0.2766\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2754 - val_loss: 0.2786\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2764 - val_loss: 0.2770\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2790 - val_loss: 0.2956\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.2780 - val_loss: 0.2814\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.2756 - val_loss: 0.2849\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2811 - val_loss: 0.2832\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2745 - val_loss: 0.2876\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2765 - val_loss: 0.2998\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2794 - val_loss: 0.2941\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2746 - val_loss: 0.2796\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2715 - val_loss: 0.2781\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2772 - val_loss: 0.2791\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2828 - val_loss: 0.2846\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2785 - val_loss: 0.2795\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2708 - val_loss: 0.2999\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2867 - val_loss: 0.2945\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2779 - val_loss: 0.2806\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2815 - val_loss: 0.2790\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2710 - val_loss: 0.2912\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2804 - val_loss: 0.2982\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2835 - val_loss: 0.2873\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2737 - val_loss: 0.2797\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2702 - val_loss: 0.2790\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2851 - val_loss: 0.2836\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2780 - val_loss: 0.2834\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2712 - val_loss: 0.2922\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2764 - val_loss: 0.2898\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2754 - val_loss: 0.2873\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2734 - val_loss: 0.2796\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2711 - val_loss: 0.2802\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2723 - val_loss: 0.2816\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2741 - val_loss: 0.2841\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2705 - val_loss: 0.2934\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2810 - val_loss: 0.2917\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2719 - val_loss: 0.2808\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2745 - val_loss: 0.2832\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2778 - val_loss: 0.2792\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.2731 - val_loss: 0.2819\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2721 - val_loss: 0.2788\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2706 - val_loss: 0.2797\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2733 - val_loss: 0.2798\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2750 - val_loss: 0.2800\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2727 - val_loss: 0.2797\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2729 - val_loss: 0.2825\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2710 - val_loss: 0.2789\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2727 - val_loss: 0.2765\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2756 - val_loss: 0.2793\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2761 - val_loss: 0.2773\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2734 - val_loss: 0.2786\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2789 - val_loss: 0.2786\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2710 - val_loss: 0.2777\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2729 - val_loss: 0.2794\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2737 - val_loss: 0.2757\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.2744 - val_loss: 0.2794\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2780 - val_loss: 0.2761\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.2739 - val_loss: 0.2816\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.2734 - val_loss: 0.2781\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2718 - val_loss: 0.2802\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2715 - val_loss: 0.2793\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2718 - val_loss: 0.2777\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2707 - val_loss: 0.2764\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2736 - val_loss: 0.2799\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2742 - val_loss: 0.2804\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.2742 - val_loss: 0.2953\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2784 - val_loss: 0.2939\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2706 - val_loss: 0.2798\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2761 - val_loss: 0.2954\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2882 - val_loss: 0.2774\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2775 - val_loss: 0.2805\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2706 - val_loss: 0.2755\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2730 - val_loss: 0.2749\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2780 - val_loss: 0.2744\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2723 - val_loss: 0.2983\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2840 - val_loss: 0.3011\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2783 - val_loss: 0.2811\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2690 - val_loss: 0.2784\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2797 - val_loss: 0.2784\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2901 - val_loss: 0.2775\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2714 - val_loss: 0.2745\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2746 - val_loss: 0.2779\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2815 - val_loss: 0.2767\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2738 - val_loss: 0.2878\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2748 - val_loss: 0.2858\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.2745 - val_loss: 0.2741\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2702 - val_loss: 0.2755\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2775 - val_loss: 0.2745\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2700 - val_loss: 0.2807\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2731 - val_loss: 0.2870\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2741 - val_loss: 0.2784\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2726 - val_loss: 0.2780\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2825 - val_loss: 0.2787\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2708 - val_loss: 0.2807\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2720 - val_loss: 0.2991\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2847 - val_loss: 0.2864\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2744 - val_loss: 0.2836\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.2821 - val_loss: 0.2831\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2811 - val_loss: 0.2766\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2691 - val_loss: 0.2795\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2718 - val_loss: 0.2834\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2747 - val_loss: 0.2818\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.2781 - val_loss: 0.2832\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2685 - val_loss: 0.2769\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2745 - val_loss: 0.2773\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2744 - val_loss: 0.2774\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2736 - val_loss: 0.2834\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2734 - val_loss: 0.2874\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2707 - val_loss: 0.2784\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2734 - val_loss: 0.2740\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2778 - val_loss: 0.2790\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2760 - val_loss: 0.2739\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2703 - val_loss: 0.2940\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2793 - val_loss: 0.2927\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2801 - val_loss: 0.2891\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.2812 - val_loss: 0.2824\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2606 - val_loss: 0.2894\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.3036 - val_loss: 0.2993\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.2737 - val_loss: 0.2807\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2778 - val_loss: 0.3492\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.3082 - val_loss: 0.3058\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2687 - val_loss: 0.2787\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2794 - val_loss: 0.3167\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.3032 - val_loss: 0.2842\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2771 - val_loss: 0.2942\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2820 - val_loss: 0.2989\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2776 - val_loss: 0.2726\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2713 - val_loss: 0.2787\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2762 - val_loss: 0.2755\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2719 - val_loss: 0.2831\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.2721 - val_loss: 0.2799\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.2730 - val_loss: 0.2732\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2715 - val_loss: 0.2735\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2712 - val_loss: 0.2750\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2708 - val_loss: 0.2742\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2754 - val_loss: 0.2796\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2712 - val_loss: 0.2737\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2733 - val_loss: 0.2757\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2722 - val_loss: 0.2791\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2704 - val_loss: 0.2852\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2757 - val_loss: 0.2984\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2765 - val_loss: 0.2748\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2695 - val_loss: 0.2740\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2754 - val_loss: 0.2752\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2821 - val_loss: 0.2747\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.2712 - val_loss: 0.2859\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2731 - val_loss: 0.2900\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2748 - val_loss: 0.2883\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2725 - val_loss: 0.2777\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2691 - val_loss: 0.2763\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2694 - val_loss: 0.2766\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2686 - val_loss: 0.2767\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2686 - val_loss: 0.2781\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2692 - val_loss: 0.2771\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.2693 - val_loss: 0.2756\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2700 - val_loss: 0.2776\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2700 - val_loss: 0.2773\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2682 - val_loss: 0.2830\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2702 - val_loss: 0.2815\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2692 - val_loss: 0.2784\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2698 - val_loss: 0.2734\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2721 - val_loss: 0.2731\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.2759 - val_loss: 0.2720\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2694 - val_loss: 0.2756\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.2696 - val_loss: 0.2972\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2777 - val_loss: 0.2873\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2751 - val_loss: 0.2786\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2719 - val_loss: 0.2764\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2733 - val_loss: 0.2789\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2710 - val_loss: 0.2771\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2685 - val_loss: 0.2767\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2693 - val_loss: 0.2772\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2710 - val_loss: 0.2798\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2692 - val_loss: 0.2764\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2678 - val_loss: 0.2740\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2685 - val_loss: 0.2726\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2687 - val_loss: 0.2750\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2679 - val_loss: 0.2743\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2695 - val_loss: 0.2734\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2680 - val_loss: 0.2775\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2733 - val_loss: 0.2769\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2707 - val_loss: 0.2775\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2707 - val_loss: 0.2836\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2725 - val_loss: 0.2780\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2701 - val_loss: 0.2737\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.2690 - val_loss: 0.2731\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2687 - val_loss: 0.2755\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2684 - val_loss: 0.2749\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2736 - val_loss: 0.2784\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2677 - val_loss: 0.2717\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2833 - val_loss: 0.2801\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.2733 - val_loss: 0.2730\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2629 - val_loss: 0.2964\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2850 - val_loss: 0.3040\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2783 - val_loss: 0.2735\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2676 - val_loss: 0.2769\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2729 - val_loss: 0.2754\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2714 - val_loss: 0.2736\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2682 - val_loss: 0.2853\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2725 - val_loss: 0.2812\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2681 - val_loss: 0.2764\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2737 - val_loss: 0.2753\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2686 - val_loss: 0.2768\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2692 - val_loss: 0.2932\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2765 - val_loss: 0.2827\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2748 - val_loss: 0.2776\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2737 - val_loss: 0.2752\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.2662 - val_loss: 0.2839\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2786 - val_loss: 0.2967\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2751 - val_loss: 0.2764\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2701 - val_loss: 0.2818\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2891 - val_loss: 0.2831\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2758 - val_loss: 0.2774\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2664 - val_loss: 0.2977\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2826 - val_loss: 0.2935\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2853 - val_loss: 0.2841\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2813 - val_loss: 0.2805\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2710 - val_loss: 0.2760\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2809 - val_loss: 0.2912\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2716 - val_loss: 0.2697\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.2771 - val_loss: 0.2769\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2744 - val_loss: 0.2714\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2681 - val_loss: 0.2865\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2819 - val_loss: 0.2867\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2756 - val_loss: 0.2731\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2695 - val_loss: 0.2734\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2729 - val_loss: 0.2784\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2693 - val_loss: 0.2806\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2695 - val_loss: 0.2793\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2688 - val_loss: 0.2794\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2683 - val_loss: 0.2742\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2696 - val_loss: 0.2707\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2697 - val_loss: 0.2753\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.2707 - val_loss: 0.2805\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2734 - val_loss: 0.2797\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2729 - val_loss: 0.2784\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2688 - val_loss: 0.2833\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2713 - val_loss: 0.2761\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2704 - val_loss: 0.2787\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2688 - val_loss: 0.2806\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2674 - val_loss: 0.2736\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.2675 - val_loss: 0.2756\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2787 - val_loss: 0.2785\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2706 - val_loss: 0.2854\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2702 - val_loss: 0.2909\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2730 - val_loss: 0.2878\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2734 - val_loss: 0.2828\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2687 - val_loss: 0.2727\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2666 - val_loss: 0.2709\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2705 - val_loss: 0.2710\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2672 - val_loss: 0.2712\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2697 - val_loss: 0.2701\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2677 - val_loss: 0.2804\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2752 - val_loss: 0.2854\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2711 - val_loss: 0.2718\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2704 - val_loss: 0.2734\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2726 - val_loss: 0.2724\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2670 - val_loss: 0.2721\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2682 - val_loss: 0.2728\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2677 - val_loss: 0.2701\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2675 - val_loss: 0.2706\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.2667 - val_loss: 0.2734\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2682 - val_loss: 0.2742\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2704 - val_loss: 0.2751\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2656 - val_loss: 0.2752\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2679 - val_loss: 0.2814\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2740 - val_loss: 0.2807\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2775 - val_loss: 0.2759\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.2679 - val_loss: 0.2741\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2674 - val_loss: 0.2838\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2715 - val_loss: 0.2815\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.2697 - val_loss: 0.2743\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2725 - val_loss: 0.2723\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2671 - val_loss: 0.2745\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2677 - val_loss: 0.2775\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2690 - val_loss: 0.2831\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2705 - val_loss: 0.2793\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2703 - val_loss: 0.2773\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2727 - val_loss: 0.2721\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2673 - val_loss: 0.2739\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2736 - val_loss: 0.2873\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2711 - val_loss: 0.2789\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2700 - val_loss: 0.2730\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2666 - val_loss: 0.2740\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2721 - val_loss: 0.2688\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2659 - val_loss: 0.2784\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2866 - val_loss: 0.2764\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2721 - val_loss: 0.2820\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2721 - val_loss: 0.2992\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2814 - val_loss: 0.2754\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.2649 - val_loss: 0.2836\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2923 - val_loss: 0.2888\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2804 - val_loss: 0.2686\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2612 - val_loss: 0.2997\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2856 - val_loss: 0.3006\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2720 - val_loss: 0.2737\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2699 - val_loss: 0.2820\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2794 - val_loss: 0.2730\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2645 - val_loss: 0.2804\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2812 - val_loss: 0.2987\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2824 - val_loss: 0.2712\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2659 - val_loss: 0.2711\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2686 - val_loss: 0.2730\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.2724 - val_loss: 0.2729\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2740 - val_loss: 0.2842\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2746 - val_loss: 0.2796\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2704 - val_loss: 0.2721\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2664 - val_loss: 0.2766\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2699 - val_loss: 0.2807\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2680 - val_loss: 0.2799\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2663 - val_loss: 0.2710\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2808 - val_loss: 0.2679\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2640 - val_loss: 0.2779\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2758 - val_loss: 0.2929\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2720 - val_loss: 0.2756\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2677 - val_loss: 0.2773\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2705 - val_loss: 0.2732\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2656 - val_loss: 0.2745\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.2681 - val_loss: 0.2757\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2666 - val_loss: 0.2696\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2702 - val_loss: 0.2695\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2665 - val_loss: 0.2720\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2749 - val_loss: 0.2864\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2720 - val_loss: 0.2762\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2640 - val_loss: 0.2732\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2671 - val_loss: 0.2724\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2683 - val_loss: 0.2686\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.2703 - val_loss: 0.2688\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2719 - val_loss: 0.2671\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2661 - val_loss: 0.2736\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2660 - val_loss: 0.2793\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.2690 - val_loss: 0.2779\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2662 - val_loss: 0.2705\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2753 - val_loss: 0.2721\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2662 - val_loss: 0.2741\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.2725 - val_loss: 0.2864\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2715 - val_loss: 0.2698\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2655 - val_loss: 0.2692\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2831 - val_loss: 0.2696\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2653 - val_loss: 0.2824\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.2746 - val_loss: 0.2962\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2802 - val_loss: 0.2721\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2715 - val_loss: 0.2824\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2831 - val_loss: 0.2774\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.2736 - val_loss: 0.2689\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2644 - val_loss: 0.2795\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2727 - val_loss: 0.2772\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2708 - val_loss: 0.2758\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2662 - val_loss: 0.2678\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2650 - val_loss: 0.2701\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.2751 - val_loss: 0.2674\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2622 - val_loss: 0.2768\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2824 - val_loss: 0.2931\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2688 - val_loss: 0.2678\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2746 - val_loss: 0.2795\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2766 - val_loss: 0.2728\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2698 - val_loss: 0.2853\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2705 - val_loss: 0.2821\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2696 - val_loss: 0.2762\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2665 - val_loss: 0.2691\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2659 - val_loss: 0.2656\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2687 - val_loss: 0.2701\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2719 - val_loss: 0.2700\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2684 - val_loss: 0.2670\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2671 - val_loss: 0.2686\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2664 - val_loss: 0.2748\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2639 - val_loss: 0.2838\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2730 - val_loss: 0.2836\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2690 - val_loss: 0.2696\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2706 - val_loss: 0.2697\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2713 - val_loss: 0.2742\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2723 - val_loss: 0.2695\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2601 - val_loss: 0.2890\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 164us/sample - loss: 0.2835 - val_loss: 0.3108\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2775 - val_loss: 0.2774\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2650 - val_loss: 0.2822\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2814 - val_loss: 0.2775\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.2729 - val_loss: 0.2662\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2631 - val_loss: 0.2836\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.2775 - val_loss: 0.2894\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2719 - val_loss: 0.2749\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2752 - val_loss: 0.2797\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2731 - val_loss: 0.2761\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2679 - val_loss: 0.2748\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2733 - val_loss: 0.2822\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2690 - val_loss: 0.2681\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2649 - val_loss: 0.2655\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2729 - val_loss: 0.2653\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2613 - val_loss: 0.2768\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2710 - val_loss: 0.3057\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2811 - val_loss: 0.2814\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2631 - val_loss: 0.2696\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2738 - val_loss: 0.2775\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2720 - val_loss: 0.2699\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2637 - val_loss: 0.2790\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2706 - val_loss: 0.2797\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2654 - val_loss: 0.2655\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2655 - val_loss: 0.2661\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.2719 - val_loss: 0.2659\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2695 - val_loss: 0.2670\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2714 - val_loss: 0.2786\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2668 - val_loss: 0.2783\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2656 - val_loss: 0.2731\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2745 - val_loss: 0.2817\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2775 - val_loss: 0.2690\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2661 - val_loss: 0.2648\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.2643 - val_loss: 0.2783\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2778 - val_loss: 0.2772\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2629 - val_loss: 0.2654\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2737 - val_loss: 0.2781\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2784 - val_loss: 0.2667\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2602 - val_loss: 0.2789\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2742 - val_loss: 0.2929\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2755 - val_loss: 0.2697\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2711 - val_loss: 0.2690\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2710 - val_loss: 0.2679\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2664 - val_loss: 0.2684\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2656 - val_loss: 0.2820\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2690 - val_loss: 0.2814\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2669 - val_loss: 0.2699\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2653 - val_loss: 0.2722\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.2683 - val_loss: 0.2717\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.2636 - val_loss: 0.2746\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2714 - val_loss: 0.2788\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2702 - val_loss: 0.2685\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2651 - val_loss: 0.2686\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2696 - val_loss: 0.2687\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2666 - val_loss: 0.2684\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2646 - val_loss: 0.2830\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.2690 - val_loss: 0.2832\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2715 - val_loss: 0.2699\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2690 - val_loss: 0.2660\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2661 - val_loss: 0.2682\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2639 - val_loss: 0.2659\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2647 - val_loss: 0.2637\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2722 - val_loss: 0.2641\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2666 - val_loss: 0.2666\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2635 - val_loss: 0.2708\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2653 - val_loss: 0.2746\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2658 - val_loss: 0.2716\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2642 - val_loss: 0.2697\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2681 - val_loss: 0.2676\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2698 - val_loss: 0.2673\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2713 - val_loss: 0.2639\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2624 - val_loss: 0.2812\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2774 - val_loss: 0.2895\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 111us/sample - loss: 0.2708 - val_loss: 0.2703\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2732 - val_loss: 0.2723\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.2705 - val_loss: 0.2693\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2685 - val_loss: 0.2778\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2698 - val_loss: 0.2753\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2653 - val_loss: 0.2667\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2666 - val_loss: 0.2650\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2661 - val_loss: 0.2648\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2669 - val_loss: 0.2705\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.2662 - val_loss: 0.2708\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2632 - val_loss: 0.2696\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.2662 - val_loss: 0.2733\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2666 - val_loss: 0.2702\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2656 - val_loss: 0.2696\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2637 - val_loss: 0.2703\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2674 - val_loss: 0.2702\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.2649 - val_loss: 0.2685\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2659 - val_loss: 0.2713\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2645 - val_loss: 0.2686\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.2672 - val_loss: 0.2695\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2659 - val_loss: 0.2708\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2665 - val_loss: 0.2765\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2659 - val_loss: 0.2702\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.2663 - val_loss: 0.2678\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2686 - val_loss: 0.2685\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2761 - val_loss: 0.2798\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.2652 - val_loss: 0.2660\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2618 - val_loss: 0.2660\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2763 - val_loss: 0.2683\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2734 - val_loss: 0.2715\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2643 - val_loss: 0.2757\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2699 - val_loss: 0.2742\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2637 - val_loss: 0.2667\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2655 - val_loss: 0.2652\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2684 - val_loss: 0.2630\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2672 - val_loss: 0.2672\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2653 - val_loss: 0.2659\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2632 - val_loss: 0.2674\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2659 - val_loss: 0.2692\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2672 - val_loss: 0.2656\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2657 - val_loss: 0.2713\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2651 - val_loss: 0.2682\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.2666 - val_loss: 0.2670\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2684 - val_loss: 0.2645\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2649 - val_loss: 0.2642\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2664 - val_loss: 0.2673\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2632 - val_loss: 0.2661\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2646 - val_loss: 0.2629\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2666 - val_loss: 0.2645\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2655 - val_loss: 0.2652\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2606 - val_loss: 0.2765\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2666 - val_loss: 0.2931\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2732 - val_loss: 0.2776\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2666 - val_loss: 0.2698\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2729 - val_loss: 0.2703\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2667 - val_loss: 0.2653\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2723 - val_loss: 0.2917\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 164us/sample - loss: 0.2755 - val_loss: 0.2773\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2620 - val_loss: 0.2719\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2718 - val_loss: 0.2851\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2807 - val_loss: 0.2717\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2770 - val_loss: 0.2769\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2790 - val_loss: 0.2772\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2580 - val_loss: 0.2723\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2760 - val_loss: 0.2908\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2830 - val_loss: 0.2653\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2626 - val_loss: 0.2754\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2682 - val_loss: 0.2798\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2712 - val_loss: 0.2637\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.2630 - val_loss: 0.2598\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2691 - val_loss: 0.2616\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2667 - val_loss: 0.2637\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2660 - val_loss: 0.2730\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.2658 - val_loss: 0.2676\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2650 - val_loss: 0.2697\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2695 - val_loss: 0.2677\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.2702 - val_loss: 0.2631\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2654 - val_loss: 0.2650\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2647 - val_loss: 0.2659\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2639 - val_loss: 0.2671\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2659 - val_loss: 0.2646\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2626 - val_loss: 0.2675\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2654 - val_loss: 0.2758\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 0.2667 - val_loss: 0.2692\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2634 - val_loss: 0.2641\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2796 - val_loss: 0.2788\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2767 - val_loss: 0.2677\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2655 - val_loss: 0.2715\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 149us/sample - loss: 0.2660 - val_loss: 0.2668\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2667 - val_loss: 0.2609\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2668 - val_loss: 0.2604\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2648 - val_loss: 0.2599\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2650 - val_loss: 0.2614\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2622 - val_loss: 0.2681\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2644 - val_loss: 0.2780\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2693 - val_loss: 0.2871\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2731 - val_loss: 0.2778\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2615 - val_loss: 0.2637\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2701 - val_loss: 0.2938\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2962 - val_loss: 0.2705\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2830 - val_loss: 0.2889\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.2810 - val_loss: 0.3034\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2821 - val_loss: 0.2782\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2654 - val_loss: 0.2669\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2679 - val_loss: 0.2676\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2678 - val_loss: 0.2658\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2630 - val_loss: 0.2675\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 157us/sample - loss: 0.2641 - val_loss: 0.2675\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2615 - val_loss: 0.2621\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2619 - val_loss: 0.2652\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2679 - val_loss: 0.2656\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2630 - val_loss: 0.2718\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2698 - val_loss: 0.2845\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2717 - val_loss: 0.2708\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2652 - val_loss: 0.2700\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2700 - val_loss: 0.2686\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.2751 - val_loss: 0.2647\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2620 - val_loss: 0.2732\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2642 - val_loss: 0.2958\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2815 - val_loss: 0.2890\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2771 - val_loss: 0.2707\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2632 - val_loss: 0.2672\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2626 - val_loss: 0.2651\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2720 - val_loss: 0.2625\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2636 - val_loss: 0.2706\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2649 - val_loss: 0.2713\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2627 - val_loss: 0.2639\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2617 - val_loss: 0.2661\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2741 - val_loss: 0.2663\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2638 - val_loss: 0.2653\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2780 - val_loss: 0.2856\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2734 - val_loss: 0.2615\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2679 - val_loss: 0.2637\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2642 - val_loss: 0.2669\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2676 - val_loss: 0.2721\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2678 - val_loss: 0.2683\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2635 - val_loss: 0.2684\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2649 - val_loss: 0.2637\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2625 - val_loss: 0.2705\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2650 - val_loss: 0.2643\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 146us/sample - loss: 0.2636 - val_loss: 0.2595\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2612 - val_loss: 0.2634\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2671 - val_loss: 0.2705\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2663 - val_loss: 0.2782\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2753 - val_loss: 0.2904\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2693 - val_loss: 0.2664\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2701 - val_loss: 0.2658\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2689 - val_loss: 0.2616\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2652 - val_loss: 0.2662\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2621 - val_loss: 0.2687\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.2642 - val_loss: 0.2652\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2625 - val_loss: 0.2639\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2643 - val_loss: 0.2638\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2701 - val_loss: 0.2665\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2668 - val_loss: 0.2658\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2645 - val_loss: 0.2626\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2616 - val_loss: 0.2670\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2631 - val_loss: 0.2707\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2653 - val_loss: 0.2721\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2638 - val_loss: 0.2752\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2641 - val_loss: 0.2700\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2637 - val_loss: 0.2707\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2655 - val_loss: 0.2681\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2668 - val_loss: 0.2704\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2663 - val_loss: 0.2626\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2622 - val_loss: 0.2630\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2684 - val_loss: 0.2623\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2669 - val_loss: 0.2613\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2609 - val_loss: 0.2669\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2654 - val_loss: 0.2711\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2672 - val_loss: 0.2653\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2623 - val_loss: 0.2653\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2614 - val_loss: 0.2678\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2698 - val_loss: 0.2734\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2620 - val_loss: 0.2640\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2617 - val_loss: 0.2625\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2645 - val_loss: 0.2614\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2653 - val_loss: 0.2639\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2619 - val_loss: 0.2656\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2634 - val_loss: 0.2679\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.2636 - val_loss: 0.2653\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2750 - val_loss: 0.2619\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2609 - val_loss: 0.2657\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2621 - val_loss: 0.2793\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2694 - val_loss: 0.2836\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2682 - val_loss: 0.2674\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2599 - val_loss: 0.2629\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2684 - val_loss: 0.2657\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2692 - val_loss: 0.2599\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2642 - val_loss: 0.2608\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2661 - val_loss: 0.2749\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2651 - val_loss: 0.2667\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2659 - val_loss: 0.2668\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2675 - val_loss: 0.2681\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.2617 - val_loss: 0.2627\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2615 - val_loss: 0.2652\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.2656 - val_loss: 0.2625\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2747 - val_loss: 0.2657\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2619 - val_loss: 0.2611\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2641 - val_loss: 0.2654\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2677 - val_loss: 0.2629\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2640 - val_loss: 0.2616\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2693 - val_loss: 0.2698\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.2644 - val_loss: 0.2595\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2636 - val_loss: 0.2626\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2632 - val_loss: 0.2677\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2667 - val_loss: 0.2696\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2628 - val_loss: 0.2628\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2648 - val_loss: 0.2603\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2626 - val_loss: 0.2593\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2622 - val_loss: 0.2590\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2636 - val_loss: 0.2599\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2637 - val_loss: 0.2631\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2621 - val_loss: 0.2639\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 0.2619 - val_loss: 0.2621\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2628 - val_loss: 0.2602\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.2614 - val_loss: 0.2601\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2622 - val_loss: 0.2608\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.2630 - val_loss: 0.2615\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2636 - val_loss: 0.2697\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2635 - val_loss: 0.2659\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2630 - val_loss: 0.2664\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2633 - val_loss: 0.2640\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2617 - val_loss: 0.2638\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2609 - val_loss: 0.2632\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 152us/sample - loss: 0.2617 - val_loss: 0.2621\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2614 - val_loss: 0.2626\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 155us/sample - loss: 0.2606 - val_loss: 0.2681\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2615 - val_loss: 0.2680\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2639 - val_loss: 0.2636\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2616 - val_loss: 0.2638\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2672 - val_loss: 0.2628\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2665 - val_loss: 0.2617\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.2750 - val_loss: 0.2837\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2723 - val_loss: 0.2731\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2610 - val_loss: 0.2654\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2670 - val_loss: 0.2694\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2650 - val_loss: 0.2654\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2571 - val_loss: 0.2810\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2696 - val_loss: 0.2905\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2751 - val_loss: 0.2697\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2622 - val_loss: 0.2628\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2649 - val_loss: 0.2687\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 127us/sample - loss: 0.2782 - val_loss: 0.2653\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2692 - val_loss: 0.2685\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2685 - val_loss: 0.2687\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2646 - val_loss: 0.2604\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.2617 - val_loss: 0.2591\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2634 - val_loss: 0.2596\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2670 - val_loss: 0.2615\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2630 - val_loss: 0.2597\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.2618 - val_loss: 0.2814\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 154us/sample - loss: 0.2719 - val_loss: 0.2794\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2696 - val_loss: 0.2621\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2748 - val_loss: 0.2636\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2659 - val_loss: 0.2636\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2629 - val_loss: 0.2691\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2653 - val_loss: 0.2668\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2650 - val_loss: 0.2603\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2641 - val_loss: 0.2632\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2630 - val_loss: 0.2628\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 117us/sample - loss: 0.2598 - val_loss: 0.2675\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 109us/sample - loss: 0.2603 - val_loss: 0.2736\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2688 - val_loss: 0.2711\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2641 - val_loss: 0.2682\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2630 - val_loss: 0.2668\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.2644 - val_loss: 0.2642\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.2685 - val_loss: 0.2652\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2654 - val_loss: 0.2594\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2612 - val_loss: 0.2658\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2625 - val_loss: 0.2659\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2617 - val_loss: 0.2643\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 102us/sample - loss: 0.2672 - val_loss: 0.2666\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2610 - val_loss: 0.2734\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2750 - val_loss: 0.2916\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 135us/sample - loss: 0.2748 - val_loss: 0.2701\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 0.2622 - val_loss: 0.2636\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.2659 - val_loss: 0.2648\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.2648 - val_loss: 0.2654\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2636 - val_loss: 0.2667\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2614 - val_loss: 0.2678\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2674 - val_loss: 0.2673\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2635 - val_loss: 0.2700\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2643 - val_loss: 0.2652\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2658 - val_loss: 0.2684\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2631 - val_loss: 0.2705\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2626 - val_loss: 0.2711\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 150us/sample - loss: 0.2613 - val_loss: 0.2625\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2622 - val_loss: 0.2599\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2615 - val_loss: 0.2603\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.2607 - val_loss: 0.2668\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2638 - val_loss: 0.2722\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.2654 - val_loss: 0.2681\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 142us/sample - loss: 0.2644 - val_loss: 0.2661\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2627 - val_loss: 0.2620\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2678 - val_loss: 0.2595\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2687 - val_loss: 0.2675\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 122us/sample - loss: 0.2768 - val_loss: 0.2753\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2645 - val_loss: 0.2620\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2641 - val_loss: 0.2672\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.2657 - val_loss: 0.2624\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2604 - val_loss: 0.2764\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2677 - val_loss: 0.2730\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 0.2666 - val_loss: 0.2620\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2619 - val_loss: 0.2642\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2620 - val_loss: 0.2634\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 0.2616 - val_loss: 0.2656\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2618 - val_loss: 0.2703\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2695 - val_loss: 0.2717\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 153us/sample - loss: 0.2615 - val_loss: 0.2620\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2718 - val_loss: 0.2613\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2622 - val_loss: 0.2626\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2763 - val_loss: 0.2811\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2694 - val_loss: 0.2625\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2617 - val_loss: 0.2690\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2674 - val_loss: 0.2668\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.2635 - val_loss: 0.2656\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2607 - val_loss: 0.2697\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2646 - val_loss: 0.2644\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2638 - val_loss: 0.2583\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2602 - val_loss: 0.2592\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2608 - val_loss: 0.2612\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2638 - val_loss: 0.2628\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2597 - val_loss: 0.2683\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2633 - val_loss: 0.2795\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2658 - val_loss: 0.2696\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.2632 - val_loss: 0.2601\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2750 - val_loss: 0.2620\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 139us/sample - loss: 0.2666 - val_loss: 0.2664\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2603 - val_loss: 0.2734\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2630 - val_loss: 0.2729\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2644 - val_loss: 0.2687\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2622 - val_loss: 0.2679\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 141us/sample - loss: 0.2657 - val_loss: 0.2615\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2707 - val_loss: 0.2642\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2624 - val_loss: 0.2576\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2635 - val_loss: 0.2576\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 126us/sample - loss: 0.2621 - val_loss: 0.2608\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2616 - val_loss: 0.2647\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 0.2609 - val_loss: 0.2738\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 130us/sample - loss: 0.2672 - val_loss: 0.2662\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2647 - val_loss: 0.2636\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2637 - val_loss: 0.2695\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 148us/sample - loss: 0.2616 - val_loss: 0.2623\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.2594 - val_loss: 0.2621\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2623 - val_loss: 0.2611\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2613 - val_loss: 0.2589\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 0.2650 - val_loss: 0.2694\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2675 - val_loss: 0.2610\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2595 - val_loss: 0.2626\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2602 - val_loss: 0.2636\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 114us/sample - loss: 0.2620 - val_loss: 0.2646\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 107us/sample - loss: 0.2618 - val_loss: 0.2591\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 145us/sample - loss: 0.2627 - val_loss: 0.2631\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.2628 - val_loss: 0.2669\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2628 - val_loss: 0.2677\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2632 - val_loss: 0.2673\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2628 - val_loss: 0.2641\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.2601 - val_loss: 0.2644\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2612 - val_loss: 0.2673\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2644 - val_loss: 0.2620\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2636 - val_loss: 0.2591\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 133us/sample - loss: 0.2633 - val_loss: 0.2599\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2680 - val_loss: 0.2654\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 136us/sample - loss: 0.2616 - val_loss: 0.2669\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 0.2648 - val_loss: 0.2642\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 147us/sample - loss: 0.2674 - val_loss: 0.2588\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 0.2624 - val_loss: 0.2657\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 124us/sample - loss: 0.2645 - val_loss: 0.2625\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 134us/sample - loss: 0.2618 - val_loss: 0.2634\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 0.2625 - val_loss: 0.2675\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 0.2608 - val_loss: 0.2637\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 0.2727 - val_loss: 0.2691\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2665 - val_loss: 0.2576\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 137us/sample - loss: 0.2569 - val_loss: 0.2810\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 0.2704 - val_loss: 0.2833\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 121us/sample - loss: 0.2727 - val_loss: 0.2629\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 0.2615 - val_loss: 0.2613\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 0.2599 - val_loss: 0.2663\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.2745 - val_loss: 0.2640\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 119us/sample - loss: 0.2606 - val_loss: 0.2664\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 0.2631 - val_loss: 0.2913\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 116us/sample - loss: 0.2813 - val_loss: 0.2830\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 125us/sample - loss: 0.2803 - val_loss: 0.2648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f439f43af60>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def neural_network():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=100,input_shape=(1,),activation='relu'))# input connected to output\n",
    "    model.add(Dense(units=50,activation='relu'))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model\n",
    "\n",
    "# calling baseline model\n",
    "neural_network = neural_network()\n",
    "print(baseline.summary())\n",
    "# initializing tensorboard\n",
    "tfb = TensorBoard('neural_network')\n",
    "# Training Model \n",
    "history = neural_network.fit(x=x_train,y=y_train,batch_size=None,epochs=1000,callbacks=[tfb],validation_data=[x_test,y_test])\n",
    "# loading into data\n",
    "data_loss_nn = pd.DataFrame(history.history)\n",
    "data_loss_nn.plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "EZRAprrrqy_S",
    "outputId": "7c05a3ce-2fa5-4591-d118-4ddac2c30139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f439f3f4cf8>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting results\n",
    "y_pred_nn= neural_network.predict(X)\n",
    "plotting()\n",
    "plt.plot(X,y_pred_base) # plot regression line\n",
    "plt.plot(X,y_pred_nn) # plot regression line\n",
    "plt.legend(['baseline model','Neural Network'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSj4aLt5sJlg"
   },
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gpeexXkxr2v2",
    "outputId": "d106801a-c9aa-49bc-8afc-492ce5c558c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 6,009,001\n",
      "Trainable params: 6,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "32/80 [===========>..................] - ETA: 7s - loss: 0.8504WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.102430). Check your callbacks.\n",
      "80/80 [==============================] - 5s 63ms/sample - loss: 0.9671 - val_loss: 0.7887\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.8125 - val_loss: 0.7672\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.7466 - val_loss: 0.6459\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.6730 - val_loss: 0.5401\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.4864 - val_loss: 1.5685\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.8193 - val_loss: 0.6553\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.6989 - val_loss: 0.6118\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5897 - val_loss: 0.5437\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.5684 - val_loss: 0.5030\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.4905 - val_loss: 0.4408\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.4167 - val_loss: 0.4294\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.3625 - val_loss: 0.3079\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2813 - val_loss: 0.3201\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2951 - val_loss: 0.3512\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2516 - val_loss: 0.3263\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.2585 - val_loss: 0.3613\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2593 - val_loss: 0.2882\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2311 - val_loss: 0.2918\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2305 - val_loss: 0.2913\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.2473 - val_loss: 0.3193\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2204 - val_loss: 0.2632\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.2398 - val_loss: 0.2378\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2100 - val_loss: 0.2653\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2186 - val_loss: 0.2484\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2005 - val_loss: 0.2578\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.2105 - val_loss: 0.2457\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2054 - val_loss: 0.2485\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.1994 - val_loss: 0.2581\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1986 - val_loss: 0.2570\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.1977 - val_loss: 0.2594\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 454us/sample - loss: 0.1914 - val_loss: 0.2495\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1895 - val_loss: 0.2500\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2006 - val_loss: 0.2497\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1955 - val_loss: 0.2506\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1932 - val_loss: 0.2594\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1950 - val_loss: 0.2454\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1900 - val_loss: 0.2425\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1816 - val_loss: 0.2374\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1878 - val_loss: 0.2423\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1819 - val_loss: 0.2422\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1805 - val_loss: 0.2483\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1805 - val_loss: 0.2449\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1794 - val_loss: 0.2456\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1784 - val_loss: 0.2547\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1809 - val_loss: 0.2629\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.2177 - val_loss: 0.2737\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1830 - val_loss: 0.2995\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2074 - val_loss: 0.2977\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2141 - val_loss: 0.2487\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1833 - val_loss: 0.2548\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.1719 - val_loss: 0.2366\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1836 - val_loss: 0.2354\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.1810 - val_loss: 0.2537\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1827 - val_loss: 0.2565\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1908 - val_loss: 0.2543\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1727 - val_loss: 0.2488\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1679 - val_loss: 0.2483\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1684 - val_loss: 0.2625\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.1658 - val_loss: 0.2603\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1767 - val_loss: 0.2642\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1645 - val_loss: 0.2629\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1708 - val_loss: 0.2668\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1683 - val_loss: 0.2547\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1611 - val_loss: 0.2556\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1626 - val_loss: 0.2649\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1843 - val_loss: 0.2610\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.1755 - val_loss: 0.2477\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1688 - val_loss: 0.2550\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.1796 - val_loss: 0.2401\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1876 - val_loss: 0.2427\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1582 - val_loss: 0.2923\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2367 - val_loss: 0.2535\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1796 - val_loss: 0.2894\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.1793 - val_loss: 0.2723\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1860 - val_loss: 0.2620\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1850 - val_loss: 0.2514\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1609 - val_loss: 0.2611\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1585 - val_loss: 0.2396\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1534 - val_loss: 0.2656\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1774 - val_loss: 0.2772\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1638 - val_loss: 0.2551\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1492 - val_loss: 0.2706\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1531 - val_loss: 0.2600\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1509 - val_loss: 0.2774\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1820 - val_loss: 0.2808\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1873 - val_loss: 0.2594\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1500 - val_loss: 0.2787\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 291us/sample - loss: 0.1562 - val_loss: 0.2699\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1776 - val_loss: 0.2732\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1559 - val_loss: 0.2595\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.1567 - val_loss: 0.2598\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1522 - val_loss: 0.2564\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1703 - val_loss: 0.2665\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1629 - val_loss: 0.2659\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1568 - val_loss: 0.2608\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1636 - val_loss: 0.2675\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1539 - val_loss: 0.2549\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1423 - val_loss: 0.2627\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1492 - val_loss: 0.2626\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1415 - val_loss: 0.2690\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1491 - val_loss: 0.2684\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1449 - val_loss: 0.2847\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1485 - val_loss: 0.2807\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.1423 - val_loss: 0.2840\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 0.1868 - val_loss: 0.2875\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1926 - val_loss: 0.2977\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1679 - val_loss: 0.2837\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1719 - val_loss: 0.2694\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1435 - val_loss: 0.2695\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1521 - val_loss: 0.2874\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.1604 - val_loss: 0.2713\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1512 - val_loss: 0.2757\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1423 - val_loss: 0.2783\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1594 - val_loss: 0.2833\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1759 - val_loss: 0.2818\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1454 - val_loss: 0.2800\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1394 - val_loss: 0.2794\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1376 - val_loss: 0.2839\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1424 - val_loss: 0.2863\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1397 - val_loss: 0.2880\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1363 - val_loss: 0.2818\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1402 - val_loss: 0.2971\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1638 - val_loss: 0.3022\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1592 - val_loss: 0.3126\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2349 - val_loss: 0.2965\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1702 - val_loss: 0.3034\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1777 - val_loss: 0.2520\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1749 - val_loss: 0.2435\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.1832 - val_loss: 0.2278\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1971 - val_loss: 0.2358\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.1581 - val_loss: 0.2653\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1773 - val_loss: 0.2405\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1564 - val_loss: 0.2469\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1536 - val_loss: 0.2492\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1661 - val_loss: 0.2618\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.1609 - val_loss: 0.2825\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1537 - val_loss: 0.2709\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1682 - val_loss: 0.2828\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1700 - val_loss: 0.2876\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1594 - val_loss: 0.2932\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1658 - val_loss: 0.2862\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1545 - val_loss: 0.2611\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1446 - val_loss: 0.2660\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1381 - val_loss: 0.2671\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.1445 - val_loss: 0.2655\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1420 - val_loss: 0.2578\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.1325 - val_loss: 0.2635\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1377 - val_loss: 0.2678\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1352 - val_loss: 0.2732\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1264 - val_loss: 0.2800\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.1367 - val_loss: 0.2820\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1360 - val_loss: 0.2773\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 282us/sample - loss: 0.1630 - val_loss: 0.3022\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.1878 - val_loss: 0.2869\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1504 - val_loss: 0.2880\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1438 - val_loss: 0.2939\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1719 - val_loss: 0.2788\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.1561 - val_loss: 0.2695\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.1449 - val_loss: 0.2619\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1352 - val_loss: 0.2772\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.1543 - val_loss: 0.2688\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1442 - val_loss: 0.2702\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1630 - val_loss: 0.2689\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1777 - val_loss: 0.2761\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1927 - val_loss: 0.2994\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1884 - val_loss: 0.2808\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1751 - val_loss: 0.2854\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1520 - val_loss: 0.2898\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1474 - val_loss: 0.2840\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1411 - val_loss: 0.2664\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1586 - val_loss: 0.2687\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.1509 - val_loss: 0.2706\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.1334 - val_loss: 0.2826\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1282 - val_loss: 0.2864\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1441 - val_loss: 0.2818\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.1392 - val_loss: 0.2948\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.1361 - val_loss: 0.2851\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1207 - val_loss: 0.2859\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.1262 - val_loss: 0.2899\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.1779 - val_loss: 0.3008\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.2444 - val_loss: 0.3006\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.2433 - val_loss: 0.3257\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2429 - val_loss: 0.3091\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.2132 - val_loss: 0.2566\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1942 - val_loss: 0.2264\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1816 - val_loss: 0.2235\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1832 - val_loss: 0.2263\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1718 - val_loss: 0.2379\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1742 - val_loss: 0.2306\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1663 - val_loss: 0.2288\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.1620 - val_loss: 0.2311\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.1618 - val_loss: 0.2377\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1623 - val_loss: 0.2392\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1547 - val_loss: 0.2388\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.1590 - val_loss: 0.2378\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1605 - val_loss: 0.2285\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1481 - val_loss: 0.2300\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.1515 - val_loss: 0.2291\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1390 - val_loss: 0.2347\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1466 - val_loss: 0.2367\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1407 - val_loss: 0.2407\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1319 - val_loss: 0.2491\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.1421 - val_loss: 0.2420\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1476 - val_loss: 0.2381\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1299 - val_loss: 0.2375\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1377 - val_loss: 0.2343\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1247 - val_loss: 0.2419\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1327 - val_loss: 0.2403\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1245 - val_loss: 0.2493\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1215 - val_loss: 0.2494\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.1194 - val_loss: 0.2613\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1284 - val_loss: 0.2500\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1291 - val_loss: 0.2486\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1408 - val_loss: 0.2586\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1460 - val_loss: 0.2630\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.1631 - val_loss: 0.2595\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.1731 - val_loss: 0.2873\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1771 - val_loss: 0.2615\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.1539 - val_loss: 0.2484\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1472 - val_loss: 0.2533\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1422 - val_loss: 0.2272\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.1296 - val_loss: 0.2295\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1309 - val_loss: 0.2278\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1256 - val_loss: 0.2338\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1202 - val_loss: 0.2394\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1199 - val_loss: 0.2451\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1228 - val_loss: 0.2446\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1187 - val_loss: 0.2512\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1217 - val_loss: 0.2519\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.1228 - val_loss: 0.2550\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1192 - val_loss: 0.2607\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1185 - val_loss: 0.2653\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1299 - val_loss: 0.2671\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1175 - val_loss: 0.2614\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1269 - val_loss: 0.2716\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1245 - val_loss: 0.2735\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1198 - val_loss: 0.2674\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1164 - val_loss: 0.2638\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.1209 - val_loss: 0.2664\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1190 - val_loss: 0.2597\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1147 - val_loss: 0.2716\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1304 - val_loss: 0.2808\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1209 - val_loss: 0.2851\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.1147 - val_loss: 0.2781\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1170 - val_loss: 0.2711\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1192 - val_loss: 0.2659\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1198 - val_loss: 0.2776\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1229 - val_loss: 0.2823\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.1345 - val_loss: 0.2832\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1735 - val_loss: 0.2628\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.1631 - val_loss: 0.2617\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1393 - val_loss: 0.3002\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 272us/sample - loss: 0.1620 - val_loss: 0.2607\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1367 - val_loss: 0.2496\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1326 - val_loss: 0.2408\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1216 - val_loss: 0.2428\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1268 - val_loss: 0.2371\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1219 - val_loss: 0.2405\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1197 - val_loss: 0.2474\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1203 - val_loss: 0.2616\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1298 - val_loss: 0.2585\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1336 - val_loss: 0.2633\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1403 - val_loss: 0.2627\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1244 - val_loss: 0.2647\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1203 - val_loss: 0.2615\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1334 - val_loss: 0.2616\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1491 - val_loss: 0.2630\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1379 - val_loss: 0.2787\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1368 - val_loss: 0.2702\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1262 - val_loss: 0.2837\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1279 - val_loss: 0.2684\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1362 - val_loss: 0.2638\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1291 - val_loss: 0.2715\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1377 - val_loss: 0.2715\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1261 - val_loss: 0.2772\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 305us/sample - loss: 0.1291 - val_loss: 0.2761\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1453 - val_loss: 0.2664\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1252 - val_loss: 0.2767\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.1165 - val_loss: 0.2655\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1352 - val_loss: 0.2620\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1274 - val_loss: 0.2651\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1151 - val_loss: 0.2590\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1307 - val_loss: 0.2638\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1231 - val_loss: 0.2768\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1221 - val_loss: 0.2749\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.1208 - val_loss: 0.2790\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1153 - val_loss: 0.2843\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1122 - val_loss: 0.2860\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1157 - val_loss: 0.2946\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.1178 - val_loss: 0.2911\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.1379 - val_loss: 0.2943\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1500 - val_loss: 0.2975\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1524 - val_loss: 0.3037\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1407 - val_loss: 0.2788\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1273 - val_loss: 0.2804\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1239 - val_loss: 0.2670\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1243 - val_loss: 0.2658\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.1253 - val_loss: 0.2654\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.1101 - val_loss: 0.2698\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1176 - val_loss: 0.2671\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.1137 - val_loss: 0.2709\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1103 - val_loss: 0.2731\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.1152 - val_loss: 0.2740\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1152 - val_loss: 0.2661\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1120 - val_loss: 0.2694\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.1035 - val_loss: 0.2716\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1094 - val_loss: 0.2743\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1065 - val_loss: 0.2721\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.1088 - val_loss: 0.2841\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.1097 - val_loss: 0.2871\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.1094 - val_loss: 0.2886\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.1086 - val_loss: 0.2780\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1113 - val_loss: 0.2829\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1435 - val_loss: 0.2823\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1151 - val_loss: 0.2876\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1143 - val_loss: 0.2848\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1130 - val_loss: 0.2846\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1123 - val_loss: 0.2845\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1170 - val_loss: 0.2839\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1137 - val_loss: 0.2891\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1200 - val_loss: 0.2920\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1255 - val_loss: 0.2877\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.1203 - val_loss: 0.2920\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1137 - val_loss: 0.2824\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.1154 - val_loss: 0.2888\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1342 - val_loss: 0.2893\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1816 - val_loss: 0.2867\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.1417 - val_loss: 0.3445\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.1659 - val_loss: 0.2781\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1487 - val_loss: 0.2573\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1190 - val_loss: 0.2605\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1247 - val_loss: 0.2415\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1214 - val_loss: 0.2392\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1224 - val_loss: 0.2473\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1138 - val_loss: 0.2616\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1246 - val_loss: 0.2573\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.1148 - val_loss: 0.2630\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1064 - val_loss: 0.2679\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1083 - val_loss: 0.2715\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1069 - val_loss: 0.2883\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1110 - val_loss: 0.2807\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1133 - val_loss: 0.2764\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1053 - val_loss: 0.2692\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1026 - val_loss: 0.2711\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1033 - val_loss: 0.2682\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1032 - val_loss: 0.2765\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1035 - val_loss: 0.2790\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0991 - val_loss: 0.2730\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1086 - val_loss: 0.2726\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1057 - val_loss: 0.2804\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1067 - val_loss: 0.2762\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1011 - val_loss: 0.2851\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1186 - val_loss: 0.2897\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.1006 - val_loss: 0.2867\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1138 - val_loss: 0.2832\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1084 - val_loss: 0.2899\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1051 - val_loss: 0.2901\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.1160 - val_loss: 0.2925\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1033 - val_loss: 0.2931\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1051 - val_loss: 0.2945\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1289 - val_loss: 0.2873\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1362 - val_loss: 0.2838\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1117 - val_loss: 0.3008\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1479 - val_loss: 0.2668\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.1334 - val_loss: 0.2851\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.1267 - val_loss: 0.2740\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.1197 - val_loss: 0.2701\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1278 - val_loss: 0.2951\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1388 - val_loss: 0.2824\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1108 - val_loss: 0.2713\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1419 - val_loss: 0.3025\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1751 - val_loss: 0.2951\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1567 - val_loss: 0.2623\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1222 - val_loss: 0.2782\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1301 - val_loss: 0.2541\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 164us/sample - loss: 0.1209 - val_loss: 0.2465\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.1208 - val_loss: 0.2731\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1074 - val_loss: 0.2725\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1066 - val_loss: 0.2860\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1223 - val_loss: 0.2866\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1078 - val_loss: 0.2827\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1088 - val_loss: 0.2939\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.1156 - val_loss: 0.2962\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1218 - val_loss: 0.2986\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1188 - val_loss: 0.2952\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1143 - val_loss: 0.2889\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1124 - val_loss: 0.2840\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1088 - val_loss: 0.2816\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1025 - val_loss: 0.2929\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1160 - val_loss: 0.2851\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1060 - val_loss: 0.2980\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1080 - val_loss: 0.2891\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1007 - val_loss: 0.2900\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1024 - val_loss: 0.2778\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1089 - val_loss: 0.2798\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.1016 - val_loss: 0.2799\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.0971 - val_loss: 0.2810\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 144us/sample - loss: 0.1021 - val_loss: 0.2928\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0956 - val_loss: 0.2907\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1034 - val_loss: 0.2877\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0962 - val_loss: 0.2845\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0933 - val_loss: 0.2762\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0964 - val_loss: 0.2818\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.0987 - val_loss: 0.2824\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.0957 - val_loss: 0.2899\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.0948 - val_loss: 0.2879\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1082 - val_loss: 0.2950\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1170 - val_loss: 0.2844\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0956 - val_loss: 0.2748\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1032 - val_loss: 0.2885\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1032 - val_loss: 0.2873\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0944 - val_loss: 0.2889\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1443 - val_loss: 0.2878\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1540 - val_loss: 0.3227\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1345 - val_loss: 0.2882\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.1539 - val_loss: 0.3082\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1364 - val_loss: 0.2947\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.1253 - val_loss: 0.2723\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1145 - val_loss: 0.2828\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1147 - val_loss: 0.2802\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1191 - val_loss: 0.2913\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1070 - val_loss: 0.2860\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1031 - val_loss: 0.2899\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1113 - val_loss: 0.2897\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1175 - val_loss: 0.2829\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1099 - val_loss: 0.2820\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1092 - val_loss: 0.2855\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 0.1251 - val_loss: 0.2742\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1031 - val_loss: 0.2900\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1032 - val_loss: 0.2750\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1022 - val_loss: 0.2875\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1005 - val_loss: 0.2813\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1083 - val_loss: 0.3093\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1116 - val_loss: 0.2994\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 0.0980 - val_loss: 0.2855\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.0991 - val_loss: 0.2888\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0962 - val_loss: 0.2943\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0966 - val_loss: 0.2941\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0946 - val_loss: 0.2934\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.0997 - val_loss: 0.2949\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1005 - val_loss: 0.2997\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0989 - val_loss: 0.3002\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.0991 - val_loss: 0.3096\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0987 - val_loss: 0.3075\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.0922 - val_loss: 0.3005\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.0914 - val_loss: 0.3069\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.0934 - val_loss: 0.3015\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0977 - val_loss: 0.3159\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 319us/sample - loss: 0.0926 - val_loss: 0.3124\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0891 - val_loss: 0.3175\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.0982 - val_loss: 0.2831\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1122 - val_loss: 0.2930\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1018 - val_loss: 0.2988\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1069 - val_loss: 0.3082\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.0957 - val_loss: 0.3065\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0978 - val_loss: 0.3086\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.0946 - val_loss: 0.3250\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.0943 - val_loss: 0.3260\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0941 - val_loss: 0.3202\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.0927 - val_loss: 0.3037\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0871 - val_loss: 0.3177\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 0.0925 - val_loss: 0.2923\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0919 - val_loss: 0.3112\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0923 - val_loss: 0.2916\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1015 - val_loss: 0.3263\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1126 - val_loss: 0.2831\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1268 - val_loss: 0.2950\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1057 - val_loss: 0.3030\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1248 - val_loss: 0.2961\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1389 - val_loss: 0.3047\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1182 - val_loss: 0.2993\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1092 - val_loss: 0.3147\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1171 - val_loss: 0.3027\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0933 - val_loss: 0.3032\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0989 - val_loss: 0.3198\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1082 - val_loss: 0.3038\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1015 - val_loss: 0.3422\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1301 - val_loss: 0.2889\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1754 - val_loss: 0.2922\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1524 - val_loss: 0.3661\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1897 - val_loss: 0.2537\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1507 - val_loss: 0.2659\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1497 - val_loss: 0.2609\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1386 - val_loss: 0.2519\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1269 - val_loss: 0.2623\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1154 - val_loss: 0.2675\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1044 - val_loss: 0.2544\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.1033 - val_loss: 0.2777\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0987 - val_loss: 0.2844\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.0956 - val_loss: 0.2871\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0941 - val_loss: 0.2994\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0988 - val_loss: 0.2831\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0980 - val_loss: 0.2915\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0987 - val_loss: 0.2839\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1364 - val_loss: 0.2966\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1494 - val_loss: 0.2995\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0994 - val_loss: 0.2515\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.1830 - val_loss: 0.2574\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1071 - val_loss: 0.3020\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1257 - val_loss: 0.2029\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1262 - val_loss: 0.2292\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1391 - val_loss: 0.2610\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1321 - val_loss: 0.2561\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1261 - val_loss: 0.2535\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1373 - val_loss: 0.2626\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1341 - val_loss: 0.2848\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1237 - val_loss: 0.2755\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 0.1387 - val_loss: 0.2787\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.1281 - val_loss: 0.2936\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1096 - val_loss: 0.2893\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.1190 - val_loss: 0.2931\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1109 - val_loss: 0.2879\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1032 - val_loss: 0.2801\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.1026 - val_loss: 0.2820\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.1112 - val_loss: 0.2898\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1122 - val_loss: 0.2989\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0961 - val_loss: 0.3054\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.0951 - val_loss: 0.2932\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1081 - val_loss: 0.2920\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1104 - val_loss: 0.2853\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.0962 - val_loss: 0.2942\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1188 - val_loss: 0.2992\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1201 - val_loss: 0.2944\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.1020 - val_loss: 0.2834\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1098 - val_loss: 0.2916\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1113 - val_loss: 0.2919\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.1045 - val_loss: 0.2950\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1039 - val_loss: 0.2775\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1221 - val_loss: 0.3136\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1212 - val_loss: 0.2826\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1283 - val_loss: 0.2812\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1015 - val_loss: 0.3220\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1115 - val_loss: 0.2924\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1055 - val_loss: 0.3045\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1040 - val_loss: 0.2921\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1007 - val_loss: 0.3154\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1032 - val_loss: 0.2877\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1256 - val_loss: 0.2977\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1192 - val_loss: 0.3075\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1264 - val_loss: 0.2795\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1368 - val_loss: 0.3183\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1453 - val_loss: 0.2783\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1317 - val_loss: 0.2602\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.1067 - val_loss: 0.2968\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1175 - val_loss: 0.2722\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1172 - val_loss: 0.2932\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1114 - val_loss: 0.2953\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1053 - val_loss: 0.2723\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1102 - val_loss: 0.3010\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1107 - val_loss: 0.2773\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1066 - val_loss: 0.2908\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1125 - val_loss: 0.2954\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0968 - val_loss: 0.2752\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1126 - val_loss: 0.2942\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1017 - val_loss: 0.2805\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0947 - val_loss: 0.2652\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1088 - val_loss: 0.2839\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.0940 - val_loss: 0.2895\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1022 - val_loss: 0.2915\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1065 - val_loss: 0.3092\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.0932 - val_loss: 0.2738\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1149 - val_loss: 0.3004\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1113 - val_loss: 0.2873\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0970 - val_loss: 0.2761\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0943 - val_loss: 0.3021\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.1025 - val_loss: 0.2847\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 285us/sample - loss: 0.1107 - val_loss: 0.3236\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1520 - val_loss: 0.3085\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.1329 - val_loss: 0.2966\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1151 - val_loss: 0.3057\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1694 - val_loss: 0.2813\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1050 - val_loss: 0.2756\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1427 - val_loss: 0.2930\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1382 - val_loss: 0.2822\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1143 - val_loss: 0.2551\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1226 - val_loss: 0.2641\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1095 - val_loss: 0.2649\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1048 - val_loss: 0.2437\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1135 - val_loss: 0.2715\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1029 - val_loss: 0.2738\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.0978 - val_loss: 0.2540\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1038 - val_loss: 0.2928\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0932 - val_loss: 0.2847\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0965 - val_loss: 0.2841\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0911 - val_loss: 0.2862\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.0954 - val_loss: 0.2839\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.0923 - val_loss: 0.2806\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.0933 - val_loss: 0.2971\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.0989 - val_loss: 0.2823\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.0897 - val_loss: 0.2871\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.0888 - val_loss: 0.2756\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0932 - val_loss: 0.2919\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0918 - val_loss: 0.2990\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0858 - val_loss: 0.2839\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0905 - val_loss: 0.2878\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0875 - val_loss: 0.2971\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.0860 - val_loss: 0.3107\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0902 - val_loss: 0.3038\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0832 - val_loss: 0.3005\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0895 - val_loss: 0.3118\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0915 - val_loss: 0.2882\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.0909 - val_loss: 0.2879\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1000 - val_loss: 0.2774\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1057 - val_loss: 0.2924\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1165 - val_loss: 0.3360\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1100 - val_loss: 0.2872\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0993 - val_loss: 0.3304\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.1343 - val_loss: 0.2680\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1174 - val_loss: 0.2804\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1023 - val_loss: 0.2996\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0895 - val_loss: 0.2837\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.0953 - val_loss: 0.2875\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0923 - val_loss: 0.2877\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0933 - val_loss: 0.3097\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0907 - val_loss: 0.2860\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1032 - val_loss: 0.3176\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0956 - val_loss: 0.2892\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.0873 - val_loss: 0.2841\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.0855 - val_loss: 0.3049\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0808 - val_loss: 0.2926\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0814 - val_loss: 0.3215\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.0819 - val_loss: 0.2844\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 0.0848 - val_loss: 0.3027\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0811 - val_loss: 0.2755\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1026 - val_loss: 0.2932\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 282us/sample - loss: 0.0771 - val_loss: 0.2705\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.0879 - val_loss: 0.3152\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1158 - val_loss: 0.2765\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1026 - val_loss: 0.3002\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.1043 - val_loss: 0.3439\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.0980 - val_loss: 0.2813\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.1552 - val_loss: 0.3355\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1483 - val_loss: 0.3320\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1936 - val_loss: 0.2840\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1314 - val_loss: 0.2760\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1650 - val_loss: 0.2550\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.1140 - val_loss: 0.2591\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.1105 - val_loss: 0.2632\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1018 - val_loss: 0.2611\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.1089 - val_loss: 0.2850\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.0975 - val_loss: 0.2772\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1071 - val_loss: 0.2868\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1038 - val_loss: 0.2763\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.0985 - val_loss: 0.2590\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.1045 - val_loss: 0.2657\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0882 - val_loss: 0.2617\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0916 - val_loss: 0.2944\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0904 - val_loss: 0.2942\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.0806 - val_loss: 0.2746\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0849 - val_loss: 0.3049\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0898 - val_loss: 0.2859\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0861 - val_loss: 0.3001\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0968 - val_loss: 0.2884\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.1166 - val_loss: 0.2886\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1016 - val_loss: 0.3245\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1241 - val_loss: 0.2935\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0796 - val_loss: 0.3010\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0963 - val_loss: 0.3013\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.0926 - val_loss: 0.2956\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.0841 - val_loss: 0.2839\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1008 - val_loss: 0.3061\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1033 - val_loss: 0.2887\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1079 - val_loss: 0.2651\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1148 - val_loss: 0.3492\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1209 - val_loss: 0.2856\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.1307 - val_loss: 0.2893\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.1235 - val_loss: 0.3017\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1061 - val_loss: 0.2451\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.1276 - val_loss: 0.2946\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1581 - val_loss: 0.2858\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1637 - val_loss: 0.2887\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1499 - val_loss: 0.3032\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.1329 - val_loss: 0.2742\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1219 - val_loss: 0.2942\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.1141 - val_loss: 0.2806\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.1013 - val_loss: 0.2855\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0968 - val_loss: 0.2981\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0899 - val_loss: 0.2829\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0982 - val_loss: 0.2815\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0860 - val_loss: 0.2873\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0836 - val_loss: 0.2959\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.0788 - val_loss: 0.2784\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.0846 - val_loss: 0.2814\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.0825 - val_loss: 0.2774\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0846 - val_loss: 0.2967\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.0948 - val_loss: 0.2851\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0856 - val_loss: 0.2888\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0998 - val_loss: 0.3060\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 0.0913 - val_loss: 0.2673\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.0947 - val_loss: 0.3031\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1019 - val_loss: 0.2935\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0968 - val_loss: 0.2777\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.0891 - val_loss: 0.3042\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0923 - val_loss: 0.2617\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0978 - val_loss: 0.2932\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.1173 - val_loss: 0.2705\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1030 - val_loss: 0.2547\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1004 - val_loss: 0.2991\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1151 - val_loss: 0.2693\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.0966 - val_loss: 0.2669\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0874 - val_loss: 0.2991\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.0860 - val_loss: 0.2957\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0878 - val_loss: 0.2932\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0762 - val_loss: 0.3126\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 0.0798 - val_loss: 0.2956\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 160us/sample - loss: 0.0795 - val_loss: 0.3161\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0735 - val_loss: 0.3059\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.0753 - val_loss: 0.3011\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.0771 - val_loss: 0.2925\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.0760 - val_loss: 0.2975\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.0800 - val_loss: 0.3172\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.0904 - val_loss: 0.3074\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0815 - val_loss: 0.2987\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.0722 - val_loss: 0.3148\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0785 - val_loss: 0.2883\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0777 - val_loss: 0.3031\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0732 - val_loss: 0.3032\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.0728 - val_loss: 0.2973\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 0.0707 - val_loss: 0.3014\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0722 - val_loss: 0.2999\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0697 - val_loss: 0.3202\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 164us/sample - loss: 0.0750 - val_loss: 0.3126\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.0723 - val_loss: 0.3117\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0705 - val_loss: 0.3129\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0728 - val_loss: 0.2959\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.0690 - val_loss: 0.2982\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.0684 - val_loss: 0.3062\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.0722 - val_loss: 0.3157\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.0691 - val_loss: 0.3098\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.0755 - val_loss: 0.2997\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0683 - val_loss: 0.3085\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0730 - val_loss: 0.2910\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.0722 - val_loss: 0.3135\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.0708 - val_loss: 0.3085\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0688 - val_loss: 0.3004\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0704 - val_loss: 0.3159\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0699 - val_loss: 0.3029\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.0755 - val_loss: 0.3439\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1219 - val_loss: 0.3215\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.1252 - val_loss: 0.3244\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1207 - val_loss: 0.3403\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1171 - val_loss: 0.3063\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.1398 - val_loss: 0.2963\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 156us/sample - loss: 0.1465 - val_loss: 0.3013\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.2062 - val_loss: 0.2582\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1527 - val_loss: 0.2609\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.1223 - val_loss: 0.2735\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1203 - val_loss: 0.2850\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1091 - val_loss: 0.2849\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1182 - val_loss: 0.2991\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1072 - val_loss: 0.2802\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 164us/sample - loss: 0.1025 - val_loss: 0.2945\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0942 - val_loss: 0.3101\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0943 - val_loss: 0.2836\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0978 - val_loss: 0.3205\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1019 - val_loss: 0.2869\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0912 - val_loss: 0.2998\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.0905 - val_loss: 0.2956\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0855 - val_loss: 0.3002\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0869 - val_loss: 0.3080\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.0861 - val_loss: 0.2985\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0910 - val_loss: 0.3037\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.0869 - val_loss: 0.2980\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 158us/sample - loss: 0.0874 - val_loss: 0.3035\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.0817 - val_loss: 0.3045\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.0824 - val_loss: 0.3001\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0843 - val_loss: 0.3131\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.0817 - val_loss: 0.3085\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0886 - val_loss: 0.3098\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.0971 - val_loss: 0.3137\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 165us/sample - loss: 0.0945 - val_loss: 0.3018\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0882 - val_loss: 0.3141\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.0862 - val_loss: 0.2932\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0818 - val_loss: 0.3274\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.0871 - val_loss: 0.2848\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0899 - val_loss: 0.3234\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.0993 - val_loss: 0.2913\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.1007 - val_loss: 0.2844\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0938 - val_loss: 0.3170\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.0847 - val_loss: 0.2599\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.1207 - val_loss: 0.3390\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.1445 - val_loss: 0.3029\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1065 - val_loss: 0.2557\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1196 - val_loss: 0.2777\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1150 - val_loss: 0.2526\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.1067 - val_loss: 0.2552\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.1030 - val_loss: 0.2872\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1065 - val_loss: 0.2326\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.0930 - val_loss: 0.3150\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.1000 - val_loss: 0.2609\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 0.0949 - val_loss: 0.2847\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0853 - val_loss: 0.2826\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0844 - val_loss: 0.2639\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.0887 - val_loss: 0.2816\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0829 - val_loss: 0.2806\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0824 - val_loss: 0.3082\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0828 - val_loss: 0.2838\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0852 - val_loss: 0.2970\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0785 - val_loss: 0.3135\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.0810 - val_loss: 0.2846\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0811 - val_loss: 0.3071\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.0874 - val_loss: 0.2767\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0803 - val_loss: 0.3036\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.0854 - val_loss: 0.2867\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0882 - val_loss: 0.2887\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.0860 - val_loss: 0.2954\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.0838 - val_loss: 0.2908\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0846 - val_loss: 0.2922\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0855 - val_loss: 0.2856\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0822 - val_loss: 0.2884\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0922 - val_loss: 0.2633\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0826 - val_loss: 0.2845\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0861 - val_loss: 0.2612\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0788 - val_loss: 0.2970\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0859 - val_loss: 0.2668\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0854 - val_loss: 0.3368\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.0975 - val_loss: 0.2866\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0886 - val_loss: 0.3049\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 169us/sample - loss: 0.0919 - val_loss: 0.3056\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.0853 - val_loss: 0.2995\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.0954 - val_loss: 0.3170\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0898 - val_loss: 0.2722\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0883 - val_loss: 0.3447\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1120 - val_loss: 0.3056\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.0954 - val_loss: 0.2860\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 161us/sample - loss: 0.0893 - val_loss: 0.3037\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0954 - val_loss: 0.2530\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.0963 - val_loss: 0.3059\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.0997 - val_loss: 0.2614\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0993 - val_loss: 0.2978\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1018 - val_loss: 0.2637\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1090 - val_loss: 0.2541\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0925 - val_loss: 0.2965\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0969 - val_loss: 0.2493\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1015 - val_loss: 0.3166\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1029 - val_loss: 0.2694\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0917 - val_loss: 0.2755\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0786 - val_loss: 0.3073\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.0928 - val_loss: 0.2462\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.1400 - val_loss: 0.2641\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.1234 - val_loss: 0.3239\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.1290 - val_loss: 0.2420\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.1344 - val_loss: 0.2490\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0991 - val_loss: 0.2765\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.1456 - val_loss: 0.2444\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.1137 - val_loss: 0.2370\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.1082 - val_loss: 0.2656\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1060 - val_loss: 0.2723\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.1004 - val_loss: 0.2539\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1220 - val_loss: 0.2872\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.1144 - val_loss: 0.2888\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0989 - val_loss: 0.2822\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1016 - val_loss: 0.3235\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1066 - val_loss: 0.2951\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.1050 - val_loss: 0.2891\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.0988 - val_loss: 0.2938\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0932 - val_loss: 0.2807\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 0.0904 - val_loss: 0.3240\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.1025 - val_loss: 0.2870\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.0998 - val_loss: 0.2928\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.1019 - val_loss: 0.3233\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.1021 - val_loss: 0.2684\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1256 - val_loss: 0.3038\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.0884 - val_loss: 0.2764\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.0914 - val_loss: 0.2992\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.0935 - val_loss: 0.2629\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.1054 - val_loss: 0.2787\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.1030 - val_loss: 0.3049\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0885 - val_loss: 0.2635\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1037 - val_loss: 0.3197\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.0961 - val_loss: 0.2708\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1024 - val_loss: 0.2766\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.0816 - val_loss: 0.3037\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.0947 - val_loss: 0.2492\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.0930 - val_loss: 0.2649\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.1100 - val_loss: 0.3004\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0803 - val_loss: 0.2797\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1405 - val_loss: 0.3309\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.1257 - val_loss: 0.3127\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.1059 - val_loss: 0.3066\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.1328 - val_loss: 0.3121\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.1308 - val_loss: 0.2667\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1240 - val_loss: 0.3016\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.1565 - val_loss: 0.2478\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.1642 - val_loss: 0.2440\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1411 - val_loss: 0.3235\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1358 - val_loss: 0.2655\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1234 - val_loss: 0.3053\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.1072 - val_loss: 0.3230\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1368 - val_loss: 0.2940\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.1302 - val_loss: 0.2736\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1290 - val_loss: 0.3331\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.1235 - val_loss: 0.2660\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1317 - val_loss: 0.2829\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1201 - val_loss: 0.3315\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.1212 - val_loss: 0.2827\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 178us/sample - loss: 0.1271 - val_loss: 0.3057\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1102 - val_loss: 0.3274\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.1106 - val_loss: 0.2801\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.1040 - val_loss: 0.3041\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0978 - val_loss: 0.2887\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 0.0976 - val_loss: 0.2614\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.0985 - val_loss: 0.3349\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.0978 - val_loss: 0.2628\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.1190 - val_loss: 0.3009\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.1000 - val_loss: 0.3365\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.0910 - val_loss: 0.2827\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.1196 - val_loss: 0.3272\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 174us/sample - loss: 0.1122 - val_loss: 0.3078\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.0970 - val_loss: 0.2385\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1131 - val_loss: 0.2836\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 173us/sample - loss: 0.1103 - val_loss: 0.2969\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.0892 - val_loss: 0.2462\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.1689 - val_loss: 0.2413\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.1159 - val_loss: 0.2947\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.1197 - val_loss: 0.2289\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.1196 - val_loss: 0.2231\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.1147 - val_loss: 0.2792\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.1090 - val_loss: 0.2745\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.1186 - val_loss: 0.2950\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1011 - val_loss: 0.3027\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0965 - val_loss: 0.3029\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.0927 - val_loss: 0.3216\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.0960 - val_loss: 0.2709\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.1176 - val_loss: 0.3046\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.1113 - val_loss: 0.3083\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 177us/sample - loss: 0.1028 - val_loss: 0.2654\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.1088 - val_loss: 0.3134\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.1104 - val_loss: 0.3122\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.0921 - val_loss: 0.2760\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.0947 - val_loss: 0.3280\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0997 - val_loss: 0.2972\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0969 - val_loss: 0.2858\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.0874 - val_loss: 0.3258\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.1013 - val_loss: 0.2811\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.0912 - val_loss: 0.3060\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.0861 - val_loss: 0.3274\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.0895 - val_loss: 0.3017\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.0911 - val_loss: 0.3134\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.0892 - val_loss: 0.3194\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.0849 - val_loss: 0.2703\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.0909 - val_loss: 0.3029\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0840 - val_loss: 0.2926\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.0824 - val_loss: 0.3043\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0802 - val_loss: 0.2912\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.0822 - val_loss: 0.2982\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.0777 - val_loss: 0.3190\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.0873 - val_loss: 0.2912\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.0759 - val_loss: 0.3008\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0790 - val_loss: 0.2977\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0778 - val_loss: 0.2879\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 166us/sample - loss: 0.0822 - val_loss: 0.2832\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.0743 - val_loss: 0.3146\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.0819 - val_loss: 0.2780\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.1037 - val_loss: 0.2926\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.0734 - val_loss: 0.3308\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.0834 - val_loss: 0.2710\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.0931 - val_loss: 0.3296\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.0829 - val_loss: 0.2982\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.0735 - val_loss: 0.3036\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0699 - val_loss: 0.3409\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 172us/sample - loss: 0.0796 - val_loss: 0.2877\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.0823 - val_loss: 0.3183\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 176us/sample - loss: 0.0746 - val_loss: 0.2941\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.0788 - val_loss: 0.2848\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0776 - val_loss: 0.3077\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.0749 - val_loss: 0.3036\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 0.0704 - val_loss: 0.3161\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.0674 - val_loss: 0.3186\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 0.0682 - val_loss: 0.3390\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.0689 - val_loss: 0.3171\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.0698 - val_loss: 0.3271\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.0721 - val_loss: 0.3093\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 184us/sample - loss: 0.0849 - val_loss: 0.2836\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.0824 - val_loss: 0.2780\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 180us/sample - loss: 0.0715 - val_loss: 0.3325\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 0.0796 - val_loss: 0.2856\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 0.0772 - val_loss: 0.3096\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0715 - val_loss: 0.3287\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 167us/sample - loss: 0.0737 - val_loss: 0.3044\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0657 - val_loss: 0.3413\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.0716 - val_loss: 0.3217\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 175us/sample - loss: 0.0690 - val_loss: 0.3168\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.0697 - val_loss: 0.3412\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.0693 - val_loss: 0.3173\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0638 - val_loss: 0.3031\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0646 - val_loss: 0.3321\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.0735 - val_loss: 0.2837\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.0700 - val_loss: 0.3186\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0698 - val_loss: 0.2913\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.0677 - val_loss: 0.3138\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.0722 - val_loss: 0.3262\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0660 - val_loss: 0.3160\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.0693 - val_loss: 0.3459\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0633 - val_loss: 0.3323\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.0651 - val_loss: 0.3406\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.0630 - val_loss: 0.3220\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 179us/sample - loss: 0.0660 - val_loss: 0.3455\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.0705 - val_loss: 0.3316\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.0654 - val_loss: 0.3150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f12302f0898>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h/30l9oQl/90AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deep_neural_network():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=1000,input_shape=(1,),activation='relu'))# input connected to output\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1000,activation='relu'))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model\n",
    "\n",
    "# calling baseline model\n",
    "deep_nn = deep_neural_network()\n",
    "print(deep_nn.summary())\n",
    "# initializing tensorboard\n",
    "tfb = TensorBoard('deep_neural_network')\n",
    "# Training Model \n",
    "history = deep_nn.fit(x=x_train,y=y_train,batch_size=None,epochs=1000,callbacks=[tfb],validation_data=[x_test,y_test])\n",
    "# loading into data\n",
    "data_loss_nn = pd.DataFrame(history.history)\n",
    "data_loss_nn.plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "vNtXyDyGsfAY",
    "outputId": "0248d207-852c-4d44-d6d7-2ff25447d75b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f11ea0520b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQrSCCGEEEJUiII0QgghhBAV+v8NcQiJXLQz2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting results\n",
    "y_pred_dnn= deep_nn.predict(X)\n",
    "plotting()\n",
    "# plt.plot(X,y_pred_base) # plot regression line\n",
    "# plt.plot(X,y_pred_nn) # plot regression line\n",
    "plt.plot(X,y_pred_dnn) \n",
    "plt.legend(['baseline model','Neural Network','Deep Neural Network'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0m3QLtDnuXom"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjwNdIfkuytA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Pq8gKPwBtS5z",
    "outputId": "ce019608-dfdb-47a2-d1b9-22af90b6f79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 1000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 6,009,001\n",
      "Trainable params: 6,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 1s 9ms/sample - loss: 58.3636 - val_loss: 51.7239\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 49.6190 - val_loss: 43.7639\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 41.8629 - val_loss: 36.6639\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 34.9908 - val_loss: 30.4776\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 29.0099 - val_loss: 25.1281\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 23.8675 - val_loss: 20.5446\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 19.4641 - val_loss: 16.6451\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 15.7072 - val_loss: 13.4112\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 12.6894 - val_loss: 10.8636\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 10.5093 - val_loss: 8.9933\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 8.5105 - val_loss: 7.1925\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 6.7953 - val_loss: 5.7547\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 5.4381 - val_loss: 4.6319\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 4.3313 - val_loss: 3.6774\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 3.4429 - val_loss: 2.9404\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 2.7457 - val_loss: 2.3703\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 2.2226 - val_loss: 1.9451\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 1.8238 - val_loss: 1.6932\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 1.5047 - val_loss: 1.3196\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 1.2353 - val_loss: 1.1130\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 1.0492 - val_loss: 0.9573\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 0.9146 - val_loss: 0.8397\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.7949 - val_loss: 0.7665\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.7277 - val_loss: 0.6940\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.6385 - val_loss: 0.6401\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.5928 - val_loss: 0.5999\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.5519 - val_loss: 0.5653\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.5175 - val_loss: 0.5606\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.5177 - val_loss: 0.5569\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.5628 - val_loss: 0.6574\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.5810 - val_loss: 0.4906\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.4791 - val_loss: 0.5297\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.5541 - val_loss: 0.5026\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.5031 - val_loss: 0.4746\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.4514 - val_loss: 0.4967\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.4515 - val_loss: 0.4600\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.4323 - val_loss: 0.4460\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.4146 - val_loss: 0.4500\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.4119 - val_loss: 0.4297\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.4038 - val_loss: 0.4375\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.4068 - val_loss: 0.4260\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.4099 - val_loss: 0.4083\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.3780 - val_loss: 0.4200\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.3795 - val_loss: 0.3934\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.3672 - val_loss: 0.3930\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.3582 - val_loss: 0.3885\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.3527 - val_loss: 0.3971\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.3599 - val_loss: 0.3902\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.3394 - val_loss: 0.3990\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.3521 - val_loss: 0.3844\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.3719 - val_loss: 0.3995\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.3639 - val_loss: 0.3808\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.3460 - val_loss: 0.3954\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.3501 - val_loss: 0.3802\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.3353 - val_loss: 0.3781\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 0.3484 - val_loss: 0.3745\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.3373 - val_loss: 0.3775\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.3358 - val_loss: 0.3860\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.3363 - val_loss: 0.3849\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.3338 - val_loss: 0.3638\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.3371 - val_loss: 0.3805\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.3342 - val_loss: 0.3718\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.3240 - val_loss: 0.3986\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 0.4040 - val_loss: 0.3593\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.3489 - val_loss: 0.4101\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 0.3441 - val_loss: 0.4039\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.3546 - val_loss: 0.3791\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.3463 - val_loss: 0.3977\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.3434 - val_loss: 0.3583\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.3355 - val_loss: 0.3802\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.3363 - val_loss: 0.3778\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.3601 - val_loss: 0.3625\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.3678 - val_loss: 0.3592\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 0.3683 - val_loss: 0.4243\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.3581 - val_loss: 0.4086\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.3367 - val_loss: 0.3550\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.3639 - val_loss: 0.3622\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 0.3425 - val_loss: 0.3953\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.3454 - val_loss: 0.3574\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.3253 - val_loss: 0.3771\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.3390 - val_loss: 0.3730\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.3345 - val_loss: 0.3634\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.3115 - val_loss: 0.3586\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.3225 - val_loss: 0.3567\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.3178 - val_loss: 0.3518\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.3192 - val_loss: 0.3675\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.3392 - val_loss: 0.3592\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.3217 - val_loss: 0.3908\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.3315 - val_loss: 0.3867\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.3297 - val_loss: 0.3481\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.3076 - val_loss: 0.3487\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.3123 - val_loss: 0.3455\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.3161 - val_loss: 0.3560\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.3178 - val_loss: 0.3650\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.3134 - val_loss: 0.4370\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 305us/sample - loss: 0.3785 - val_loss: 0.3624\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.3267 - val_loss: 0.3912\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.3289 - val_loss: 0.3701\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.3248 - val_loss: 0.3451\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.3104 - val_loss: 0.3492\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.3202 - val_loss: 0.3486\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.3373 - val_loss: 0.3431\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.3234 - val_loss: 0.3628\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.3440 - val_loss: 0.4171\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.3696 - val_loss: 0.3642\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.3323 - val_loss: 0.3957\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.3224 - val_loss: 0.3702\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.3561 - val_loss: 0.3456\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.3424 - val_loss: 0.3951\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.3431 - val_loss: 0.3771\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 0.3337 - val_loss: 0.3459\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.3138 - val_loss: 0.3692\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.3185 - val_loss: 0.3542\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.3122 - val_loss: 0.3475\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 296us/sample - loss: 0.3079 - val_loss: 0.3434\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.3043 - val_loss: 0.3448\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2984 - val_loss: 0.3426\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.3069 - val_loss: 0.3461\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2996 - val_loss: 0.3626\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 0.3051 - val_loss: 0.3399\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.3003 - val_loss: 0.3391\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.3005 - val_loss: 0.3393\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.3017 - val_loss: 0.3505\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2995 - val_loss: 0.3457\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2940 - val_loss: 0.3578\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3025 - val_loss: 0.3466\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.3001 - val_loss: 0.3399\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.3081 - val_loss: 0.3625\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.3101 - val_loss: 0.3375\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.3049 - val_loss: 0.3438\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.3101 - val_loss: 0.3527\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2976 - val_loss: 0.3449\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.3131 - val_loss: 0.3613\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 416us/sample - loss: 0.3063 - val_loss: 0.3479\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.3058 - val_loss: 0.3323\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.3133 - val_loss: 0.3419\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2943 - val_loss: 0.3867\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.3104 - val_loss: 0.3331\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.3118 - val_loss: 0.3390\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.3057 - val_loss: 0.3534\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 293us/sample - loss: 0.2916 - val_loss: 0.3380\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.3069 - val_loss: 0.3564\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.3132 - val_loss: 0.3457\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.3078 - val_loss: 0.3670\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.3108 - val_loss: 0.3473\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2976 - val_loss: 0.3443\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2996 - val_loss: 0.3413\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2920 - val_loss: 0.3490\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.3004 - val_loss: 0.3395\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2975 - val_loss: 0.3430\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2968 - val_loss: 0.3667\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.3076 - val_loss: 0.3494\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.3276 - val_loss: 0.3906\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.3362 - val_loss: 0.3422\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.3104 - val_loss: 0.3416\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.3368 - val_loss: 0.3409\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.3112 - val_loss: 0.4582\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.3705 - val_loss: 0.4131\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.3342 - val_loss: 0.3484\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.3441 - val_loss: 0.3405\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 0.3346 - val_loss: 0.3867\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.3263 - val_loss: 0.3524\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.3244 - val_loss: 0.3468\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.3162 - val_loss: 0.3437\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.3052 - val_loss: 0.3464\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.3002 - val_loss: 0.3426\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.3013 - val_loss: 0.3438\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2918 - val_loss: 0.3352\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2971 - val_loss: 0.3559\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.3099 - val_loss: 0.3420\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2974 - val_loss: 0.3257\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2955 - val_loss: 0.3565\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.3007 - val_loss: 0.3255\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.3079 - val_loss: 0.3340\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.3037 - val_loss: 0.3511\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2923 - val_loss: 0.3387\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 0.3040 - val_loss: 0.3495\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2979 - val_loss: 0.3427\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2971 - val_loss: 0.3404\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2907 - val_loss: 0.3609\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2996 - val_loss: 0.3414\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2822 - val_loss: 0.3540\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2875 - val_loss: 0.3348\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2911 - val_loss: 0.3583\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2982 - val_loss: 0.3370\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2993 - val_loss: 0.3842\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.3097 - val_loss: 0.3418\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.3133 - val_loss: 0.3502\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.3037 - val_loss: 0.3465\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2985 - val_loss: 0.3731\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.3212 - val_loss: 0.3278\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2922 - val_loss: 0.3374\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2981 - val_loss: 0.3368\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2961 - val_loss: 0.3419\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2906 - val_loss: 0.3527\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2928 - val_loss: 0.3304\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2829 - val_loss: 0.3374\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2892 - val_loss: 0.3464\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2847 - val_loss: 0.3410\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2896 - val_loss: 0.3535\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2852 - val_loss: 0.3378\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2852 - val_loss: 0.3329\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2841 - val_loss: 0.3379\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2928 - val_loss: 0.3436\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2936 - val_loss: 0.3476\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.2846 - val_loss: 0.3944\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2976 - val_loss: 0.3578\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.3580 - val_loss: 0.3664\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.3110 - val_loss: 0.4300\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.3378 - val_loss: 0.3520\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.3188 - val_loss: 0.3555\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.3012 - val_loss: 0.3442\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 0.2862 - val_loss: 0.3322\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.3147 - val_loss: 0.3281\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2933 - val_loss: 0.3515\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.3015 - val_loss: 0.3469\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2881 - val_loss: 0.3327\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2899 - val_loss: 0.3314\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.2796 - val_loss: 0.3573\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.2859 - val_loss: 0.3325\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2903 - val_loss: 0.3561\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2967 - val_loss: 0.3355\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.2989 - val_loss: 0.3383\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2969 - val_loss: 0.3675\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.3048 - val_loss: 0.3494\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3274 - val_loss: 0.3553\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.2902 - val_loss: 0.3752\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.3248 - val_loss: 0.3243\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2912 - val_loss: 0.3291\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2897 - val_loss: 0.3360\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2829 - val_loss: 0.3465\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2965 - val_loss: 0.3359\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2879 - val_loss: 0.3479\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2883 - val_loss: 0.3412\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2912 - val_loss: 0.3338\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2862 - val_loss: 0.3374\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2745 - val_loss: 0.3338\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2903 - val_loss: 0.3347\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2773 - val_loss: 0.3274\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2837 - val_loss: 0.3450\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2776 - val_loss: 0.3301\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2918 - val_loss: 0.3465\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2939 - val_loss: 0.3221\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.3016 - val_loss: 0.3408\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.3101 - val_loss: 0.3721\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.3037 - val_loss: 0.3444\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2829 - val_loss: 0.3420\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2876 - val_loss: 0.3360\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2767 - val_loss: 0.3416\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 0.2881 - val_loss: 0.3381\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.2777 - val_loss: 0.3350\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2714 - val_loss: 0.3359\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2838 - val_loss: 0.3267\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2745 - val_loss: 0.3331\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2935 - val_loss: 0.3514\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2904 - val_loss: 0.3448\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2742 - val_loss: 0.3384\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.3030 - val_loss: 0.3293\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2763 - val_loss: 0.3365\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2761 - val_loss: 0.3387\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2819 - val_loss: 0.3301\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 0.2721 - val_loss: 0.3263\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2748 - val_loss: 0.3533\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 315us/sample - loss: 0.2856 - val_loss: 0.3342\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2897 - val_loss: 0.3325\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2687 - val_loss: 0.3751\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2926 - val_loss: 0.3343\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.3017 - val_loss: 0.3298\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2742 - val_loss: 0.3861\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.3075 - val_loss: 0.3517\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.3024 - val_loss: 0.3306\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2870 - val_loss: 0.3512\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2889 - val_loss: 0.3327\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2870 - val_loss: 0.3302\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2764 - val_loss: 0.3455\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2736 - val_loss: 0.3293\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2935 - val_loss: 0.3398\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2919 - val_loss: 0.3356\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2929 - val_loss: 0.3759\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.3060 - val_loss: 0.3731\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.3039 - val_loss: 0.3305\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2792 - val_loss: 0.3275\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2890 - val_loss: 0.3422\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 0.2747 - val_loss: 0.3419\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2831 - val_loss: 0.3402\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2774 - val_loss: 0.3270\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2662 - val_loss: 0.3510\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.3075 - val_loss: 0.3343\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2807 - val_loss: 0.3298\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2874 - val_loss: 0.3435\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2802 - val_loss: 0.3551\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2899 - val_loss: 0.3285\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2833 - val_loss: 0.3692\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2940 - val_loss: 0.3305\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2753 - val_loss: 0.3428\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2826 - val_loss: 0.3552\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2802 - val_loss: 0.3378\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2792 - val_loss: 0.3530\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2779 - val_loss: 0.3305\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2731 - val_loss: 0.3479\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2726 - val_loss: 0.3493\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2736 - val_loss: 0.3338\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2719 - val_loss: 0.3224\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2770 - val_loss: 0.3309\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.2746 - val_loss: 0.3519\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.3107 - val_loss: 0.3554\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.3094 - val_loss: 0.3351\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2696 - val_loss: 0.3640\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.3272 - val_loss: 0.3610\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.2764 - val_loss: 0.3586\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.3181 - val_loss: 0.3808\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 0.3275 - val_loss: 0.3478\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.3112 - val_loss: 0.3670\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 319us/sample - loss: 0.3142 - val_loss: 0.3563\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.3014 - val_loss: 0.3433\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2921 - val_loss: 0.3828\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.3217 - val_loss: 0.3442\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2902 - val_loss: 0.3293\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2772 - val_loss: 0.3610\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2964 - val_loss: 0.3275\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2829 - val_loss: 0.3357\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.2705 - val_loss: 0.3623\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2914 - val_loss: 0.3373\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2751 - val_loss: 0.3281\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2699 - val_loss: 0.3393\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2829 - val_loss: 0.3183\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2926 - val_loss: 0.3284\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2841 - val_loss: 0.3612\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2798 - val_loss: 0.3312\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2798 - val_loss: 0.3338\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2733 - val_loss: 0.3461\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2747 - val_loss: 0.3342\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2716 - val_loss: 0.3446\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2695 - val_loss: 0.3315\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2853 - val_loss: 0.3462\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2830 - val_loss: 0.3667\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2825 - val_loss: 0.3361\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 0.3217 - val_loss: 0.3316\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2675 - val_loss: 0.3944\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.3165 - val_loss: 0.3171\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2870 - val_loss: 0.3383\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2864 - val_loss: 0.3528\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2973 - val_loss: 0.3433\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2795 - val_loss: 0.3361\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2842 - val_loss: 0.3330\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2830 - val_loss: 0.3546\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2789 - val_loss: 0.3300\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2830 - val_loss: 0.3276\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2686 - val_loss: 0.3576\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2798 - val_loss: 0.3269\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2843 - val_loss: 0.3387\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 0.2818 - val_loss: 0.3509\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2723 - val_loss: 0.3437\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2768 - val_loss: 0.3259\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2723 - val_loss: 0.3413\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2690 - val_loss: 0.3261\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2800 - val_loss: 0.3215\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2615 - val_loss: 0.3522\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2692 - val_loss: 0.3235\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2781 - val_loss: 0.3186\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2763 - val_loss: 0.3362\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2667 - val_loss: 0.3195\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2817 - val_loss: 0.3344\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 0.2691 - val_loss: 0.3162\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 0.2732 - val_loss: 0.3208\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2654 - val_loss: 0.3343\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2632 - val_loss: 0.3173\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2680 - val_loss: 0.3446\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2741 - val_loss: 0.3321\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2646 - val_loss: 0.3282\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2648 - val_loss: 0.3351\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2641 - val_loss: 0.3439\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2683 - val_loss: 0.3315\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2594 - val_loss: 0.3441\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2653 - val_loss: 0.3303\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.2707 - val_loss: 0.3235\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2648 - val_loss: 0.3269\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2735 - val_loss: 0.3305\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2693 - val_loss: 0.3600\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2701 - val_loss: 0.3403\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2902 - val_loss: 0.3518\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2855 - val_loss: 0.3519\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2918 - val_loss: 0.3332\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2828 - val_loss: 0.3480\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2699 - val_loss: 0.3285\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.3030 - val_loss: 0.3271\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2853 - val_loss: 0.3616\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2728 - val_loss: 0.3393\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2909 - val_loss: 0.3297\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2652 - val_loss: 0.3442\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2757 - val_loss: 0.3234\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2685 - val_loss: 0.3245\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2628 - val_loss: 0.3570\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2861 - val_loss: 0.3324\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 0.2767 - val_loss: 0.3190\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2842 - val_loss: 0.3256\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2793 - val_loss: 0.3383\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2984 - val_loss: 0.3499\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2726 - val_loss: 0.3356\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2865 - val_loss: 0.3497\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2985 - val_loss: 0.3407\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2656 - val_loss: 0.3361\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2846 - val_loss: 0.3266\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2683 - val_loss: 0.3718\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2828 - val_loss: 0.3384\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2757 - val_loss: 0.3335\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2721 - val_loss: 0.3452\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2689 - val_loss: 0.3288\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2624 - val_loss: 0.3268\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2658 - val_loss: 0.3367\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2625 - val_loss: 0.3309\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2646 - val_loss: 0.3270\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2655 - val_loss: 0.3228\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2748 - val_loss: 0.3340\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 0.2764 - val_loss: 0.3352\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2660 - val_loss: 0.3317\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2756 - val_loss: 0.3487\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2722 - val_loss: 0.3245\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2684 - val_loss: 0.3284\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2757 - val_loss: 0.3549\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2684 - val_loss: 0.3350\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2739 - val_loss: 0.3441\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2792 - val_loss: 0.3384\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2566 - val_loss: 0.3224\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2724 - val_loss: 0.3375\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2800 - val_loss: 0.3482\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2677 - val_loss: 0.3344\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.2773 - val_loss: 0.3387\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2634 - val_loss: 0.3670\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2711 - val_loss: 0.3253\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.2693 - val_loss: 0.3387\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2601 - val_loss: 0.3554\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2658 - val_loss: 0.3259\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2609 - val_loss: 0.3407\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2601 - val_loss: 0.3326\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2594 - val_loss: 0.3322\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.2656 - val_loss: 0.3391\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.2607 - val_loss: 0.3260\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2685 - val_loss: 0.3252\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.2673 - val_loss: 0.3428\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2833 - val_loss: 0.3185\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2664 - val_loss: 0.3521\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2694 - val_loss: 0.3225\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2693 - val_loss: 0.3306\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2670 - val_loss: 0.3288\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2664 - val_loss: 0.3316\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2648 - val_loss: 0.3471\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2655 - val_loss: 0.3438\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2584 - val_loss: 0.3239\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2594 - val_loss: 0.3287\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2600 - val_loss: 0.3284\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2537 - val_loss: 0.3214\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2583 - val_loss: 0.3279\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2588 - val_loss: 0.3324\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.2612 - val_loss: 0.3244\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2519 - val_loss: 0.3252\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2649 - val_loss: 0.3326\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2572 - val_loss: 0.3236\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 0.2635 - val_loss: 0.3402\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2831 - val_loss: 0.3427\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2673 - val_loss: 0.3208\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2762 - val_loss: 0.3492\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2647 - val_loss: 0.3197\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 295us/sample - loss: 0.2598 - val_loss: 0.3346\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 0.2611 - val_loss: 0.3256\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2675 - val_loss: 0.3171\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 0.2580 - val_loss: 0.3577\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2723 - val_loss: 0.3146\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2640 - val_loss: 0.3220\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2531 - val_loss: 0.3487\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2642 - val_loss: 0.3220\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2612 - val_loss: 0.3178\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2562 - val_loss: 0.3503\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2595 - val_loss: 0.3261\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2568 - val_loss: 0.3280\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2627 - val_loss: 0.3410\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2560 - val_loss: 0.3225\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2636 - val_loss: 0.3293\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.2654 - val_loss: 0.3251\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2524 - val_loss: 0.3521\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2701 - val_loss: 0.3291\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2550 - val_loss: 0.3314\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2538 - val_loss: 0.3253\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2590 - val_loss: 0.3235\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2542 - val_loss: 0.3355\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2652 - val_loss: 0.3280\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2668 - val_loss: 0.3413\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2562 - val_loss: 0.3281\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2585 - val_loss: 0.3319\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.2589 - val_loss: 0.3190\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2650 - val_loss: 0.3208\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2676 - val_loss: 0.3308\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2802 - val_loss: 0.3362\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2593 - val_loss: 0.3671\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2811 - val_loss: 0.3245\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2570 - val_loss: 0.3157\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2562 - val_loss: 0.3484\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2679 - val_loss: 0.3270\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2529 - val_loss: 0.3207\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2610 - val_loss: 0.3327\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2596 - val_loss: 0.3390\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2527 - val_loss: 0.3327\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2744 - val_loss: 0.3394\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2783 - val_loss: 0.3355\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.2479 - val_loss: 0.3294\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2929 - val_loss: 0.3299\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2780 - val_loss: 0.3350\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2604 - val_loss: 0.3344\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2625 - val_loss: 0.3255\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2626 - val_loss: 0.3348\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2641 - val_loss: 0.3467\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2630 - val_loss: 0.3223\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2531 - val_loss: 0.3499\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2762 - val_loss: 0.3185\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 0.2606 - val_loss: 0.3183\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 0.2619 - val_loss: 0.3391\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.2557 - val_loss: 0.3172\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2560 - val_loss: 0.3356\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2646 - val_loss: 0.3176\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2563 - val_loss: 0.3234\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2525 - val_loss: 0.3450\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 0.2695 - val_loss: 0.3266\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2965 - val_loss: 0.3211\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2748 - val_loss: 0.3564\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2742 - val_loss: 0.3290\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2758 - val_loss: 0.3297\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2584 - val_loss: 0.3299\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2641 - val_loss: 0.3229\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2596 - val_loss: 0.3268\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2591 - val_loss: 0.3230\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2546 - val_loss: 0.3331\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2559 - val_loss: 0.3385\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2643 - val_loss: 0.3206\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2574 - val_loss: 0.3306\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2505 - val_loss: 0.3199\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.2615 - val_loss: 0.3269\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.2683 - val_loss: 0.3266\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2657 - val_loss: 0.3212\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2537 - val_loss: 0.3471\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2588 - val_loss: 0.3183\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 0.2545 - val_loss: 0.3202\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2537 - val_loss: 0.3342\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2555 - val_loss: 0.3282\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 0.2569 - val_loss: 0.3334\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2545 - val_loss: 0.3452\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2560 - val_loss: 0.3277\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2572 - val_loss: 0.3385\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2527 - val_loss: 0.3290\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2547 - val_loss: 0.3270\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2558 - val_loss: 0.3245\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.2531 - val_loss: 0.3190\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2768 - val_loss: 0.3295\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2593 - val_loss: 0.3307\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2500 - val_loss: 0.3217\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2585 - val_loss: 0.3414\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2616 - val_loss: 0.3336\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2544 - val_loss: 0.3351\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2609 - val_loss: 0.3313\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2551 - val_loss: 0.3311\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2721 - val_loss: 0.3289\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2669 - val_loss: 0.3369\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2557 - val_loss: 0.3289\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2618 - val_loss: 0.3424\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.2584 - val_loss: 0.3257\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.2630 - val_loss: 0.3256\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.2899 - val_loss: 0.3255\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2562 - val_loss: 0.3336\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.2639 - val_loss: 0.3466\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2720 - val_loss: 0.3349\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2830 - val_loss: 0.3253\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2876 - val_loss: 0.3397\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2492 - val_loss: 0.3410\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2854 - val_loss: 0.3302\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.2637 - val_loss: 0.3381\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2607 - val_loss: 0.3292\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2609 - val_loss: 0.3290\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2572 - val_loss: 0.3312\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2529 - val_loss: 0.3228\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2539 - val_loss: 0.3289\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2528 - val_loss: 0.3259\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2519 - val_loss: 0.3262\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.2487 - val_loss: 0.3323\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2595 - val_loss: 0.3312\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2515 - val_loss: 0.3441\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 0.2517 - val_loss: 0.3313\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2505 - val_loss: 0.3224\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2628 - val_loss: 0.3352\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2823 - val_loss: 0.3362\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2770 - val_loss: 0.3374\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2817 - val_loss: 0.3582\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 0.2697 - val_loss: 0.3258\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 290us/sample - loss: 0.2603 - val_loss: 0.3275\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.2455 - val_loss: 0.3455\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2725 - val_loss: 0.3369\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2555 - val_loss: 0.3315\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2520 - val_loss: 0.3259\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2517 - val_loss: 0.3287\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2484 - val_loss: 0.3303\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2469 - val_loss: 0.3380\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2584 - val_loss: 0.3354\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2483 - val_loss: 0.3274\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2500 - val_loss: 0.3448\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2521 - val_loss: 0.3311\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2542 - val_loss: 0.3377\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2518 - val_loss: 0.3313\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2533 - val_loss: 0.3289\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2516 - val_loss: 0.3237\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2508 - val_loss: 0.3314\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2511 - val_loss: 0.3199\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2607 - val_loss: 0.3396\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2738 - val_loss: 0.3478\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2523 - val_loss: 0.3305\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 245us/sample - loss: 0.2768 - val_loss: 0.3467\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2750 - val_loss: 0.3450\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2598 - val_loss: 0.3318\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 0.2546 - val_loss: 0.3585\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 0.2648 - val_loss: 0.3293\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2522 - val_loss: 0.3242\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2551 - val_loss: 0.3476\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 0.2621 - val_loss: 0.3308\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 0.2654 - val_loss: 0.3360\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2790 - val_loss: 0.3471\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2589 - val_loss: 0.3417\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2913 - val_loss: 0.3433\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2666 - val_loss: 0.3726\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2874 - val_loss: 0.3587\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 189us/sample - loss: 0.2829 - val_loss: 0.3240\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 0.2551 - val_loss: 0.3597\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2824 - val_loss: 0.3295\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2544 - val_loss: 0.3311\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2553 - val_loss: 0.3431\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2876 - val_loss: 0.3452\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2621 - val_loss: 0.3425\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2864 - val_loss: 0.3345\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2686 - val_loss: 0.3388\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2527 - val_loss: 0.3396\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2912 - val_loss: 0.3282\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2980 - val_loss: 0.3566\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2904 - val_loss: 0.3493\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.3010 - val_loss: 0.3406\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 272us/sample - loss: 0.2567 - val_loss: 0.3419\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 0.2861 - val_loss: 0.3311\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2607 - val_loss: 0.3636\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2796 - val_loss: 0.3281\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2585 - val_loss: 0.3315\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2658 - val_loss: 0.3273\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2718 - val_loss: 0.3279\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2612 - val_loss: 0.3346\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2593 - val_loss: 0.3245\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2559 - val_loss: 0.3219\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2515 - val_loss: 0.3249\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.2545 - val_loss: 0.3279\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 267us/sample - loss: 0.2547 - val_loss: 0.3246\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.2491 - val_loss: 0.3253\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2511 - val_loss: 0.3304\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 0.2504 - val_loss: 0.3295\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.2559 - val_loss: 0.3255\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2488 - val_loss: 0.3397\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2552 - val_loss: 0.3288\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2478 - val_loss: 0.3362\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2514 - val_loss: 0.3269\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.2577 - val_loss: 0.3239\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2517 - val_loss: 0.3339\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 0.2527 - val_loss: 0.3278\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 0.2652 - val_loss: 0.3204\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2488 - val_loss: 0.3392\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2556 - val_loss: 0.3280\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.2819 - val_loss: 0.3278\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.2456 - val_loss: 0.3519\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2667 - val_loss: 0.3295\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2710 - val_loss: 0.3282\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2704 - val_loss: 0.3473\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2676 - val_loss: 0.3269\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2532 - val_loss: 0.3225\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2554 - val_loss: 0.3330\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2612 - val_loss: 0.3339\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2694 - val_loss: 0.3310\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2649 - val_loss: 0.3370\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2648 - val_loss: 0.3284\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 183us/sample - loss: 0.2629 - val_loss: 0.3319\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2753 - val_loss: 0.3546\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2602 - val_loss: 0.3285\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2603 - val_loss: 0.3281\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2526 - val_loss: 0.3298\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2610 - val_loss: 0.3255\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2595 - val_loss: 0.3261\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2583 - val_loss: 0.3382\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2682 - val_loss: 0.3216\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2522 - val_loss: 0.3233\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 193us/sample - loss: 0.2498 - val_loss: 0.3333\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2576 - val_loss: 0.3278\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 0.2548 - val_loss: 0.3457\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2655 - val_loss: 0.3411\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2856 - val_loss: 0.3445\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2634 - val_loss: 0.3305\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2764 - val_loss: 0.3245\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2699 - val_loss: 0.3514\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2740 - val_loss: 0.3256\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2575 - val_loss: 0.3190\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2499 - val_loss: 0.3239\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 0.2472 - val_loss: 0.3263\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2484 - val_loss: 0.3258\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2533 - val_loss: 0.3295\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2449 - val_loss: 0.3270\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 0.2522 - val_loss: 0.3297\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2508 - val_loss: 0.3232\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2523 - val_loss: 0.3246\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 329us/sample - loss: 0.2504 - val_loss: 0.3305\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.2432 - val_loss: 0.3259\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2467 - val_loss: 0.3340\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2489 - val_loss: 0.3374\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 0.2495 - val_loss: 0.3275\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2533 - val_loss: 0.3272\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2567 - val_loss: 0.3405\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2535 - val_loss: 0.3347\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2629 - val_loss: 0.3387\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 0.2527 - val_loss: 0.3305\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2536 - val_loss: 0.3276\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2472 - val_loss: 0.3336\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2581 - val_loss: 0.3271\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2662 - val_loss: 0.3260\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2586 - val_loss: 0.3508\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.2862 - val_loss: 0.3291\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2487 - val_loss: 0.3271\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 0.2556 - val_loss: 0.3238\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2482 - val_loss: 0.3255\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2482 - val_loss: 0.3229\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2607 - val_loss: 0.3272\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2480 - val_loss: 0.3286\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2539 - val_loss: 0.3369\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2500 - val_loss: 0.3311\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2625 - val_loss: 0.3261\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.2484 - val_loss: 0.3473\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2662 - val_loss: 0.3251\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2725 - val_loss: 0.3352\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2616 - val_loss: 0.3478\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2627 - val_loss: 0.3217\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2587 - val_loss: 0.3273\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2595 - val_loss: 0.3338\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2571 - val_loss: 0.3243\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2428 - val_loss: 0.3274\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2565 - val_loss: 0.3293\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2582 - val_loss: 0.3338\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.2511 - val_loss: 0.3243\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2570 - val_loss: 0.3243\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2476 - val_loss: 0.3432\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2569 - val_loss: 0.3234\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2660 - val_loss: 0.3372\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2589 - val_loss: 0.3400\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2701 - val_loss: 0.3204\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2504 - val_loss: 0.3300\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2633 - val_loss: 0.3320\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2596 - val_loss: 0.3260\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2801 - val_loss: 0.3356\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2689 - val_loss: 0.3264\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 0.2583 - val_loss: 0.3228\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 291us/sample - loss: 0.2520 - val_loss: 0.3194\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.2537 - val_loss: 0.3183\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2464 - val_loss: 0.3262\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2539 - val_loss: 0.3296\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2451 - val_loss: 0.3288\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2540 - val_loss: 0.3322\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 0.2457 - val_loss: 0.3289\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2430 - val_loss: 0.3321\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2477 - val_loss: 0.3360\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2461 - val_loss: 0.3335\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2494 - val_loss: 0.3337\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2425 - val_loss: 0.3251\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 0.2461 - val_loss: 0.3254\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 185us/sample - loss: 0.2540 - val_loss: 0.3430\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2501 - val_loss: 0.3380\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2436 - val_loss: 0.3279\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2604 - val_loss: 0.3286\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2483 - val_loss: 0.3359\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2499 - val_loss: 0.3342\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2431 - val_loss: 0.3290\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2628 - val_loss: 0.3272\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2482 - val_loss: 0.3451\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2454 - val_loss: 0.3259\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2520 - val_loss: 0.3304\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2416 - val_loss: 0.3387\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2452 - val_loss: 0.3243\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2553 - val_loss: 0.3225\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2505 - val_loss: 0.3417\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2455 - val_loss: 0.3237\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2487 - val_loss: 0.3268\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2420 - val_loss: 0.3348\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2448 - val_loss: 0.3271\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2533 - val_loss: 0.3306\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2419 - val_loss: 0.3271\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2507 - val_loss: 0.3321\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2444 - val_loss: 0.3346\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2417 - val_loss: 0.3291\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 192us/sample - loss: 0.2399 - val_loss: 0.3401\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 0.2473 - val_loss: 0.3351\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2456 - val_loss: 0.3299\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2460 - val_loss: 0.3325\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2432 - val_loss: 0.3310\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2474 - val_loss: 0.3308\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2451 - val_loss: 0.3426\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2555 - val_loss: 0.3266\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2383 - val_loss: 0.3461\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2673 - val_loss: 0.3294\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2534 - val_loss: 0.3365\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2534 - val_loss: 0.3463\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2616 - val_loss: 0.3375\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2477 - val_loss: 0.3239\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2520 - val_loss: 0.3313\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 241us/sample - loss: 0.2595 - val_loss: 0.3399\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 290us/sample - loss: 0.2614 - val_loss: 0.3250\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2386 - val_loss: 0.3438\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2739 - val_loss: 0.3313\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2541 - val_loss: 0.3419\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2648 - val_loss: 0.3397\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2804 - val_loss: 0.3316\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2705 - val_loss: 0.3841\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2852 - val_loss: 0.3319\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2653 - val_loss: 0.3392\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2494 - val_loss: 0.3345\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2737 - val_loss: 0.3224\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 0.2476 - val_loss: 0.3377\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 0.2465 - val_loss: 0.3238\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2457 - val_loss: 0.3261\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 188us/sample - loss: 0.2482 - val_loss: 0.3350\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2442 - val_loss: 0.3394\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2464 - val_loss: 0.3292\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2437 - val_loss: 0.3289\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2491 - val_loss: 0.3345\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2474 - val_loss: 0.3364\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2497 - val_loss: 0.3389\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2496 - val_loss: 0.3277\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2406 - val_loss: 0.3423\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2501 - val_loss: 0.3266\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2476 - val_loss: 0.3234\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2496 - val_loss: 0.3437\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 315us/sample - loss: 0.2535 - val_loss: 0.3338\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2393 - val_loss: 0.3292\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2545 - val_loss: 0.3323\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2449 - val_loss: 0.3403\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2424 - val_loss: 0.3275\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2492 - val_loss: 0.3337\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2416 - val_loss: 0.3380\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 224us/sample - loss: 0.2497 - val_loss: 0.3294\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2423 - val_loss: 0.3434\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2520 - val_loss: 0.3294\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2481 - val_loss: 0.3328\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2406 - val_loss: 0.3389\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2491 - val_loss: 0.3332\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.2419 - val_loss: 0.3307\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2412 - val_loss: 0.3443\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2495 - val_loss: 0.3299\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2401 - val_loss: 0.3278\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2442 - val_loss: 0.3391\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 0.2411 - val_loss: 0.3308\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2489 - val_loss: 0.3378\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2471 - val_loss: 0.3452\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2635 - val_loss: 0.3344\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2516 - val_loss: 0.3312\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2400 - val_loss: 0.3438\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 232us/sample - loss: 0.2547 - val_loss: 0.3412\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2535 - val_loss: 0.3285\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.2438 - val_loss: 0.3469\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2645 - val_loss: 0.3386\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.2416 - val_loss: 0.3272\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2414 - val_loss: 0.3352\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2487 - val_loss: 0.3280\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.2402 - val_loss: 0.3276\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2473 - val_loss: 0.3391\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2471 - val_loss: 0.3377\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2404 - val_loss: 0.3348\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2431 - val_loss: 0.3291\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2395 - val_loss: 0.3397\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 0.2431 - val_loss: 0.3408\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.2456 - val_loss: 0.3377\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2447 - val_loss: 0.3309\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2413 - val_loss: 0.3307\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2385 - val_loss: 0.3427\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2389 - val_loss: 0.3309\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2499 - val_loss: 0.3349\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2446 - val_loss: 0.3463\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2447 - val_loss: 0.3331\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 199us/sample - loss: 0.2548 - val_loss: 0.3474\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2656 - val_loss: 0.3439\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2519 - val_loss: 0.3614\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2706 - val_loss: 0.3474\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.2884 - val_loss: 0.3553\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2506 - val_loss: 0.3445\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.3021 - val_loss: 0.3216\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2414 - val_loss: 0.3447\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2748 - val_loss: 0.3213\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2460 - val_loss: 0.3434\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2651 - val_loss: 0.3174\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2421 - val_loss: 0.3401\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 0.2625 - val_loss: 0.3143\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2493 - val_loss: 0.3184\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 231us/sample - loss: 0.2493 - val_loss: 0.3277\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 216us/sample - loss: 0.2511 - val_loss: 0.3307\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2484 - val_loss: 0.3282\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2497 - val_loss: 0.3468\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2657 - val_loss: 0.3433\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2521 - val_loss: 0.3299\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2463 - val_loss: 0.3409\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 201us/sample - loss: 0.2468 - val_loss: 0.3439\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 0.2483 - val_loss: 0.3269\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2750 - val_loss: 0.3224\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2423 - val_loss: 0.3605\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.2732 - val_loss: 0.3233\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2584 - val_loss: 0.3262\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2436 - val_loss: 0.3499\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2658 - val_loss: 0.3283\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 226us/sample - loss: 0.2467 - val_loss: 0.3257\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2442 - val_loss: 0.3316\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 187us/sample - loss: 0.2436 - val_loss: 0.3377\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 267us/sample - loss: 0.2483 - val_loss: 0.3264\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 229us/sample - loss: 0.2553 - val_loss: 0.3231\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 0.2455 - val_loss: 0.3424\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2490 - val_loss: 0.3258\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2419 - val_loss: 0.3322\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2418 - val_loss: 0.3377\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2417 - val_loss: 0.3309\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2474 - val_loss: 0.3356\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.2457 - val_loss: 0.3403\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2402 - val_loss: 0.3262\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2548 - val_loss: 0.3290\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2526 - val_loss: 0.3353\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2486 - val_loss: 0.3258\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.2545 - val_loss: 0.3370\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2507 - val_loss: 0.3273\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2430 - val_loss: 0.3297\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 244us/sample - loss: 0.2444 - val_loss: 0.3308\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2432 - val_loss: 0.3376\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2539 - val_loss: 0.3401\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2539 - val_loss: 0.3349\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.2542 - val_loss: 0.3483\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2580 - val_loss: 0.3321\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2605 - val_loss: 0.3271\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2488 - val_loss: 0.3462\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 200us/sample - loss: 0.2480 - val_loss: 0.3265\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 228us/sample - loss: 0.2478 - val_loss: 0.3237\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2437 - val_loss: 0.3439\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2515 - val_loss: 0.3260\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 214us/sample - loss: 0.2383 - val_loss: 0.3325\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2476 - val_loss: 0.3310\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2440 - val_loss: 0.3270\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 215us/sample - loss: 0.2406 - val_loss: 0.3354\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 225us/sample - loss: 0.2413 - val_loss: 0.3391\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 234us/sample - loss: 0.2444 - val_loss: 0.3323\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2517 - val_loss: 0.3299\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 0.2420 - val_loss: 0.3424\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.2454 - val_loss: 0.3263\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 0.2432 - val_loss: 0.3313\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 0.2444 - val_loss: 0.3268\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 0.2494 - val_loss: 0.3341\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2346 - val_loss: 0.3295\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2531 - val_loss: 0.3352\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 222us/sample - loss: 0.2407 - val_loss: 0.3360\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2489 - val_loss: 0.3353\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 230us/sample - loss: 0.2511 - val_loss: 0.3246\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2508 - val_loss: 0.3328\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2448 - val_loss: 0.3297\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2447 - val_loss: 0.3274\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 212us/sample - loss: 0.2387 - val_loss: 0.3331\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2447 - val_loss: 0.3279\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 208us/sample - loss: 0.2428 - val_loss: 0.3336\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 0.2429 - val_loss: 0.3344\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2452 - val_loss: 0.3357\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2518 - val_loss: 0.3409\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 240us/sample - loss: 0.2453 - val_loss: 0.3346\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2477 - val_loss: 0.3389\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 239us/sample - loss: 0.2563 - val_loss: 0.3404\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2503 - val_loss: 0.3301\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 238us/sample - loss: 0.2521 - val_loss: 0.3391\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 0.2452 - val_loss: 0.3435\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 223us/sample - loss: 0.2463 - val_loss: 0.3288\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2410 - val_loss: 0.3350\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 211us/sample - loss: 0.2480 - val_loss: 0.3335\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 217us/sample - loss: 0.2486 - val_loss: 0.3399\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2465 - val_loss: 0.3295\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 329us/sample - loss: 0.2442 - val_loss: 0.3339\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2429 - val_loss: 0.3306\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2376 - val_loss: 0.3359\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 218us/sample - loss: 0.2445 - val_loss: 0.3321\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 227us/sample - loss: 0.2398 - val_loss: 0.3263\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 206us/sample - loss: 0.2384 - val_loss: 0.3277\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 194us/sample - loss: 0.2396 - val_loss: 0.3361\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 204us/sample - loss: 0.2398 - val_loss: 0.3331\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2381 - val_loss: 0.3394\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 0.2397 - val_loss: 0.3337\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 235us/sample - loss: 0.2436 - val_loss: 0.3298\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 207us/sample - loss: 0.2381 - val_loss: 0.3332\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 219us/sample - loss: 0.2391 - val_loss: 0.3270\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.2393 - val_loss: 0.3327\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 0.2446 - val_loss: 0.3353\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.2430 - val_loss: 0.3269\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.2436 - val_loss: 0.3427\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2406 - val_loss: 0.3274\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 0.2420 - val_loss: 0.3335\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 243us/sample - loss: 0.2423 - val_loss: 0.3403\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 233us/sample - loss: 0.2439 - val_loss: 0.3317\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.2431 - val_loss: 0.3350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f11d456aef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTUoBLiLSpBTgIiJNSgEuItKk/j/wAnLl6Xz1ggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deep_neural_network_reg():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=1000,input_shape=(1,),activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))# input connected to output\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1000,activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model\n",
    "\n",
    "# calling baseline model\n",
    "deep_nn_reg = deep_neural_network_reg()\n",
    "print(deep_nn_reg.summary())\n",
    "# initializing tensorboard\n",
    "tfb = TensorBoard('deep_neural_network_reg')\n",
    "# Training Model \n",
    "history = deep_nn_reg.fit(x=x_train,y=y_train,batch_size=None,epochs=1000,callbacks=[tfb],validation_data=[x_test,y_test])\n",
    "# loading into data\n",
    "data_loss_dnn_r = pd.DataFrame(history.history)\n",
    "data_loss_dnn_r.plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "EIPzvzMvxJ_J",
    "outputId": "0eb6227a-0809-4f2a-a411-01f100ca599a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f11d42967b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngd4YY3zOAr0xxvjc/wFZ3m560ZE0DAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loss_dnn_r[30:].plot(kind='line') # visualizing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "l7l5hskHvktq",
    "outputId": "68a06b2a-5a2e-4836-ab02-84e646f829ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f11d43b4cc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeViV1dr48e9isxlVcDYRp1IEAYFA\nRYXUVDxlZqVms1k5dDxmeqx86y3rp2VpWZqpmWbzmzbYfEJLj0CamuIsYooDmogKyjyt3x8bdmzY\nwGYShPtzXV6y136e9Sw2l3R3r+FWWmuEEEIIIUT9YlfXAxBCCCGEEKVJkCaEEEIIUQ9JkCaEEEII\nUQ9JkCaEEEIIUQ9JkCaEEEIIUQ9JkCaEEEIIUQ9JkCaEEHVEKTVHKfVxXY9DCFE/SZAmhKj3lFID\nlFK/KaVSlVIXlVIxSqmQavY5XikVXaJtjVJqbvVGW+o5a5RSOUqptMKxb1BK9ahCPwlKqSE1OTYh\nRP0mQZoQol5TSjUDvgeWAC0AD+BFILsux2WNUsq+jLde01o3AToAScCaqzYoIcQ1S4I0IUR91x1A\na/2Z1jpfa52ptY7UWu8tukAp9ZhS6pBS6opS6qBSKqiw/Rml1J/F2u8obPcGlgOhhRmuFKXUROA+\n4KnCtu8Kr22vlPpSKXVeKXVcKTWt2HPnKKW+UEp9rJS6DIwv7xvRWmcAnwK+1t5XSo1USh0oHM/m\nwnGilPoI6Ah8Vzi2p6r2UQohriUSpAkh6rsjQL5S6gOl1D+UUs2Lv6mUGgPMAR4EmgEjgQuFb/8J\nhAFumLJvHyulrtNaHwImA1u11k201u5a63eBTyjMemmtb1NK2QHfAXswZfBuBqYrpSKKDeF24AvA\nvfD+MimlmmAKBHdbea878BkwHWgN/IgpKHPQWj8AnARuKxzbaxV/bEKIa50EaUKIek1rfRkYAGhg\nJXBeKfWtUqpt4SWPYgqsdmiTo1rrE4X3rtNan9FaF2itPwfigd6VeHwI0Fpr/ZLWOkdrfaxwDOOK\nXbNVa72+8BmZZfTzb6VUCnAUaIL1jNvdwA9a6w1a61xgIeAM9KvEeIUQDUhZ6yeEEKLeKMx8jQco\nXHT/MfAmcA/giSljVopS6kFgBtC5sKkJ0KoSj+4EtC8MsIoYgKhir0/Z0M9CrfVzFVzTHjhR9EJr\nXaCUOoUpgyeEaIQkSBNCXFO01oeVUmuASYVNp4DrS16nlOqEKet1M6ZsV75SKhZQRV1Z677E61PA\nca11t/KGVInhl+cM4Ff0QimlMAWgiTX8HCHENUKmO4UQ9ZpSqodSaqZSqkPha09MGbRthZe8h2k6\n8UZlckNhgOaKKbA5X3jfw1gu2D8HdFBKOZRo61rs9XbgilLqaaWUs1LKoJTyre7xH2VYC9yqlLpZ\nKWUEZmLawfpbGWMTQjRwEqQJIeq7K0Af4HelVDqm4Gw/piAGrfU6YB6mXZNXgPVAC631QeB1YCum\nAMcPiCnW76/AAeAvpVRyYdsqwKdwd+V6rXU+MAIIAI4DyZiCQrea/ia11nHA/ZiOGkkGbsO0USCn\n8JJXgOcKx/bvmn6+EKL+UVpLBl0IIYQQor6RTJoQQgghRD0kQZoQQgghRD0kQZoQQgghRD0kQZoQ\nQgghRD0kQZoQQgghRD3U4A6zbdWqle7cuXNdD0MIIYQQokJ//PFHsta6tbX3GlyQ1rlzZ3bu3FnX\nwxBCCCGEqJBS6kRZ78l0pxBCCCFEPSRBmhBCCCFEPSRBmhBCCCFEPdTg1qQJIURDlJuby+nTp8nK\nyqrroQghqsDJyYkOHTpgNBptvkeCNCGEuAacPn2apk2b0rlzZ5RSdT0cIUQlaK25cOECp0+fpkuX\nLjbfJ9OdQghxDcjKyqJly5YSoAlxDVJK0bJly0pnwussSFNKeSqlNimlDiqlDiilnrByzUClVKpS\nKrbwz/N1MVYhhKgPJEAT4tpVlX+/dZlJywNmaq19gL7AP5VSPlaui9JaBxT+eenqDlEIIQRAQkIC\nvr6+tdb/5s2bGTFiBADffvst8+fPr7VnVdXAgQMrPIfTlmuEsFWdrUnTWp8FzhZ+fUUpdQjwAA7W\n1ZiEEELUvZEjRzJy5Mi6HoYQda5erElTSnUGAoHfrbwdqpTao5T6SSnV86oOTAghhFleXh733Xcf\n3t7ejB49moyMDABeeuklQkJC8PX1ZeLEiWitAVi8eDE+Pj74+/szbtw4ANLT05kwYQK9e/cmMDCQ\nb775ptRz1qxZw9SpUwEYP34806ZNo1+/fnTt2pUvvvjCfN2CBQsICQnB39+fF154weqYmzRpwqxZ\ns+jZsydDhgxh+/btDBw4kK5du/Ltt98CpvV+Dz/8MH5+fgQGBrJp0yYAMjMzGTduHN7e3txxxx1k\nZmaa+42MjCQ0NJSgoCDGjBlDWlpadT9eIUqp892dSqkmwJfAdK315RJv7wI6aa3TlFK3AOuBblb6\nmAhMBOjYsWMtj1gI0Zit353Igp/jOJOSSXt3Z2ZFeDEq0OOqjuHF7w5w8EzJX5fV49O+GS/cVv7/\nB8fFxbFq1Sr69+/PhAkTeOedd/j3v//N1KlTef5505LhBx54gO+//57bbruN+fPnc/z4cRwdHUlJ\nSQFg3rx5DB48mNWrV5OSkkLv3r0ZMmRIuc89e/Ys0dHRHD58mJEjRzJ69GgiIyOJj49n+/btaK0Z\nOXIkW7ZsITw83OLe9PR0Bg8ezIIFC7jjjjt47rnn2LBhAwcPHuShhx5i5MiRLF26FKUU+/bt4/Dh\nwwwbNowjR46wbNkyXFxcOHToEHv37iUoKAiA5ORk5s6dy8aNG3F1deXVV1/ljTfeMH8GQtSUOs2k\nKaWMmAK0T7TWX5V8X2t9WWudVvj1j4BRKdXKynXvaq2DtdbBrVtbrVEqhBDVtn53IrO/2kdiSiYa\nSEzJZPZX+1i/O7Guh3ZVeHp60r9/fwDuv/9+oqOjAdi0aRN9+vTBz8+PX3/9lQMHDgDg7+/Pfffd\nx8cff4y9vSknEBkZyfz58wkICGDgwIFkZWVx8uTJcp87atQo7Ozs8PHx4dy5c+Z+IiMjCQwMJCgo\niMOHDxMfH1/qXgcHB4YPHw6An58fN910E0ajET8/PxISEgCIjo7m/vvvB6BHjx506tSJI0eOsGXL\nFnO7v78//v7+AGzbto2DBw/Sv39/AgIC+OCDDzhxoszyi0JUWZ1l0pRpm8Mq4JDW+o0yrmkHnNNa\na6VUb0xB5YWrOEwhhDBb8HMcmbn5Fm2Zufks+DnuqmbTKsp41ZaSu9OUUmRlZfH444+zc+dOPD09\nmTNnjvmYgR9++IEtW7bw3XffMW/ePPbt24fWmi+//BIvLy+LvoqCL2scHR3NXxdNpWqtmT17NpMm\nTSp3zEaj0TxuOzs7c192dnbk5eXZ+J1b0lozdOhQPvvssyrdL4St6jKT1h94ABhc7IiNW5RSk5VS\nkwuvGQ3sV0rtARYD43TRv1AhhLjKzqRkVqq9oTl58iRbt24F4NNPP2XAgAHmgKxVq1akpaWZ14wV\nFBRw6tQpBg0axKuvvkpqaippaWlERESwZMkSc7C1e/fuKo0lIiKC1atXm9eCJSYmkpSUVKW+wsLC\n+OSTTwA4cuQIJ0+exMvLi/DwcD799FMA9u/fz969ewHo27cvMTExHD16FDBNqR45cqRKzxaiPHW5\nuzMaKPfQEK3128DbV2dEQghRvvbuziRaCcjauzvXwWiuPi8vL5YuXcqECRPw8fFhypQpuLi48Nhj\nj+Hr60u7du0ICQkBID8/n/vvv5/U1FS01kybNg13d3f+93//l+nTp+Pv709BQQFdunTh+++/r/RY\nhg0bxqFDhwgNDQVMGwQ+/vhj2rRpU+m+Hn/8caZMmYKfnx/29vasWbMGR0dHpkyZwsMPP4y3tzfe\n3t7ceOONALRu3Zo1a9Zwzz33kJ2dDcDcuXPp3r17pZ8tRHlUQ0tMBQcHazmjRghRG4rWpBWf8nQ2\nGnjlTr9an+48dOgQ3t7etfoMIUTtsvbvWCn1h9Y62Nr19eIIDiGEuBaMCvRgzsi/z9xWwLO3eF/1\n3Z1CiMZBgjQhhKgEdxcHAF66vScoSExtHOvRhBBXnwRpQghRCRsOnqOZkz339O7IrX7X8eFvCVxK\nz6nrYQkhGiAJ0oQQwkb5BZpfDycxqEcbjAY7pt3cjfScfFZFH6/roQkhGiAJ0oQQwkZ/nLjExfQc\nhvq0BaB726bc4teONb8lkJqRW8ejE0I0NBKkCSGEjTYc/AujQXFT978rm/xrcDfSsvNYFSPZNCFE\nzZIgTQghbKC1ZsPBc4Re34qmTkZzu/d1zYjo2Zb3Y46Tmtmws2lKKWbOnGl+vXDhQubMmVPrzx04\ncCDWjlYaOHAgwcF/n1ywc+dOBg4cWG5fCQkJ5gNqa1JCQgK+vr413q9o3CRIE0IIGxxNSiPhQoZ5\nqrM4Pw83rmTl0evFSPrP/7XB1vJ0dHTkq6++Ijk5uUb71VpTUFBQpXuTkpL46aefbL6+NoK0qpaX\nEqIiEqQJIYQNIg+aaksO9bYM0tbvTmTppj/Nrxty0XV7e3smTpzIokWLSr13/vx57rrrLkJCQggJ\nCSEmJgaAOXPmsHDhQvN1vr6+JCQkkJCQgJeXFw8++CC+vr6cOnWKKVOmEBwcTM+ePXnhhRdsGtOs\nWbOYN29eqfb8/HxmzZpFSEgI/v7+rFixAoBnnnmGqKgoAgICWLRoEbfeequ53FNgYCAvvfQSAM8/\n/zwrV65Ea82sWbPw9fXFz8+Pzz//HIDNmzcTFhbGyJEj8fHxsXj2sWPHCAwMZMeOHTZ9D0KUpc7K\nQgkhxLVkw8Fz+Hdwo52bk0V7nRRd/+kZ+GtfzfbZzg/+Mb/Cy/75z3/i7+/PU089ZdH+xBNP8OST\nTzJgwABOnjxJREQEhw4dKrev+Ph4PvjgA/r27QvAvHnzaNGiBfn5+dx8883s3bsXf3//cvsIDQ3l\n66+/ZtOmTTRt2tTcvmrVKtzc3NixYwfZ2dn079+fYcOGMX/+fBYuXGguRZWdnU1UVBSdOnXC3t7e\nHFxGRUWxfPlyvvrqK2JjY9mzZw/JycmEhIQQHh4OwK5du9i/fz9dunQhISEBgLi4OMaNG8eaNWvo\n1atXhZ+nEOWRTJoQQlQg6XIWsadSSmXRoPEVXW/WrBkPPvggixcvtmjfuHEjU6dOJSAggJEjR3L5\n8mVz8fOydOrUyRygAaxdu5agoCACAwM5cOAABw8etGlMzz33HHPnzrVoi4yM5MMPPyQgIIA+ffpw\n4cIF4uPjS90bFhbGli1biImJ4dZbbyUtLY2MjAyOHz+Ol5cX0dHR3HPPPRgMBtq2bctNN91kzpD1\n7t2bLl26mPs6f/48t99+O5988okEaKJGSCZNCCHKsH53Igt+jjMXVTfYqVLX1EnRdRsyXrVp+vTp\nBAUF8fDDD5vbCgoK2LZtG05OlplGe3t7i/VmWVlZ5q9dXV3NXx8/fpyFCxeyY8cOmjdvzvjx4y2u\nLc/gwYN57rnn2LZtm7lNa82SJUuIiIiwuHbz5s0Wr0NCQti5cyddu3Zl6NChJCcns3LlSnMx9fIU\nHz+Am5sbHTt2JDo6utQUqBBVIZk0IYSwoqiYevEAbPEv8aXWms2K8MLZaLBoczYamBXhdVXGWRda\ntGjB2LFjWbVqlblt2LBhLFmyxPw6NjYWgM6dO7Nr1y7AND14/Lj1o0ouX76Mq6srbm5unDt3rlKb\nAcCUTXvttdfMryMiIli2bBm5uaYdt0eOHCE9PZ2mTZty5coV83UODg54enqybt06QkNDCQsLY+HC\nheYpzbCwMD7//HPy8/M5f/48W7ZsoXfv3lbH4ODgwNdff82HH35YKztIReMjQZoQQlhhba1ZVl4B\nC36Os2gbFejBK3f60b5wrVpTR3teudOvwRddnzlzpsUuz8WLF7Nz5078/f3x8fFh+fLlANx1111c\nvHiRnj178vbbb9O9e3er/fXq1YvAwEB69OjBvffeS//+/Ss1nltuuYXWrf8+v+7RRx/Fx8eHoKAg\nfH19mTRpEnl5efj7+2MwGOjVq5d5A0RYWBht2rTB2dmZsLAwTp8+TVhYGAB33HEH/v7+9OrVi8GD\nB/Paa6/Rrl27Msfh6urK999/z6JFi/j2228r9T0IUZLSWtf1GGpUcHCwtnaejhBCVEaXZ37A2m9H\nBRyff6vVe4LnbmBYz3a8fIdfjY/n0KFDeHt713i/Qoirx9q/Y6XUH1rrYGvXSyZNCCGsKGtNWXlr\nzZq7OEixdSFEjZEgTQghrKjKWrPmrg5clCBNCFFDJEgTQggritaaORlNvyY93J0rXGvWwsWBSxkS\npAkhaoYcwSGEEGUYFejB2p2nyM4r4Msp/Sq8vrmrAxdPNOz6nUKIq0cyaUIIUY7UzFzcnI0VXwg0\ndzFyKSOHhrYhSwhRNyRIE0KIcqRk5OJuY5DWwtWB/ALN5SwpuC2EqD4J0oQQohyXM3NpZnMmzQGg\nwe7wNBgMBAQE0LNnT3r16sXrr79uUU2gNowfPx4PDw+ys7MBSE5OpnPnzrX6TIA1a9YwdepUq+12\ndnbmouzwd9H48rz55ptkZGTU9DAZOHAgcuxUwyVBmhBClCG/QHMlO8/m6c4WrqYg7WID3Tzg7OxM\nbGwsBw4cYMOGDfz000+8+OKLtf5cg8HA6tWra7zfvLyqZTw7dOjAvHnzKnVPbQRp+fn5FV8krmkS\npAkhRBkuZ5o2Abi72JhJc60/mbT1uxPpP/9XujzzA/3n/1qqnFV1tWnThnfffZe3334brTX5+fnM\nmjWLkJAQ/P39WbFihfnaBQsWmNtfeOEFABISEujRowf33Xcf3t7ejB49uswgZvr06SxatMhqUFVW\n376+vuZrFi5cyJw5cwBT5mn69OkEBwfz1ltv8d1339GnTx8CAwMZMmQI586dq/B7HzFiBAcOHCAu\nLq7Ue5GRkYSGhhIUFMSYMWNIS0tj8eLFnDlzhkGDBjFo0CDWrVvHjBkzAHjrrbfo2rUrAMeOHTNX\nWvjll18IDAzEz8+PCRMmmDOJnTt35umnnyYoKIh169aZn1tQUMD48eN57rnnKhy/uHZIkCaEEGVI\nKQzSbM6kFU13ZtTtDs/idUc1kJiSyeyv9tV4oNa1a1fy8/NJSkpi1apVuLm5sWPHDnbs2MHKlSs5\nfvw4kZGRxMfHs337dmJjY/njjz/YsmULAHFxcTz++OMcOnSIZs2a8c4771h9TseOHRkwYAAfffSR\nRXt5fZcnJyeHnTt3MnPmTAYMGMC2bdvYvXs348aNs6j/WRY7OzueeuopXn75ZYv25ORk5s6dy8aN\nG9m1axfBwcG88cYbTJs2jfbt27Np0yY2bdpEWFgYUVFRAERFRdGyZUsSExOJiooiPDycrKwsxo8f\nz+eff86+ffvIy8tj2bJl5ue0bNmSXbt2MW7cOMCUEbzvvvvo1q0bc+fOrXD84tohQZoQQpQhtZJB\nWnNX03V1nUmzVnc0Mze/VN3RmhQZGcmHH35IQEAAffr04cKFC8THxxMZGUlkZCSBgYEEBQVx+PBh\n4uPjAfD09DRnju6//36io6PL7H/27NksWLDAYg1ceX2X5+677zZ/ffr0aSIiIvDz82PBggUcOHDA\npu/33nvvZdu2bRYF47dt28bBgwfp378/AQEBfPDBB5w4caLUve3atSMtLY0rV65w6tQp7r33XrZs\n2UJUVBRhYWHExcXRpUsXc53Thx56yCL4LD5+gEmTJuHr68uzzz5r09jFtUPOSRNCiDJUNkhr4miP\n0aDqfE3amZTMSrVX1bFjxzAYDLRp0watNUuWLCEiIsLimp9//pnZs2czadIki/aEhASUUhZtJV8X\n161bNwICAli7dq25TWttte/Tp09bBHNZWVkW77u6upq//te//sWMGTMYOXIkmzdvNk+LVsTe3p6Z\nM2fy6quvWoxn6NChfPbZZxXe369fP95//328vLwICwtj9erVbN26lddff73CTQjFx1/U16ZNm5g5\ncyZOTk42jV9cGySTJoRo2PauhUW+MMfd9PfetRXfUyilMNiydU2aUqpe1O+sSt3Ryjp//jyTJ09m\n6tSpKKWIiIhg2bJl5OaaAtsjR46Qnp5OREQEq1evJi0tDYDExESSkpIAOHnyJFu3bgXg008/ZcCA\nAeU+89lnn2XhwoXm12X13bZtW5KSkrhw4QLZ2dl8//33ZfaZmpqKh4episQHH3xQqc9g/PjxbNy4\nkfPnzwPQt29fYmJiOHr0KADp6ekcOXIEgKZNm3LlyhXzvWFhYSxcuJDw8HACAwPZtGkTjo6OuLm5\n4eXlRUJCgrmfjz76iJtuuqnMcTzyyCPccsstjB07tsqbIUT9JEGaEKLh2rsWvpsGqacAbfr7u2k2\nB2pFGwdsPYIDTDs867p+Z1XqjtoiMzPTfATHkCFDGDZsmHmx/qOPPoqPjw9BQUH4+voyadIk8vLy\nGDZsGPfeey+hoaH4+fkxevRoc7Di5eXF0qVL8fb25tKlS0yZMqXc5/fs2ZOgoCDz67L6NhqNPP/8\n8/Tu3ZuhQ4fSo0ePMvucM2cOY8aM4cYbb6RVq1aV+jwcHByYNm2aOehs3bo1a9as4Z577sHf35/Q\n0FAOHz4MwMSJExk+fDiDBg0CTEHaqVOnCA8Px2Aw4OnpaQ5SnZyceP/99xkzZgx+fn7Y2dkxefLk\ncscyY8YMAgMDeeCBB2r9WBRx9aiGdjJ2cHCwljNjhBCAKXOWeqp0u5snPLm/wtvf/jWehZFHiJs7\nHEd7Q4XXA9zz7jbyCgpYN7niMlKVcejQIby9vW2+fv3uRBb8HMeZlEzauzszK8Kr3LqjV1tCQgIj\nRoxg//6Kfw5CNBTW/h0rpf7QWgdbu17WpAkhGq7U05VrL3lZZi7ORoPNARqYNg/E/XWl4gtr2ahA\nj3oVlAkhKk+mO4UQDZdbh8q1l5CSYXvdziLNXRzq/AiOa0Hnzp0liyZEBSRIE0I0XDc/D8YSi+WN\nzqZ2G1SmuHqRFq4OpGTkkF/QsJaSCCGuPgnShBANl/9YuG2xaQ0ayvT3bYtN7TaoSpDW3MWBAv33\npgMhhKgqWZMmhGjY/MfaHJSVlJqZi2cLl0rdU7x+Z1GZKCGEqIo6y6QppTyVUpuUUgeVUgeUUk9Y\nuUYppRYrpY4qpfYqpYKs9SWEELWhSpm0elS/UwhxbavL6c48YKbW2gfoC/xTKeVT4pp/AN0K/0wE\nliGEEFdJldak1ZP6nbXBYDCYz0nr1asXr7/+eq2fyTV+/Hg8PDzMBcaTk5Pp3LlzrT4TYM2aNUyd\nOtVqu52dHXv37jW3+fr6Vlgl4M033yyzgHx1DBw4kOoeO/X888+zceNGoPQ4mzRpUuH9a9asoXXr\n1gQEBNCjRw8WLVpUrfGUZfz48XzxxReVumf58uV8+OGHlX7W5s2b+e2336rdT3XVWZCmtT6rtd5V\n+PUV4BBQcr/47cCH2mQb4K6Uuu4qD1UI0Qjl5heQkZNfhUxa/ajfWRucnZ2JjY3lwIEDbNiwgZ9+\n+okXX3yx1p9rMBhYvXp1jfdb1dP5O3TowLx58yp1T20Eafn5+RVfZIOXXnqJIUOGAFUf5913301s\nbCwxMTHMmzePU6esnE94leXl5TF58mQefPDBSt9bMkiraj/VVS82DiilOgOBwO8l3vIAiv+kT1M6\nkEMpNVEptVMptbOoPIcQQlRHUd1OW0tCFSm+Jq1OVaMcli3atGnDu+++y9tvv43Wmvz8fGbNmkVI\nSAj+/v6sWLHCfO2CBQvM7UUVChISEujRowf33Xcf3t7ejB49uszgYPr06SxatMhqUFVW376+vuZr\nFi5caK7JOXDgQKZPn05wcDBvvfUW3333HX369CEwMJAhQ4Zw7ty5Cr/3ESNGcODAAeLiShesj4yM\nJDQ0lKCgIMaMGUNaWhqLFy/mzJkzDBo0iEGDBrFu3TpmzJgBwFtvvUXXrl0BUy3UooLzv/zyC4GB\ngfj5+TFhwgRzJrFz5848/fTTBAUFsW7dOvNzCwoKGD9+PM8995zFeHbs2MGdd94JwDfffIOzszM5\nOTlkZWWZn1uUoSo5ziLPPvssvXr1om/fvhV+Pi1btuSGG27g7NmzgKl82F133UVISAghISHExMSY\n24cOHUrPnj159NFH6dSpE8nJyeX+7Ip76aWXCAkJwdfXl4kTJ1J0MH/Jn++cOXNYuHAhZ86cISAg\nwPzHYDBw4sQJqz//hIQEli9fzqJFiwgICCAqKsrcD0BsbCx9+/bF39+fO+64g0uXLpmf/fTTT9O7\nd2+6d+9OVFRUuZ+VLeo8SFNKNQG+BKZrrS9XpQ+t9bta62CtdXDr1q1rdoBCiEYpJaNyxdWLmA6/\ntavbTFo1y2HZqmvXruTn55OUlMSqVatwc3Njx44d7Nixg5UrV3L8+HEiIyOJj49n+/btxMbG8scf\nf7BlyxYA4uLiePzxxzl06BDNmjXjnXfesfqcjh07MmDAAD766COL9vL6Lk9OTg47d+5k5syZDBgw\ngG3btrF7927GjRvHa6+9VuH9dnZ2PPXUU7z88ssW7cnJycydO5eNGzeya9cugoODeeONN5g2bRrt\n27dn06ZNbNq0ibCwMPN/wKOiomjZsiWJiYlERUURHh5OVlYW48eP5/PPP2ffvn3k5eWxbNnfq31a\ntmzJrl27GDduHGDKGN13331069aNuXPnWowpMDCQ2NhY87N8fX3ZsWMHv//+O3369LG4tuQ4wVR/\ntG/fvuzZs4fw8HBWrlxZ7mdz8uRJsrKy8Pf3B+CJJ57gySefZMeOHXz55Zc8+uijALz44osMHjyY\nAwcOMHr0aE6ePFnh517c1KlT2bFjB/v37yczM9OiPmvxn2+R9u3bExsbS2xsLI899hh33XUXnTp1\nsvrz79y5M5MnT+bJJ58kNjaWsLAwi2c/+OCDvPrqq+zduxc/Pz+LbHJeXh7bt2/nzTffrJEsc53u\n7lRKGTEFaJ9orb+yckki4FnsdYfCNiGEqFWpVajbCX8XWa/T+p2/vAS5mZZtuZmm9irudK1IZGQk\ne/fuNa8ZSk1NJT4+nsjISJL3FKgAACAASURBVCIjIwkMDAQgLS2N+Ph4OnbsiKenpzlzdP/997N4\n8WL+/e9/W+1/9uzZ3H777dx6660Wzyyr7/Lcfffd5q9Pnz7N3XffzdmzZ8nJyaFLly42fb/33nsv\n8+bN4/jx4+a2bdu2cfDgQfP3lJOTQ2hoaKl727VrR1paGleuXOHUqVPce++9bNmyhaioKO68807i\n4uLo0qUL3bt3B+Chhx5i6dKlTJ8+vdT4ASZNmsTYsWN59tlnSz3L3t6e66+/nkOHDrF9+3ZmzJjB\nli1byM/PLxV8WOPg4MCIESMAuPHGG9mwYYPV6z7//HO2bNnC4cOHefvtt3FycgJg48aNHDx40Hzd\n5cuXSUtLIzo6mq+//hqA4cOH07x58wrHUtymTZt47bXXyMjI4OLFi/Ts2ZPbbrsNKP35FBcTE8PK\nlSuJjo4GKv/zT01NJSUlxVzw/qGHHmLMmDHm94uyljfeeGOF6xRtUZe7OxWwCjiktX6jjMu+BR4s\n3OXZF0jVWp+9aoMUQjRaReecuVcySAPTDs9LdTndWc1yWLY6duwYBoOBNm3aoLVmyZIl5mzF8ePH\nGTZsGFprZs+ebW4/evQojzzyCGAKaIsr+bq4bt26ERAQwNq1f2cDy+rb3t7eYkNDVlaWRV+urq7m\nr//1r38xdepU9u3bx4oVK0pdWxZ7e3tmzpzJq6++ajGeoUOHmsdz8OBBVq1aZfX+fv368f777+Pl\n5WXOrG3dutUc4JWn+PiL+tq0aVOZYw8PD+enn37CaDQyZMgQoqOjiY6OtilIMxqN5p+LwWAocx3f\n3Xffzd69e/ntt9945pln+OuvvwDTNOy2bdvMn0liYmK5mxEq+tkVtT3++ON88cUX7Nu3j8cee8zi\nupKfT5GzZ8/yyCOPsHbtWvMYqvrzL4ujoyNQ/mdVGXU53dkfeAAYrJSKLfxzi1JqslJqcuE1PwLH\ngKPASuDxOhqrEKKRSck0BVmVne4EaOFqrNtMWjXLYdni/PnzTJ48malTp6KUIiIigmXLlpGbawpu\njxw5Qnp6OhEREaxevZq0tDQAEhMTSUpKAkxTY1u3bgXg008/ZcCAAeU+89lnnzWvCwLK7Ltt27Yk\nJSVx4cIFsrOzLabCSkpNTcXDw7TU+YMPPqjUZzB+/Hg2btxI0Vrovn37EhMTw9GjRwHTVOGRI0cA\naNq0KVeu/F3TNSwsjIULFxIeHk5gYCCbNm3C0dERNzc3vLy8SEhIMPfz0UcfmTM31jzyyCPccsst\njB071mpgEBYWxptvvkloaCitW7fmwoULxMXFWaz9KlJynJUVHBzMAw88wFtvvQXAsGHDWLJkifn9\noqnX/v37mwPuyMhI87ouW352RYFUq1atSEtLs2nHZ25uLmPGjOHVV181Zyih7J9/WZ+Dm5sbzZs3\nN09XV/Szqa663N0ZrbVWWmt/rXVA4Z8ftdbLtdbLC6/RWut/aq2v11r7aa2rt89YCCFslFrFNWlQ\nD+p3VrMcVlkyMzPNR3AMGTKEYcOGmRfrP/roo/j4+BAUFISvry+TJk0iLy+PYcOGce+99xIaGoqf\nnx+jR482/8fPy8uLpUuX4u3tzaVLl5gyZUq5z+/ZsydBQX8fl1lW30ajkeeff57evXszdOhQevTo\nUWafc+bMYcyYMdx44420atWqUp+Hg4MD06ZNMwedrVu3Zs2aNdxzzz34+/sTGhrK4cOHAZg4cSLD\nhw83L8gPCwvj1KlThIeHYzAY8PT0NAepTk5OvP/++4wZMwY/Pz/s7OyYPHmy9UEUmjFjBoGBgTzw\nwAOljkXp06cP586dIzw8HAB/f3/8/PysZi5LjrMqnn76ad5//32uXLnC4sWL2blzJ/7+/vj4+LB8\n+XIAXnjhBSIjI/H19WXdunW0a9eOpk2b2vSzc3d357HHHsPX15eIiAhCQkIqHNNvv/3Gzp07eeGF\nF8ybB86cOVPmz/+2227j66+/Nm8cKO6DDz5g1qxZ+Pv7Exsby/PPV+/fVXlU0Y6IhiI4OFhX98wY\nIYR4a2M8izYeIX7ePzAaKvf/s89/s59vYs+w54VhNTaeQ4cO4e3tbfsNe9ea1qClnjZl0G5+vtbW\no1VFQkICI0aMkCLrjVR2djYGgwF7e3u2bt3KlClTzFm2hszav2Ol1B9a62Br10tZKCGEsCI1M5cm\njvaVDtDAlElLzcwlL78A+yrcXyOqUQ5LiNp28uRJxo4dS0FBAQ4ODhXuGm2sJEgTQggrUjJzqjTV\nCX+flZaSmUurJo41OawGo3PnzpJFa8S6devG7t2763oY9V6dn5MmhBD10eXM3Eofv1FE6ncKIWqC\nBGlCCGFFamZulY7fgNqr39nQ1hAL0ZhU5d+vTHcKIYQVqZm5dG1VcXFpa4rqd1o7hmP97kQW/BzH\nmZRM2rs7MyvCi1GBpardleLk5MSFCxdo2bJlueeJCSHqH601Fy5cMB/yaysJ0oQQwoqUjFzra9Ly\nsiHmLfjzV7h9KbS8vtQlRWvSSh5ou353IrO/2kdmrqkwdmJKJrO/2gdQYaDWoUMHTp8+jdQnFuLa\n5OTkRIcOlTurUII0IYSwIjUzF7eSxdUTYuD76ZB8BOyd4P1/wIPfQBvLLfXNC6c7S2bSFvwcZw7Q\nimTm5rPg57gKgzSj0WhzySIhRMMga9KEEKKErNx8svMK/s6kZVyEb/4Ja26B3Cy4dx1M/C+g4P1b\n4Izl+U5ORgPORkOpjQNnUkrU06ygXQjRuEmQJoQQJRTV7XRzNsKBr+HtYIj9DPo/Af/cBt2HQZse\n8PCP4OAKH4yEk79b9NHC1YGLJaY727uXqAJQQbsQonGTIE0IUWXrdyfSf/6vdHnmB/rP/5X1uxPr\nekg1IiUzF1cyCT/4AqwbD+4dYdJ/YehLpqCsSMvr4eGfwLUlfHQHHPuv+a3mrsZSmbRZEV44Gw0W\nbc5GA7MivGrz2xFCXKMkSBNCVEnRIvjElEw0fy+CbwiBWt7J7fzg8D94nvoGwmfBIxugnZ/1i909\nTYGae0f4ZAwk/gGY1qVdLHEEx6hAD1650w8Pd2cU4OHuzCt3+tm0u1MI0fjIxgEhRJVUZxF8vVWQ\nD1Fv0GPzK5xRzfnz1s+5IdiG+ptN28H4H+DdgbB2PEz6Ly1cHThxIaPUpaMCPa7dz0cIcVVJJk0I\nUYot05gNbhF8yklYcytsmsvp64ZxS/YrGLv0t/1+15YwZg1cOQtfT6aFs71UHBBCVIsEaUIIC7ZO\nYzaoRfD7voBl/eGv/XDHCjZ4v8xlXCtfu7PDjRDxMsT/zJBL/8eV7Dxy8gpqZ8xCiAZPgjQhhIXy\npjGLq8wi+Hq7wSArFb6aCF8+YjrrbEo09BpHalYeSkFTpyqUher9GPS8k9CEd+hrd5CUDMmmCSGq\nRtakCSEs2DqNWbSuqqISR9U5Zb+yKlVy6eQ2+OoxSE2Egf8DYTPBYPqVeDkzl6aO9hjsqlB+SSkY\nuZiME7tZcmUJqefvoE2zG6rxXQkhGisJ0oQQFtq7O5NoJVCzNo1pyyL4q7XBwOZgMC8H/jsfoheB\nmydM+A949rboKyUjp3S1gcpwbMrRgUvx+m4UDpGPw8QfzQGgEELYSqY7hRAWZkV44WRv+auhOmd5\nXa0NBjZN0yYdhvduhqjXIeA+mBJTKkCDwpJQlV2PVoKjhx/P5k7A7dzvsGlutfoSQjROEqQJISyM\nCvRg6uC/p+eqe5bX1dpgUG4wWFAA25bBinC4fAbGfQq3vw2OTa3ek5qZi7uzQ7XG08LVga8Kwonv\ncKcpaxf3n2r1J4RofCRIE0KUYrAz/Wpo4epAzDODqzUtebVO2S8r6Ovllg4f3wH/eQauHwSPb4Ue\nt5bbV01k0twLp0s3dJxhOgj360lw6US1+hRCNC4SpAkhStly5DwA6dl51e6r6JT9oqCnhYtDrZyy\nXzoY1Ix22Mragplwagfc9hbc83/QpE2FfaVm5tKsutOd9gaaONpzPtsOxn4IusBUYiovu1r9CiEa\nDwnShBAW0rPz2HniIkaDIjuvgPwCXe0+R/Zqb84sOTsYiOjZrtp9ljQq0IOX7/AFwI00Vrm8w0K7\nJTi0Kzxa48bxpp2XFdBam6Y7q7NxoJC5fmeLrnD7UjizC/3zs+w6eYm8fDk/TQhRPgnShBAWth27\nQG6+pt/1rQDIyKk4m1ZRwPHL4SROXMjgodBOJKZksmzz0RoZa0k3ebUh3G4PPzs+zWB+h5ufN9XV\nbNHV5j4yc/PJzdfVnu4EU9bQXL/TZyT0/Sdqx0pWL3+dz3eeqnb/QoiGTYI0IYSFqPhknIx2hHdv\nDUBmTn6513+87QT9X/2VC2llT+Otij6Gh7sz/zvCh9sD2rN8yzFOXEiHvWthkS/McTf9vXdt1Q++\nzUmHH2bwocOrXNYuRPb71HT2mZ2h4nuLSc00BVU1EaQ1d3WwKA11PPApdunuzDeuZPeu7dXuXwjR\nsEmQJoSwsOXIefp2bUnzwum+jAqCtGPn0zl3OZv5Px22+v6BM6lsO3aRh/p1wt5gx//c4o3RTvHi\nJxvhu2mQegrQkHqKvG/+RfTX71RYkqqUU9th+QCaH/yYlXm3MDJ3Ht8lta7Cdw8pGTUXpLVwceBi\nYZCWk1fAE+v287SaAfaOPHb2RS6lpFT7GUKIhkuCNCGE2amLGRxLTie8W2tcHEwZqPQKpjuLpkPX\n/XGaHQkXS72/Kvo4Lg4G7g7pCEDbZk5MH9KdX8/YszHL2+Ja+/wspvN/Fm3WSlKZ5eXALy/B6gjI\nzyOy9yrm5d1PyA3tiTmaTEEV1tMVZdLcayqTVlgW6o0NR9h7OpWZowdyfuhSuqnTpHzxRLWfIYRo\nuCRIE0KYbYk37eoM794aFwfTCfkVTXem5+RznZsTHu7OPPf1fnKLrU9LupzFd3vOMDbY0yIzNb5/\nZ7qp07yY9yBZ2jIYaq8ulHqG1TPQzh2A9wZbHEy7x64n9naKUQEeXMrI5cCZyzZ/70WKgrTq7u4E\naO5iJCMnn18Pn2PFlj+5p3dHhvteR+c+I1hjP4Yup9fDro+q/RwhRMMkQZoQwizqSDLt3Zy4vrWr\nOZNW0XRnRnYeLZs48MJtPsSdu8KamATzex9vO0FegWZ8v84W9xgNdrzo/gOndBuW599m8d7pglal\nnmFxBlpBPkS9AStugit/wbjPTAfTOjXjbGoWbZs5Edbd1EfU0fOV+O5NUmtwurO5q+lA3Cf+L5au\nrVx5foQPAEopEntNI6bAD/3jv+GvfdV+Vr1hZZ2hEKJqJEgTQgCmHZoxfyYT3r01SilzJq2i3Z3p\nOXm4ONgz1KctN/dow6KNRzibmklWbj4f/36Sm3u0pXMr11L39fvH/Yyw3847ebczNPs1QrOW4Jv1\nHuG5b1pcZ3Hw7YU/YfVw+OVF6HELPP676e9CZ1Mzae/uRJumTvRo15To+ORKfw7mjQM1cARHCxdT\nkJadW8DiewJxdvh7E8Nwfw+m5TxOln0zWPsQZFU+61fv7F1bap0h302TQE2IKpIgTQgBQOypFK5k\n5Zl3ddqcScvJx9XBgFKKOSN7kl+geem7g6zfncjF9BweGdDF+o3+Y/nfEb4McozjBnWGAU7HGONl\nZJhPO/NxZuaSVL2ug+0rYfkASI6DO9+DMR+Aa0uLLs+mZtHOzZR1G3BDK3YmXKpwurak1MxcDHaK\npo7VL4ju0dw0lqf/0YOe7d0s3gvq2BzVpA3LWj0HlxLg26mgq38mXZ365SXILTE1nZtpahdCVFr1\nfwsJIRqELfHJ2CnoX3g+mq1BWlp2Hp4tXADwbOHCvwbfwMLII2w7dgGf65rRt2uLMu9t23cMK/qW\nbn/y81g2xSXx31kDsU87YyrrdGwzXH+zaWqzWftS92itOZuaxfCeTgAM6NaK96KPsz3hIjd1t32n\nZ2pmLs2c7FE2HHxbET8PN36deRNdrGQSDXaKiJ5teW93Hv8a8r8Yf50Dv6+AvpOr/dw6k3q6cu1C\niHJJJk0IAZiO3ujl6W6e5nNxtG26MyPblEkr8lh4V7q2duVSRi4TBnSpUrAz1KctKRk5nPh1FbzT\nz1TWacQiuP9LqwEawMX0HHLyCmjnZgrS+nRpiYPBjuj4yq1LS6mBup1FlFJ0bd2kzM9guG87MnLy\n2dzyHvC6BSKfg9M7a+TZdcKtQ+XahRDlkiBNCEFKRg57T6cQ3u3vjFNRHcyKMmlFa9KKONobWDim\nF3cEenBbr+uqNJ6bPGClw5tcH/NvaOtjKusUPKHcsk5nU7MAuK5wutPZwUBw5+ZEVXJdWmpmLm6F\na8lqW9+uLXFzNvLTgb9g1DvQ7DrT+rSM0keZXBNufh6MJQrdG51N7UKISpMgTYhGbv3uRAYt3EyB\nhk9+P2E+ONZgp3C0tys3SNNam9akOVqe6h/UsTmL7g7A0b5yp/0DcPBbXN/rz012sSx1GI9+6Hub\nyjr9HaQ5mdsGdGvF4b+ukHQly+bHp9ZgJq0iRoMdQ7zbsvHgOXId3EyF2NOT4KuJUHAN1vb0Hwu3\nLQY3T0CZ/r5tsek92fEpRKXVaZCmlFqtlEpSSu0v4/2BSqlUpVRs4R/53zEhatD63YnM/moflwqP\nnUhOy7E44d/V0b7c6c6iAuzFM2lVlnkJvnwM1j4Abh34T//PWXB5GEeTrZyRZsXZVNN117n/HaSF\n3WDKDMYctT2bdrkGgzRbSlwN923H5aw8th27AO0DYfgrcHQDRL9eI2O46vzHwpP7YU6K6W+QHZ9C\nVFFdZ9LWAMMruCZKax1Q+Ee2CAlRgxb8HEdmrmWmrPgJ/85GQ7mZtKL3iq9Jq5L4jfBOKBz4CgbO\nhkd/oXfv/gBsOHTOpi7OpGRhNChauTqa23q2b0ZzF2OlpjxTMnJwc65+0FkUAFdU4iqsWytcHAz8\ntP8vU0PwI+A7Gja9DMe3VHscdU52fApRZXUapGmttwDX6OILIa59Vk/yL9bu6mggI7vsIC09O6/w\nuioGNdlp8N10+OQucHKDRzfCwGfAYKSdmxP+HdzYcNC2IO2v1EzaNnPCzu7vdWt2dop+N7QiOj4Z\nbcPxFlprLmfl4e5c/TVpFQXARZyMBgb1aEPkgXPkF2jTurvb3oKWN8AXE+Dy2WqPpU7Jjk8hqqyu\nM2m2CFVK7VFK/aSU6lnXgxGiIbE4yd9Ku7ODPRm5NmTSqhKkJcTAsn7wxxroNw0m/tc03VfMEO+2\nxJ5KsWlN2ZnULNq7lf5+wm5oRdKVbOKT0irsIy07j/wCXSPTnRUFwMUN79mO5LRsdp28ZGpwbGJa\nn5aTDl8+Avnl77Ct12THpxBVVt+DtF1AJ611L2AJsN7aRUqpiUqpnUqpnefPV74MjBCN1awIL/Mu\nziLFT/h3MRrILGdNWlHxdZfKTHfmZsLPz8KaW01Zo4d/gmH/D4xOpS4d6tMWreHXQ0kVdvtXapb5\n+I3iBnQznfu25UjFvxvM1QZqIEirKAAublCPNhjslOUY23ibjh05EQOb5lZ7PHVGdnwKUWX1OkjT\nWl/WWqcVfv0jYFRKlSrsp7V+V2sdrLUObt3a9kMrhWjsRgV68MqdfubX5hP+Az0A03RnejnTnUVT\noTZn0hJ3mWpubn0bQh6ByTHQKbTMy3u0a4qHu3OFU54FBZq/UrMsNg0U6dDcha6tXfn5wF8VDi8l\no+aKq1cUABfXxNGe7m2bsud0quUbvcZB0EMQvQji/lPtMdWJsnZ8+o+t65EJUe/V6yBNKdVOFZ4C\nqZTqjWm8F+p2VEI0LCP8TWeZzRjanZhnBpsDNDBNd5ZcV1VcpTJpe9fBe0Mg+wrc/xXc+rppWq8c\nSimG+rQl+mhyubtML6TnkJNfYHW6E+D+Pp3YkXCJ3/4sfwPB5cJMmnsN1O0sCoA93J1RlA6Ai1u/\nO5ETF9LZcuQ8/V75xXJzwT9eg3Z+8PUkuHSi2uOqEyV3fEqAJoRN6rQslFLqM2Ag0EopdRp4ATAC\naK2XA6OBKUqpPCATGKdtWf0rhLBZ0Zoza4GWi9FQbnBU9J5rRUdwxG+A9ZOhUz+4+2Nwdrd5fEN9\n2rLmtwSi4pOJ6NnO6jV/FZ6RZm26E+DePh1Z/t8/eXNDPKFdW5ZZAaAmpzvBFKhZC8qKK9oFWhQM\nn0nNYvZX+8z3Y3QyrU9bMRDWPQQTfgZ7x3J6FEI0FHW9u/MerfV1Wmuj1rqD1nqV1np5YYCG1vpt\nrXVPrXUvrXVfrfVvdTleIRqiogLkTkYrQVoFuzvTCt9zcSwnk3byd/j8AWjbE8Z9WqkADaB3lxY0\ndbIvd8rzTOEZaWVl0pyMBv456Aa2J1zktz+tJ+OLgiWA8e9vt3qmWW2waRdoi64waimc2W1azyeE\naBTq9XSnEKL2Fe3QtJpJczCQkZtf5vEVGdmlM2nFD3B94OX3yflotKne5n1fglOzSo/PaLBjkFcb\nfj2cZDqiwoqKMmkAd4d40q6ZE4s2HCn1/RQFaCmFmbRzl7OtnmlWG2zeBep9G4ROhR0rYd8XtT4u\nIUTdkyBNiEYuo5x1ZS4O9uQXaHLyrZcoSi8M8IoWyBc/wNVDJbEwew6XcgxE3rgcmlR9U89Qn7Zc\nTM/5+4iKEs6kZuJgsKOla9nnmzkZDfxz8A3sPHGp1OG2tp5pVhsqswuUIXPAsw989wQkx9fquIQQ\ndU+CNCEauazC4MTZyrqyosCtrCnPjOw8XBwM5gNki4KdFlzmI+MrOJLLAznP8GJUxWeUlecmr9YY\n7BT/jbN+jMbZFNPxG8UPsrVmbHAH2rs5sWijZTatMmea1TRru0Cd7O2s7gLFYITR75vWpK19EHIy\nan18Qoi6I0GaEI1cRdOdQJkH2qbn5FvU7SwKambar6O9usCEnFkc0Z7VDnaaORnp2b4Z2xOsFygp\n64y0khztTdm03SdT2FzsTLJWTawvxC8ry1WTiu8CLTJ18A2MMsRYL0ru5gF3roSkQ/DDTLBxL5Ut\ndUSFEPWLBGlCNHIZJaYsiysKwIrWnpW+Nw/XYpsG2rs7044LjDb8l3X5N7FLdze3V1fvzi2IPZVi\nzvwVdyY1k/Y2BGkAY270xMPdmTcL16Z98vsJLmbkUDIHV9aZZrVhVKAHMc8MZsusQQC0SNlXflHy\nG26Gm56CPZ/C7o8q7N/WOqJCiPpFgjQhGrlMWzJpZRRZT8/Ot9g0MCvCi8cdfsRAAcvzRwI1F+z0\n7tKCnLwC9pY48LWgQHPuchbtytjZWZKDvR0DbmjFntOpdJn9I89+vZ/ubZsw7w5fm840q02eLZxp\n7mJkz949FRclv+lp6DoQfpwFf+0rt9/KrLmTjJsQ9UednpMmhKh75kyalSDNuYIgrWQmbVQ3I3nG\nTfzMQBKzW+Ph7sysCK8aCXZCOrcAYPvxC/Tu0sLcnpyeTW6+pr2VagPWrN+dyDexloHH8fPpuDjY\nE/PM4GqPszqUUvh3cGdPfDuwNgNbvCi5nQHufA9WhJnWp03cbCpSb4Wta+5KntlWlHEDrnrAKoSQ\nTJoQjZ55d6ex9P+zFWXJyjrQtuSaNLYuxT4/m1unLOD4/FtLVTCojuauDni1bcr2BMsdnmdTTMdv\nXGdjJm3Bz3Fk5VnuVs3KK7gqOzlt0cvTnSO6AxnaSpRWsih5k9amjQSXTsA3U8tcn2brDtK63OUq\nhChNgjQhGrnMcjJpFU13ZmQXy6RlXIQd74HvndDqhloZa+8uLfgj4SJ5xY4EOZtaFKTZlkmry52c\ntgjwdKMAO/bblZgiLqsoeadQ09Ech76F35db7dPWOqJlfQaJKZkyBSpEHZAgTYhGLjM3H3s7hYN9\n6V8HRYFbZpnTncUyab+vgJw0CJtZa2MN6dKC9Jx8Dp69bG47W1htwNYgrVLnktUB/w6migx7ejxp\ne1Hyfv8Cr1sh8jk4taPU27bWES3rM1Agmw6EqAOyJk2IRi4jJ99qFg3+nu5ML2O6My07D1cHA2Rd\nht+XQY8RpvJPtaS3eV3aRXMwczY1Cwd7O1qUc5BtcbMivCzWXcHV3clZkVZNHPFwdyY2/zpTMXJb\nKAWj3oEV4bBuPEzaAq4tLS6xpY6otc8GoOQkatEUqKxTE6J2SSZNiEYuMyff6s5OsG3jgIujvWma\nMyu1VrNoYCr71KmlC78f//u8tLOpWVzn5lRm0fSSbM0q1aUAT3f2nEqp3E3O7jD2A0hPgq8egwLr\nVSLKMyrQgwkDOtt0bX2ZHhaiIZNMmhCNXEZuicX/xTja22GwU1anO3PyCsjN17gZcmDr23DDEPAI\nqu3h0rtzCzYcOkdBgcbOTnE2JZN2zWyb6ixiS1apLvXydOOHfWe5kJZNyzIO2rWqfSAMnw8/zICo\n1+GmWZV67tnUTP5v+ymub+3K+n/2Z33sGZ5fv79UJg3qz/SwEA2ZZNKEaOQyc/JwsnKQLZiOhHAx\nGqxOdxbt+Aw4/y1kXIDwygUEVdW7SwtSMnI5et5UaupsalaDCxiKpnJLnglnk+AJ4DcGNr8Mx/5r\n8205eQU8/skusnLzWfHAjTR1MvJA3048e6s3Jatt1afpYSEaMgnShGjkMsqZ7gTTlKe1TFp6Tj72\n5OF36mPo2A869q3NYZr16WJaa/X78YvkFx5ka+umgWuFn4cbdgpiKzvlCab1aSPehJbd4MtH4PJZ\nm277f98fZPfJFBaM6cUNbZqa2x8N68rCMb1o4mjKtrZu4ljvpoeFaKgkSBOikcvMLT9Ic3W0t7om\nLSM7j1vsfsc18yz0n1abQ7Tg2cKZds2c2H78Islp2eQV6AYXpLk62tOtTVP2nK5CkAbg2ATGfgg5\n6aZALd/6xo8iX/5xmo+2nWBieFdu8buu1Pt3BnXgs8dMQfi8O3wlQBPiKpEgTYhGLjMn32rdziLO\nRoPVw2zTs/OYZP896c26QreI2hyiBaUUIV1asP34BfPidVsPsr2W9PJ0Y+/pVLSNBdRLadPDlFE7\nEQO//r8yL/vjxCX+yWS17gAAIABJREFU5+t99O3agqfKmcJs28y0Nu7cleyqjUcIUWkSpAnRyFU0\n3eniYLCaSTOe2EJPuxOc6/kY2F3dXyW9u7Tg3OVsthfu8mzXwDJpYFqXdjE9h9OXqrGLstfdcON4\niHkT4v5j8VZOXgGvR8YxdsVWWjVxZMk9Qdgbyv45tmziiJ2CpMtZVR+PEKJSJEgTopEznZNW9kZv\nF0d70q0EaW33v0uSdifTe3RtDs+qPoW1O7+JPQM0zJ2GAZ6mzQNVWpdW3PBXoZ0/fD3JVD4K2J+Y\nysi3o1ny61FGBXjw47QwWjctfxepwU7Ruqkj5yRIE+KqkSBNiEYuMyev/Eya0UBmyenOv/bR6lw0\na/IicHZ2qeURlnZD6yY0dzFy8OxlHO3taO5ivOpjqG1e7ZribDTw6+Gk6nVkdDKtT9OagrUP8dbP\n+7l9aQwX03NY9VAwr4/thZuNn1/bZk6cuyzTnUJcLRKkCdGIaa3JyC1/TZrV6c7flpBrcObj/Jtx\ndbz6xy3a2Sk6NDcFh9l5BQx4dVODK1NkNNhxT++OfBObyPHk9Op11qILjHoHu7O7cY+aw+0B7dnw\n5E3c7N22Ut20aeokmTQhriIJ0oRoxLLzCtDaenH1Ii6OJYK01NOw/0sOt7+TyzQpNwtXW9bvTuRw\nsfqdDbWe5OSBXXGwt2PJL/HV78x7BN+43MlD9ht4w+dPm7NnxbVt5kiSbBwQ4qqRIE2IRqzo/LPy\nNw7YW+7u3LYMtGZ727vN719tC36OI7fActdjUT3JhqRNUyfu79OJ9bGJ/Fl4eG9Vaa2Zkz6aE67+\n8O00OH+k0n20bebExfQcsvOslwkTQtQsCdKEaMQycm0J0gxk5RaQX6AhMwX+WAO+d3LOrg1ORlPZ\nqKutrLqRDbGe5KSbrq+RbFpiSiaXsuGPkIVg7whrHzSdo1YJRcdwnJdsmhBXhQRpQjRiRRsCyt3d\nWRjAZebmwx/vQ04a9JtGenYernWQRYOyd3M2xF2erZs68mBoZ77dc4ajSVXPpsX9dQWAjp27wV3v\nwfnD8MNMqMQ5bG2amo46kc0DQlwdEqQJ0YgVrTVzKe8w28JALCMjHbYth64D4Tp/0/lqjld/PRrA\nrAivUpsdGnI9yYnhXXG0N7C4Gtm0w4VBWvd2TeH6/8/eeYc3Ve9//HWSNEnbdEBLgbZMWbI3iBtU\nwAGICogDHKjX67j6E0Wv13W9F6/jet0DQcGBICKKoIjgRNll7w0tpaXQNm2TZp3fHydJV5ImbVbb\n7+t5eErPOTn5QptzPucz3u/hcPFjsHU+bJ7n9znS3Jk0MTwgEIQDEaQJBE2YMj960uKd+1Q7FkFJ\nLgxTLKAimUkb1y+DmeN7kZEciwRkJMc2aj/JVIOOW4e1Y+m2HPafMlbZtyQrm/NfWE2HGcs4/4XV\nXocn9uYayUiOJVHvHBi4+FEl4F4+HU5u82sdLRNFJk0gCCeRucIKBIKowDU4oK+lJ03CgWHzu9Cy\nl5KFoXanglAzrl9Gow3KPHH3Refw8Z9HeW3Vft6c3B9QArTHF29XStFUTLkCNf5v9uYa6dqqwjgd\nlZrlXf7JoENjKX3neu6KfYV7R/X3+X/aPE6LRiUJGQ6BIEyITJpA0ITxJ5MWq9VwiWor+rP7YNj9\nICmDAqUWW0Q00poqzeO1TBnWnmXbT7LPmU17acVed4DmwtOUq8Xm4GB+SZUgbUlWNv+3LId7yu8n\nU8rnobLXeHzxNp8yJiqVRFqCTmTSBIIwIYI0gaAJ47rBx8V4D7bitWru1nyLOa419Bzv3l5Wbo9Y\nubOpkpkcCzJc8eqvDJu5imw/p1wPnS7B5pDpVilIcwV4m+Su/Mc2idHqDdzoWFarjElaop480ZMm\nEIQFEaQJBE2YiulO75m0ZoU7GKrazZFOU0BdIYBaarFFbHCgKbIkK5vnl+3GNYuZU2TGm/hJ9SlX\n12Rn5Uxa5UBulv0qVtgH8rjmM1oWbfW5jpaJwr9TIAgXIkgTCJow/pQ7W+54n2I5jv2Z46tsL7OI\nTFo48VTalKFGoOZpynVPrhGNSqJjqsG9rWogJzHdejc5cgrv6N6A0gKv6xD+nQJB+BBBmkDQhHEF\naV69O88cJv7gMj6zj6DIoa+yq7RcZNLCiTehXhlqnXLdm2vknBYGtJqKS351GZNi4nlYfohUlREW\nTwOHw+P7tUzUU2SyYrYK1wGBINSIx2CBoAljstrRx6hQeXMN+PMtkNTMsY1iWiX/TpvdQbnNITJp\nYSQ9OdZjD5pBp2HNjOE+X7s318iAds2qbHMFci+t2EtOoYn05FhuGTkWtV0P3z4Ev72syHRUIy1B\n0UrLKy6nbUpcXf85AoHAD8QVViBowpRZbN6zaGVnIOsT5F43kLeuWRWT9VI/yqSC4DJ9ZNcqchsA\nakmi1GJj45EzDGzf3OPris1WsgtNTB7StsY+jzIm8m1w9E/46d/QZrCipVYJt1aa0SyCNIEgxIhy\np0DQhDFZHN4N0jd8ADYTqvMfQKdRVTFZd/1dSHCED08Cvv+6tidtmsXx0MItGM1Wj6/b5xwaqDzZ\n6RNJgqtfhdQu8OWdUHyyyu4KQVsxPCAQhJqIBmmSJM2RJClPkqQdXvZLkiS9LknSAUmStkmS1D/c\naxQIGjMmq83zZKfVBOveg85XQNq5xGnVVTNp5SKTFgnG9ctgzYzhHH7hKtbMGM6kwW15dWIfss+a\nePF7z9IZezxMdtaKzgAT5ikG7ItuB3tFgO4qd4rhAYEg9EQ6k/YRMMrH/tFAZ+efu4B3wrAmgaDJ\n4NU1YOt8KDvttoCK02qqBGnuTJroSYs4A9o1Z+KgNnyx6ThFpprZtL25RhJ0GjICNZ9P6wbXvAbH\n/oDVz7k3J8fFoFWrhFaaQBAGIhqkybL8K3DGxyFjgXmywlogWZKk1uFZXWjx129PIAglZRZ7zZ40\nhx3+eBPS+0H7CwCcmbSKbIo7kyamO6OCm4a0w2x18NXmEzX27c010qVVApLkTVXNB70nwMDbYc1r\nsGc5AJIkkZaoI09k0gSNmGi5R0c6k1YbGcDxSt+fcG6rgiRJd0mStFGSpI35+flhW1xdcfntZRea\nkKnw2xOBmiDcmDxl0vYuhzMHlSya88ZevdwpMmnRRc+MJHpnJjF//XFkWXZvl2WZPbnFgZU6qzNy\nJrTuA0vugbNHAJdWmsikCRon0XSPjvYgzS9kWX5fluWBsiwPbNGiRaSXUyv++u0JBKGmzGKrOTiw\n5nVIbgfnjnFvitNq3GbsUDHdKQYHoofJg9uy95SRzcfOurflFpspNtv8HxrwRIwebpirCLItnAK2\ncuE6IGjURNM9OtqDtGygTaXvM53bGjTeRCm9bRcIQoXJYkdfudx5bC2cWA/n3QfqigAsTqumtPJ0\nZ7lrulOUO6OFa/qkY9Bp+HTdMfc299BAy3oEaQDNO8C178DJLbDiCdIS9KLcKWi0RNM9OtqDtG+A\nW51TnkOBIlmWT9b2ominuq9ebdsFglBhslYrd655HWKbQb+bqhwXp/OcSfMq3yEIO/E6DWP7prNs\n20mKypQBgr1u+Y3E+r9Bt6tg2P2w4QPON/2CsdxGabmt9tcJBA2MaLpHR1qCYz7wJ9BVkqQTkiTd\nIUnSPZIk3eM8ZDlwCDgAzALujdBSg0p1Oxbw7LcnEISaKtOd+fuUfrRB00AbX+W4uBjPmTQhwRFd\nTB7SlnKbg8VZygDB3lwjrZP0JMXFBOcNRjwNbc/j0n3PcY6UTZ5RZNMEjY9oukdH9DFYluUba9kv\nA38N03LChic7lukju9ZU/hYIQojdIVNuc1TopP35Bmh0MPiuGsfGVtdJs9jRalTEqKM9Gd+06JGe\nRJ82yXy27hhTh7VnT66xfkMD1VHHwPVzcLx1Pm/HvEZ+wWg6pMbX/jqBoAERTfdoUauIEB7tWASC\nMOJqjI3TqsF4CrZ+Dn1vAkPN4Zt4nRqTxY4sy0iSRGm5jXiRRYtKJg9uw2NfbmftoTMczCvhoi6p\nwX2DxHTyr3iLzt9M5sTvf4cuc91TwIGwJCs7Km6CAoEnouUeLR6DBYImiktGI1argfXvgd2q9Bx5\nIE6rweaQsdgdAJR6mgoVRAWuAYKZ3+3GYnfUb7LTCwk9Luc123jaHv8aNs8L+PXRJHEgEEQzIkgT\nCJoorkGABMkMG2YrjeEp53g81tWf4XpNWbldTHZGKXFaDeP6pbPtRBEAXVsGYWigGgk6DbNU13Eo\ncTAsnw4ntwb0+miSOBAIohkRpAkETRRXj1nnnCVgLoTzH/R6rCsgc71GZNKim8mD2wGgVkmckxb8\nnjFJkmiRGMesFk9AXIqin2Yu8vv10SRxIBBEMyJIEwiaKCarHTV2Oh6YB22GQpvBXo+NdQZkrhJp\nmUVk0qKZ7umJ9G+bTJeWCeg0ofk5tUzQc6hMDzd8CIXHYMm9UMntwBfRJHEgEEQzIkgTCJooJoud\nK1XriC09Aec/4PPYuJhqmbTyMGfSti2EV3vCM8nK120Lw/feDZR3bx7A+7cMCNn50xJ1igRH26Fw\n+bOw51tY+7Zfr40miQOBIJoR9QqBoIlSVm7jLs23mJPOQd9ltM9j46qVO8ssdgzhsoTathCWPgBW\nZyms6LjyPSgG4AKPpCXqQ3r+lol6Vu/JUyZ+z7tPcatY+RRkDIS2Q3y+NpokDkBMmgqiFxGkCQRN\nlLicNfRSHSGv34voVb6T6nE1yp228AnZrnquIkBzYTUp20WQFjFaJuoos9gpKbeRoI+BsW/B+xfD\notvg7l8h3rf0R7RIHLgmTV2DDK5JUyAq1ido2ohyZ30QJRhBA6bDvtnky0nYetYe6Lg00SrKnfbw\nmasXnQhsuyAstHRm6k65PDxjkxUj9tLTsHgaOOw+Xh09iElTQTQjgrS64irBFB0H5IoSjAjUBA2B\nUztJz1/DR7aRxMXVPv0XWylIszvkmp6foSQpM7DtgrCQlqAEaXnF5oqN6X1h9AtwcDX8+nKEVhYY\nYtJUEM3UGqRJknS/JEnNwrGYBoWvEoxAEO388QZWVSyf2C9DH1N7sOUud5bb3FmH+HANDox4CmKq\nTf3FxCrbBRGjZaIOgFNGc9UdA26D3hPh55lw8KcIrCwwxKSpIJrxJ5PWEtggSdJCSZJGSVId/D8a\nI6IEI2ioFGXD9i/YljYGo2RAp6n9MuDKmpVZ7ZS6zNXDJcHRewJc8zoktQEk5es1r4t+tAiTVr3c\n6UKS4OpXoUVX+PJOKM6JwOr8x9OkqVqSxKSpICqo9eosy/KTQGdgNjAV2C9J0r8lSfIsTd5UECUY\nQUNl3Tsgy/yeOpE4rQZ/nrt0GhUqSXEacAVpYcukgRKQPbQDnilUvooALeIYdBoMOg2nis01d2rj\nYcI8pbqw6HbFcixKGdcvg39cfa77+3idGocs0ynNEMFVCULFkqxszn9hNR1mLOP8F1ZHvRWZXz1p\nsizLQK7zjw1oBiySJOnFEK4tuhElGEE9OHG2jDdX70f2U/wzaJiLYONH0GMcuaoW7l6z2pAkiTit\nhjKL3T08ELaeNEHAhOtGlJaoI696Js1Fi65wzWtw7M+obwPJbBYHwKd3DuGPGSNoHq/l2aU7w//5\nFISUhugZ609P2oOSJG0CXgTWAL1kWf4LMAC4LsTri15ECUZQD5ZtO8nLP+wjv8TLDS5UbPoILEYY\n9gBllsCa/+O0akxWW0UmLVzTnYKACOeNKC1B5zmT5qL3DTDwdvjjddizPOjvHyyyjhUiSdA7M4mk\n2BgeGdmVDUfO8vQ3OxtU1kXgm4Y4yetPJq05MF6W5ZGyLH8hy7IVQJZlB3B1SFcX7YgSjKAOLMnK\n5o3VBwC4+vXfw3fht1lg7bvQ4SJI70uZxV6jF8cXcVo1peUikxbthPNG1DJRX3NwoDojZ0LrvrDk\nHjhzOOhrCAZZx8/SJS1B0XsDJgxsQ2ZyLB//ebRBZV0EvmmIk7z+9KQ9LcvyUS/7dgd/SQJB48WV\n5ShxZqPyjOUBX/jrXMrasQiMOTBMMVI3BZxJU8qdpRaRSYtmwnkjapmoJ6+43HdZMEYPE+Yqf/9i\nClhrCerCjCzLbDleSL+2ye5tapWE2Wan+r8q2rMuAt80xEleoZMmEISR+mY5lmRl89iX2wJ/updl\nWPM6pPWATiPc7+tvTxpUlDvLykUmLZoJ540oLUFHuc1Bscnm+8Bm7WHcu3ByK6x4POjrqA9HCsoo\nLLNWCdIACkosHo+P5qyLwDcN0TNWPAoLBGGkvlmOl1bspdzmqLLNFeT5tLDZvxLydys3Suc0Z5nF\nTrM4rX8LRxG0NZpt7kxa2Lw7BQExfWTXKjZHELobkct1YOXuU7RLiUOjkohRq9CoJSw2B2arA7PV\nrvyx9eXcTrfTeeMc1tu7cjTjKlSSxPBuaTSL9//3MNhkHTsLQL+2VeVA05NjyfbwuYzmrIvAN9Hm\nGesP4iorEISR+l74Pb0W/Ajy/ngdEtKhZ8WsjylA/814rYa84vJKPWni8hGNhPNG1D5Fcat45Iut\nfh2v4RI+1f5Bz81P8cRaiQNyJt1aJbDoL8MiFvRnHSskQaehU4uqkhvTR3ZlxpfbMFd6KIr2rIug\ndgLxjP34zyOc3ymVji0iJ8cirrICQRipT5ZDlmW0GhWWapk0qCXIy94MR36Dy/8JmoqMRV2mO8uc\n050xagmtHyK4gsgQLvPynhmJfPfghRSZrNjsMla7A6vdgd0hE6NWoY9Ro4+p+KqSJGJKe6JfcAXf\nNZ/Fzxcv4J6Fe/nb51t4/5YBqFTh10rPOn6W3m2Sary36//v0UXbsNgdZDSArIsgeHy67ij/+Hon\nU85rx7Nje0ZsHSJIEwjCiOsC//DCLThkSNRreG5sT78u/D/uzsNicxCjkrA6Klqaaw3y/ngddIkw\nYGqVzSZLYD1psVo1Zc7pTpFFE4Cin3du68TAXtTCADfMRj1vHJcffIGnrnqCp5fu4sUVe5kxulto\nFuoFk8XO7pNG/nKxZ232cf0y2HzsLIs3Z/Pbo5dGJIgUhJ9vt+Xw5JIdDO+WxpNXd4/oWsSjsEAQ\nZsb1yyBGrXz0bhrazq8AzWp3MPO73XRsEc8L1/VG63x9RnIsM8f38n6OM4dh19dKgKavuJnKskxZ\ngCbp8TrndGe5jXgxNCCoDx0vgUufgO0LuVW7mslD2vLuLwf5clN4bfW2Zxdhd8g1hgYq0yM9kZJy\nG0fPlIVxZYJI8cu+fB5asIWB7Zrx1uT+7mt1pBBBmkAQZsxWu7v53yUMWxufrz/GofxSnhh9LtcN\nyOTyHi3pkBrPmhnDfQd5a98GSQ1D/1Jls8VZkgokIxYbo8ZktWM024gTQwOC+nLhI9DpMqTvHuO5\nASbO65jC44u3s+nombAtwTU00LeNryAtCYCdOUVhWZMgcmw6epZ7Pt5Ep7QEPpgyKKBKQ6gQQVo9\ncDhkXvhuDzuyxYdX4D/F5gofwxJz7UFasdnKqz/uZ2jH5ow4Nw2AFgYdp421uBWU5MPmj6HXDZCY\nXmWX2aIEifoAxWwBCkrLRSZNUH9UKhg/CxJbo1k0lXeubUvrZD13f7yJE2fDk7XKOlZIu5Q4Ugw6\nr8d0bmkgRi2xM6c4LGsSRIa9uUZu/2gDLRN1zLt9MEmxMZFeEiCCtHpxtszC0q05TJmznkP5JRFd\nS0MzjW3KVNaUMtaSSVuSlc2wmas5U2rhYF4pX2/JASDVoMVYbsNcTXOtCn+8DvZyuPDhGrvKrMr7\nBjQ44MyenS6xiJ40QXCIaw4TP4GyApKX383sW/pitjp44bs9IX9rWZbZfOws/Xxk0QB0GjWd0xLE\nw3gUU9/73/EzZdwyex36GBUf3zGEFgneg/ZwI4K0epBi0PHxHYMBuGX2+oiJHDZE09imTJHJv0za\nkqxsZny5ze1OkF9S4U7gevIvKPUsuEnpadjwgSK5kdq5xu66WDvFObNu+cZy4nUikyYIEq37wFX/\nhcO/0mnHa4zpm85Pe/J8P4AEgZNFZvKM5TX00TzRIz2RXTnFwnA9Cqnv/S/fWM4ts9dRbnPw8R1D\naNM8LrQLDhARpNWTji0MzL19MMUmK7fMXscZbzfNENIQTWObMq5yZ3JcjDsA88RLK/ZW0WiCip9r\nqjNI81ry/ON1sJrgokc97jY5g7RAvDtdgVlJuU1k0gTBpd9NMOA2+P1VJidspdRi54+Dp0P6llnH\nCpW39jE04KJnRhIFpRZOFdfSYiAIO/W5/xWbrUz9cD2nisuZM3UQXVomhGqZdUYEaUGgZ0YSH0wZ\nyImzJqZ+uN7njTcUNETT2KZMsTOTlp4U6/N3xdfPNdWg6J2dLvFw0ygtgPXOLFqLLh7PURdB2thK\nx4pMmiDojP4PZAygx/oZ9NLlsWLHqZC+Xdaxs+g0Krq1ql1CpEe6cowoeUYfdb3/ma12ps3dyN5c\nI+/c3J8B7WrPqEYCEaQFiSEdU3hrcn925hQzbe7GkKfqK9MQTWObMq4gLaNZLEYf5U5fP1dXJs2j\nv+Cfb4C1jB/Tpnjt0yhzWjsF6t1Z8XeRSRMEGY0OJsxD0miZrX2ZDbsOYHeErryYdbyQXhlJfoky\nn9s6EUlCDA9EIXW5/9nsDh6Yn8W6w2d4ZUIfLumaFqrl1RsRpAWRy7q35OUbevPnoQIemJ+FzV5T\nGT4UNETT2KZMsTMwS0/SU1Ju9Xrc9JFd0VQTz3T9XF1BWn71TFrZGVg/ixMZo7h/ZZnXPg3XQ0Sg\njgMu4oUEhyAUJGXCpM9Isefxgu0FNh/KDcnbWGwOdmQX+ZTeqEy8TkOH1HghwxGFBHr/k2WZv3+1\ngx92neKZa7oztm90O0iIIC3IXNsvk2eu6c4Pu07x+OLtYWk0Hdcvg5nje5GRHIuEHwKngohSbLKi\n06hIMegwWx1eg/lx/TK4uEsL9/eVf66xWjXxWnXNcuefb4KllBn5o3z2aZTVoSetcvZMSHAIQkbb\noVjGvMNg1V7ilt0HjuA/7O7JLabc5vBraMBFj/QkkUmLQgK9/y3adIIFG49z//BOTD2/Q3gXWwfE\n43AImHp+B86WWXlt1X6S42J44spzkaTQ2omEy6tPUH+KzVYSY2PchtKl5XaS4jw/L7VM0pNq0LHx\nyctq7EtN0FUtd5adgXXvQ49xrNnUosbxUNGnUafpzsrlTpFJE4SQ2L7Xs+DntUw8Owt51bNIlz8b\n1PNvPa4MDfT1Y2jARY/0RJZuzeFsqYVm8draXyAIG/7e/3KLzDz37S4Gd2jOQ5d57teNNkQmLUT8\n7bLOTDmvHbN+O8zbPx+M9HIEUUSRyUpSbAwGvRLoGH2UPItNVhJjPQdEKfHaqpm0tW+DxQgXPVpr\nn4Z7urOu5U6RSROEGMd59/OJbQTSmv/BxjlBPfeeXCNJsTGkJ+n9fk1Pp/PArpMim9YQkWWZxxdv\nw2p38NL1vRuMD6sI0kKEJEk8fU0PxvVN56UVe/l03dFIL8knQgw3fBSbbCTqNe5Mmq8Jz2KzjUS9\nZ+XrVIOuIkgrPAZr34XuY6Fld6aP7EqM2nM/G9RturPysWJwQBBqLuveimfsUznc7HxY9gjs+yFo\n5953ykjXlgkBVThcE56iL61h8uXmbH7am89jo7rRLiU+0svxm4gGaZIkjZIkaa8kSQckSZrhYf9U\nSZLyJUna4vxzZyTW6ZOcLLB51kZTqSReuqEPw7ul8eSSHSzdmhPmxfmHEMMNL9XLnb4EbZVMmpcg\nLUHH6RILWM2w4BaQJBjxNKCk/y87t6X72LQEXZU+jTKrDa1GhTqAp0m1SnJPwgkJDkGoaZGgo2/b\nVP5P/hu07AFfTIXsTfU+ryzL7M010qWVIaDXNYvXkp6kZ0e2yKQ1NHKLzDy7dCeD2zdnynntI72c\ngIhYkCZJkhp4CxgNdAdulCSpu4dDF8iy3Nf554OwLrI2SvLhw6tg3lhF4d0DMWoVb03uz8B2zXh4\n4RZ+2Zcf5kXWTlMQw42mTGHNcqevTJqVRL3nrFWqQcfZMguO5dPh5Ba49j1IOce9v7L33AvXVW2k\nNVnsAfWjuXCVOUUmTQCh/1yN7NGKzblWsq/6GOJT4dMb4PSBep3zVHE5xWYbXesgXNojI0lk0hoY\nsizzxFfbsdod/KcBlTldRDKTNhg4IMvyIVmWLcDnwNgIridwDC1gzOuQsxnevxRyt3s8LFar5oMp\ng+iUlsA9H29i09EzYV6obxq7GG60ZQqLTVYS9TEk+JVJs3nPpBm0TFD9hCprHlz4CHS7ssr+nCIz\nGc4etCOnqxpWmyx2t81TILiCM5FJE4Tic1U96HNVI78/KsMtXwESfHItGOsudLv3lBGgTuryPdIT\nOXS61K0zGJVsWwiv9oRnkpWv2xZGekURZfHmbFbvyWP6yG50SG04ZU4XkQzSMoDjlb4/4dxWnesk\nSdomSdIiSZLahGdpAdDrerjtO3DYYPYVsOtrj4clxcYw7/bBtEzUcduHG9iTGz0p83CJ4UYqmxVN\nmUJZlpU+s1iNO5NWWmsmzXOQdo51H89pPqIk40K49Ika+3MKTfTMSCRBp+HYmapBWpnVjr4OmTRX\n9i1eZNKaPMH+XHkK+l75YR+tk/Ss2JmrZIlvWqg4anx6HZjrdg3dl1ufIC0JWYbd0To8sG0hLH0A\nio4DsvJ16QNNNlArKCnn2aU7GdS+GbcNax/4CUxnQyIBEwjRPjiwFGgvy3JvYCUw19NBkiTdJUnS\nRkmSNubnR6CcmNEf7vpJ6ZtYeCv8/ILHH2yLBB0f3zGEOK2GW2av51hBmYeT1U6wg51wiOFGMpsV\nTZnCUosdu0NWyp21DA6YrXYsNofn6c7SAgaue5B8ktg+9BVQVf35ybLMyUIT6cmxtEuN40hBaZX9\ndSl3LsnKdp/nhnf/FD2LTZxgf668BX0lZhsbj5yhoKQcMgbAxHmQtxsW3AS2wL00954ykpagq5OM\nRs8M1/BAlAb1b2DGAAAgAElEQVRpq55TPHsrYzUp25sgX2VlU2y28fy4XoGXOfP2wHsXw+p/hmZx\nfhLJIC0bqJwZy3RucyPLcoEsy65P4QfAAE8nkmX5fVmWB8qyPLBFC8/6UCEnoRVM+Rb6TIafZ8IX\nU8Ci3NAqB1WT3l/L1GHtsdod3Dx7HXnF5oDeJhTBTjjEcCOZzYom2yyXJVSiPsadjfJmDeUyYq+R\nSXPY4cs70JpO8xfL38i11UzhF5tslFrspCfF0i4lnqPVHgjKLDbiYvzPhrl+76x2RZw5t9gshkua\nOMH+XHkL7ozlNhwyrNqdp2zodBmMfRsO/wpf3R1wpmNvrpGurepmpN0qUU/zeC07o3V4oOhEYNsb\nOUu2ZNMnMynwn/eBH2H25UqA2+3q0CzOTyIZpG0AOkuS1EGSJC0wCfim8gGSJLWu9O0YYHcY1xc4\nMXoY9zZc8S/Y8y3MHsmKNetrBFWvrdrPbcPac7qknFvnrKeozLtOVnVCFeyM65fBmhnDOfzCVayZ\nMTzowriRzGZFk22WO/CKjUGlkojXqr1m0opNNvexVfjxaTj0E6Yr/sN2uSOnjTWni3OKlP/X9ORY\n2qfEcfxMWRVnA5PFHpBGWjSVjAXRQbA/V16DviQ9GcmxSsnTRZ+JcPk/YedX8P0M8NPZxe6Q2Z9n\nrFOpExRppR7pieyI1uGBpMzAtjdi9p8ysiO7OPB72fpZ8OkESG4L01ZDpsfcUNiIWJAmy7INuA9Y\ngRJ8LZRleackSc9JkjTGedgDkiTtlCRpK/AAMDUyqw0ASYJh98HkL6DwGINXXkcP284qh5isdhZu\nPMH7twzkUH4pt8/d4HcjajSV7gIhktmsaLLNcgdezuyYQa/xOjhQ5M66Vcp4bZkPf7wBg6YRO+Q2\ntGpVTWsoKn4fWifraZcSj80hk1NYkbUtC7Dc2VB/7wShI9ifK29B36OjujGyRyt+O3C66gPN+Q/A\neffB+vfg9//69R7Hz5RhtjrqNNnpokd6EvtOGbHYItur5JERT0FMtWtqTKyyvYmxZEs2apXE1b3T\n/XuB3QbLp8PyR6Dz5XD795Ac+Tb4iHb/yrK8HFhebdtTlf7+OPB4uNcVFDpfBtNWcfb1a/hM+y/+\nYbudBfZL3btzCk1c0DmV1yb15a+fbeYvn2xm1q0D3TpU3khPjiXbw40xEqW7QJg+siuPL95eJRsT\nzmxWtNhmuQIvlzyGQafxnkmrlHUD4PgGpQm4/YUwaiaSJJFq0CpaadXIKVICsozkWGzOEuWRglLa\npsQBSpAWSCatof7eCUJLMD9XrvO8tGIvOc5+yukjuzKuXwbpybHMWXOYn/bkcU2fSjfdy/8JJXlK\nz1V8GvS/xed7uCc761juBGXC02pXMnI9nC4EUUPvCcrXVc8pJc6kTCVAc21vIjgcMkuycrigUyot\nEnS1v8BcBItuV8qc590Hlz9Xo883UkT74EDDJrUz98a+yJ+OHvwnZhZPa+aiRglSXDe30b1aM3N8\nL37Zl8/DC7dgd/hO20dT6S4QoimbFUncPWnOYQCDPsarTlrl/jWKspVG6cR0mDAP1ErgllLZdaAS\nOYUmYtQSLQw62jsDs6OVhgfM1sAyaQ31907QsPDWdjGgXTNSDVq+r1zyBFCpYOxbcM5wWPog7P3e\n5/ldk52d0wITsq1MzwwlMNuRHaUlz94T4KEd8Eyh8rWJBWgAG4+eJbvQxLX+3F/OHlGUGQ79DFf/\nD0b+K2oCNBAG6yHnnlED+OviGTxo+4RpmuV0lk7wCA8zfWRf9zETB7WlsMzKzO/2kBQbw/Pjenq1\nK/H1tBntREs2K5JUHwZI0GkoMXvuSSx2lkETNVb4fLIyiHLr1xDX3H1MqkFLntFzkNYyUY9KJdEi\nQUdsjJojlYYHyiz2GkGXLxry752g4aNWSVzevRVfb8nGbLWjr/y7q9HChI9h7tWKK8GtX0PbIR7P\ns/eUkTbNY4nX1f3W1655HAl6DdtOFDFxUJ1PIwghX2VlE6dVc0WPlr4PPLZWubY6bHDzYuh4cXgW\nGAAiSAsxFTe3WPYa2/DvmNn8GP8MhoxFVY67++JzOFtm5d1fDtIsTssjPjIUIthpuLjKnQnOPjOD\nTkO+hyALXJk0mZRV/wcnt8KN8yHt3CrHpBp07D5prPHak4Vmd7ZWkiTapcS5M2kOh4zJaic2QK0z\n8XsniCSjerZi/vpj/L7/NJd1r3bz1RmUPuA5I+GzCXD7CkjrVuMcLs/O+qBSSfTOTGLricJ6nUcQ\nGsptdpZty2Fkj1a+nVG2LoBv7lNKwpMXQmrn8C0yAES5Mwy4Uvgv/+tFtHd8h4Fy+OCyGqn5x0Z1\n5cbBbXjzpwN88NuhCK1WEEqKTTYMOg0atfLRM+h996TdH7MU9c4vYcQ/oOvoGsekJugoKC1Hrjbd\nll1oIj1J7/6+XUqcO5NmtrnM1aMnpS8Q1MZ5HVNI0GtqljxdGFrALYtBo4NPxistApWw2Bwcyi+t\n82RnZXpnJrPnpBFztYlnQeT5aU8+xWab9wdKhwNWPw9f3QWZg+HOVVEboIEI0sJPm8Fw18+Kevb8\nSfD7q+7xcUmSeH5cL67s1Yrnl+1m0aamqW0TLiLhgFDdi9Og02D0Uu7MyP2Zh9QLoNcNcMHDHo9J\nidditcvuDB0oMgOnis1Vmvrbp8RzrKAMu0OmzCKCNEHDwfU57frkd9jsMsu3n8Rq9zJZ2aw93LRI\ncSP45DpFMd7J4dOl2BxynTXSKtMnMwmbQ45e54EmzJKsbFINWs4/J6XmTksZLLoNfn0J+t2sWI1V\nah+JRkSQFgmSMhQrqZ7j4cdnYPE0t0q0WiXx6sS+XNg5lce+3MYP3p4aBfUiUg4IRSZrFd0z13Rn\n9UwYebuZcOxZ9qnOgTFvgJceRdfkUuXhgXxjOTaHTOtKQVq7lHgsdge5xWZMziAtkJ40gSASVP+c\nmqx2yix2Xl25z/uLWveGGz+DMwdh/o3ua2t9PDur06dNMgDbTkTp8EATpchkZbVzAthVrXBjzIWP\nrlKsGy//J4x5U+lnjHJEkBYptHFw3WxlPHr7IvhwNBTnAKDTqHn35gH0zEjivvlZ/HmwIMKLbXxE\nSpy1uHqQptfgkKm6lrIzMH8SZimWl5p50D2qRKrBFaRVyHC4hGwzkivKne4Jz9OllTJpoiVVEN14\n+pwCzP3jiO8XdrgIxr+vNIYvugPsNvblGlGrJDq2qL/JdqtEPS0SdKIvLcr4bvtJLHZHzanOk9tg\n1nDI3wOTPlU09rw8+EYbIkgLMgGV0CQJLvw/mPQZnN4P718KJzYCEK/T8NHUQbRrHse0eRvZLp7Y\ngkqkxFmLzbYqNk9u/06XoK3dqvi/Fp/kX4lPYotv7ek0biqCtIpMmlvINqlSJi1VuTEdKShz3/RE\nuVMQ7Xj7PJZa7DhqkSuix7Uw+kXYuwyWPcze3GI6pMaj09T/916SJPpkJolMWpTxVVY2HVvE0yuj\nkn7dnuUwZ5Ty99u/h25XRWZxdUQEaUGkziW0blfCHSuVhtcPr4StnwPQLF7Lx3cMISk2hikfrudA\nXkno/xFNhEg5ICiZtIoMlmvK062V9v3jcOQ3GPM6m2zn1LSEqkaKQUnXn640IXrS6SxQ+d/SOlGP\nVqPiaEGp291CL8qdgijH1+cx67gfWawhd8GFj8DmuVxw4v16T3ZWpndmMgfzS7z2lArCS3ahiXWH\nz3Bt3wxFwkqWFXeWzydDi66KxVPrPpFeZsCIIC2I1KuE1rK7MlDQZrBiGvzDP8Bhp1WSnk/uHIJK\ngltnr/Oo+i4InOkju1I92R0OcdZik9XtNgDVMmkb58CGWTDsAegzqcaQgSeaxWlRSVBQWlHuzC40\nEa9VV3mtSiXRtnkcRwpK3T1pIpMmiHY8iSjrNSrUKqmql6cvhj+Jrc/NTLEu5DrHiqCtrXdmErIM\n26NV1LaJ8fUWJRkytm8G2CyKO8sPT0L3MTB1GSS0ivAK64YI0oJIvUtocc2VaZNB0+CP15XpT3MR\nHVLj+ei2wRjNNm6ZvY4CDwrzgsC4pGuLKt+nJ+lD7oBgd8gYyz2XO9XH1ii+cZ0uh8ueQZZlik22\nWjNpapVE8/iqrgMnixSx2eqCyO1T4jhaUCamOwUNBk9OJS9c15sLO6fy/Y7cmgM3npAkdg14lpX2\n/lx68D+wc0lQ1tY7UwwPRBPfbMmhf9tk2saaFQmWzfPgoulw/UdKD3gDRQRpQSQoJTR1DFz1Mlz9\nKhxcreipFRykZ0YSs6cOIvusiakfbhAp9nqy7vAZZOBypyjmiocuCrlQq6vvrPrgQKaUR5df7oXm\nHeH62aBSY7Y6sNgdVQI6b6QatOQbKw0OFJqrTHa6aJcSz5FK5c5AvDsFgkjhySpqVI9WHDtT5lHI\n2RN780zcb72f8lb9lWn6w7/Ve13N47W0bR7HthOFEZHzEVSwJ7eYPblGbulsVe6Zx9fBte/D8CcV\n67AGTMNefZQRVH/Dgbcr9ialp2HWpXBwNYM7NOftm/qz62Qxd83bJIQU68G6Q2fQaVTuIO1Uceiz\nk9XN1QESJDOzYl4B2QE3fg56peG1wly99gnMVEPNTFrlyU4X7VPiMFsdblFbMd0paKhc1r0lKgm+\n33HSr+P3nTLi0MQSc8siaNZB6VPK3V7vdfTOTOLPgwURkfMRVPD1lhzOV+9i7KYpYC6EKUuhz8RI\nLysoiCAtiATdRLz9BXDXT5CYqQgzrn2HEd3SeOWGPvx5qIAH5mdh8ybqKPDJ2kMFDGjXjDbNlDR4\nXrE55O9Z4dvpDI4cDlqtepAu0gl+7v2iInDsOtZU1ePTF6kGLQWlSpBmtto5XWIhPclzJg1gj1OA\nU5Q7BQ2VVIOOQe2b890O//rS9p4qoXOaAXV8c8WVQGtQrqlnj9ZrHX0ykzlbZo2InE/I2LYQXu0J\nzyQrX7ctjPSKfOJwyMgb5zI3ZiYqQ0vFQaDt0EgvK2iIIC3IeErN14tm7eGOH6DrlfD9DPjmfsb1\nSuWZa7rzw65TPL54u399GQI3hWUWducWM7RjCi0TFQmLU8YwBGmuwMuVSft5JtoD3/G87Wb2G6o6\nNVdk0vwJ0nScdpY7c4uUf4fncqcSkO4+aUSSQKcRH39Bw+XKXq3Zn1fC/lO1lzz35Vby7EzKVAI1\nm7N3qfR0ndfQOzPJ675Qy/mEhG0LlYb7ouOArHxd+kD0BmoOO3lfPsIM29sUtBgCd66E5h0ivaqg\nIq7SDQGdASZ8DBc9Clkfw9wxTO0Tz4MjOvPFphP8e/nuxh+oBfHpbv3hM8gyDO2YQlqiUhbMC3e5\nc8di+PVF6Hczn3IlRnNV/85ik7N/rZbpToAUgw6T1U5puc19Y0j3UO7MSI5Fo5LILTYTG6OuMVgg\nEDQkRvdshSTBsu2+S55FZVZyi810qWwHlXauYqpddAI+vQHK6yZv1DPDe5AWajmfkLDqObdDgxur\nSdkebZSXwIKbabXzAz51XEH87V+520UaEyJIayioVDD873D9h3ByK7x/KX/rUcaU89ox67fDvPPL\nwUivMHQE+elurbMfrU+bJAw6DfFadVh60lzZsRTjblhyL7QZAlf9F4M+hpJyq8djk/zKpClaaQUl\nFnKcmTRP5U6NWkVmM2W7KHUKGjppiXoGtW/O8lqCtH15SqathkZa26HO6+kWRUDaHvgwVrxOQ6tE\nPapqzzvhkPMJCUVe/KKLjkdPCXTbQnilG8zMQN67nBfkKaw99wkMsTUfTBsDIkhraPQcD3coWj/S\nnFE8fc5+xvZN58Xv9/LZumMRXlzg+DUVFeSnu7WHCujftplbebxloj5M5U4bqRSRunQqxKXAxE9A\no8Og01Babq92bADlTqd/Z35JuTuT1irJ8wXL1ZcmJjsFjYGrerVm36kSDuR5L3nuyXV6dnoyVu92\nJVz9Pzi4Cr6+DxyB9/he2DmVOK2G9CR9cHqRI0lSppcdUnSUQLctVH5ORiUw32rvyLvlIxmbdCj8\nawkTIkhriLTuowwUtO6NatFU/ttiOcO7pPD3JdtrfaqMJvx2aPD6dOdluw+KyqzufjQXaYm6sAwO\nlJSW8p72v0jmQsUA2pAGKFppNcqdzu8T/Ch3tqhkDXWyyESqQevVTcDl4RkXIyY7BQ0fd8lzm/cB\ngqVbc0hP0pPu5cGFAVMUqYZtn8OPTwe8ht5tkikpt7HwnvOC14scKUZ48gqWgGrtNJEqgX4/A+wV\nVY859tEkY+SiPfVbSzRLqIggraFiSFPGjPvejPq3F5mlf4PzM/U8+HkWv+3Pj/Tq/MJvhwZvT3de\nn/q8s/6Iqx+tuXtby0R96MudsszF+//NANV+pHFvV7EnMeg0NcudJiv6GJVfPoNua6iScrILzT57\nYUQmTdCYSEvUM6hdc5Ztz/G4f+ORM6w/fIZpF3X03YN54SMw+C5FRHz9rIDW0Mc5PNAoRG17T4Br\nXoekNoDk/Oql37kOD8l1Rpbhl5egrMC9qVTWsdIxgCvV69AW131Kt852jmFCBGkNGY0Oxr4Jo15A\nvX85c3mSYc1LufvjTWzxx9cuwvjt0ODp6S4mVtkeIGsPFTj70ZLd21om6skzmkM7fLH2HfqfWc6H\nmomK8XMlDHoNJeXVM2lWv+Q3AFLilUxaQYmFk4UmWnvLGADtU52ZNBGkCRoJV/Zq5bXk+fbPB2ke\nr2XSoLa+TyJJMOoF6DIavnsMDvzo9/t3a5WIVq1i64nov+b6Re8J8NAOeKZQ+ZrUxvNxdXhIrhO2\ncsUq8afnIabCOWClYyAm9IxTr6nXWupl5xgGRJDW0JEkGPoXuGkRamM2H1ofZUTsfm77cL3PPo1o\nwG+HBk9Pd9e8rmwPEFc/WuVyYFqCDrPV4S4xBp0Dq+CHv7Mx7gIWJ95UY7dBp3G7EbjwxxLKhVaj\nIik2htPOnjR/MmkiSBM0Fkb3au2x5Lkrp5jVe/K4bVh7/zLHKjVc9wGkdYcvboO83X69v1aj4tzW\nCWxtAA/GdSKID8kBU5IPc6+BbQuUkvTV/3Ov5Wv7MNI5zUDt8Xqtpd52jiFGBGmNhU4j4M7VqOJT\neN36DDfwI7fMXh/VhuwBOTRUf7qrQ4BWVGZl18lihlQqdQKVZDhC0Jd2+oBywU/rzqvxD5MYp6tx\niPdMmv99Y6kGLYfySym12D1OdrrIbBaLSsJrz5pA0NBomahnYLtmNfpx3/nlIAadhlvPa+//yXQG\nmPy5Egh8NkEJEvygd2YyO7KLcTgaoRRSEB+SAyJvN3wwXFEzuOEjxYezz0S45nUKErrxq6M3Y+K2\noxrzWr3WEhQ7xxAigrTGRGonmLYKqeOlPOF4jwfL32XqB2ui1pA96A4NtVDRj5ZSZXtL53Rk0PvS\nTIUwf6Lix3rjfPLKNR5LmAmeBgdMVr8zaaBopW1zllt8XVy+256LWiXx7baTUdcgKxDUlat6tWbv\nKSMH8hS9syOnS1m2LYebhrYlKc7/zxGglM5unA8leYp9lLXqw5unJvM+zuGBQ6frprcW9QThITkg\n9v8Is69QSp23La/aItJ7AsuHLcCOmrHTnqr3WoJq5xgCRJDW2NAnweQFMOwBJvED/yx+igfmrKqR\nqYkWgu7Q4IN1hwrQalT0rdSPBsqTOMCpYGbSHHZYdDucPQITP4bktl77zAw6DeU2B9ZKFl/FZpvf\nPWmgTHi6yrWtPQjZQkWDrNWuPO1HW4OsQFBXXCVPVzbtvV8PolGruOOCOqrPZwyAa9+DE+vh678q\njet4bzI/bVQe8LYebwTDA/UgKFOS696Hz26A5HYwbbXys6jGV1nZdG2ZwLmtE+u95nAnCwJFBGmN\nEZUarvgnXPsegzT7mVnwIM/PWYTF1rR9PtceLqB/2+Qapb40pzVUnjGImbSVTynaS1e9Au2GAYrj\ngKeneoOzrFlaKZAuMln9Mld34RK0BeUi44lob5AVCOpK5ZJnbpGZRZtOMGFgJmkJ9RA47TEOhv8D\ndiyC3/8LeP8MzfvzCDqNir1+WFQ1Vuo9JWm3wbJH4Lvp0GUU3P69x4GAYwVlbD5WyNh+6UFbeziT\nBYEigrTGTJ9JqG//jhZ6mSdzH+TDD99unD0TfmA0W9mZU8yQDik19sVpNSToNMHLpGV9Cn++qYz0\nD5gKQLnNjtnq8NhnFq/TONeoBGmyLCvlzgAyaalOrTSNSnL/vTrR3iArENSHK3u1Zk+ukb9/tR2H\nDHdfdE79T3rh/0HP62DVP2H/Sq+flZNFZjq2MPjlI9pYqddDoLlIyZ5tmAXD7leEvnUGj4d+vUUJ\n+sb2jZ5AKpSIIK2xkzmQ2L/+SlliR6adeJKfPniMrzYdj1rhvlCx+6QRWYY+bTx7u6Ul6sgLhuvA\n8fXw7d+gw8UwcqZ7sysA89RnluAM0lwlaZPVjs0hB9yTBorTgLq6R42TaG+QFQjqw+ierQFYtSeP\nMX3SadM8rpZX+IEkwZg3oGVPWHQHgxPPejwsPTmWTmkGDuQ30p40P6jzQ+CZw0r/2eFflf/rK55X\nqkEekGWZJVuyGdyhudeKQRWC6PkcKUSQ1hRITKfF/avYlXI5I3LeI2bJNAoKC6NSuC9U7D5ZDED3\n1p6DtKAI2hadgM9vgsR0ZRpJXZE1q2KuXg1XudMVpFWYqweSSVPKnb4CrmhvkBUI6kOrJKXkCfCX\nS4KQRXOhjYdJn4BKxSzdf0mJsVTZ7foMdU4zcOKsCZPF7uVEjZs6PQQe/RM+GAHGXLjlK+h/q8/3\n2JlTzMH8Usb5k0ULsudzpBBBWhNB0sbR/a8LeMUxmStVa/lC+yytUdSbm0Jf0q6cYprFxdAy0XMp\nUAnS6pFJs5Q5J8FMcOMCiKsq8+H24vQyOAC4tdJc5uoB9aQ5J1S9Wt8Q/Q2yAkF9efzKbjw/ridd\nqpup15dm7eH6D0ksOczSzE/J8ODT2SnNgCzDwSaaTQv4IXDr5zBvDOiTlQGBDhfV+h5fZWUTo5a4\nsler2hcUZM/nSCEM/JoQKrWKNyxXs1OVzmsxb/GN7knutjzEZrlLo+9L2p1bTPf0RK/WMEq5sxxZ\nln3bx3hClpUJsJPb4MbPIa1bjUOK3eXOmh85lz+n0Z1J855188bmo0oZZsmWHDYcOcv0kV09Bl/j\n+mWIoEzQaBnQrjkD2jWv/cC6cM6lcPk/Sf/h76wZPkTR7apE5zSlh+pAXgk9Mzxn7BszruvKSyv2\nukW1PV6HHA746V/w28vQ/kKYMK/GQ60n7A6ZpVtzuKRrGslx2lqPD6bncyQRQVoTIyM5ltWF/bnW\n8iwfxLzCfO3z/N12B38mjIr00kKGze5gT66RW4e283pMWoIei81Bkcnq3wWgMr+9DDsXw2XPQFfP\n/4++M2nKthqZND/LnUuysnm5UibUVcIGREAmEAST8/6qiKuu/he06g1dRrp3tUuJR62S3FptTRFP\nD4FLsrLdgVuHJBUfp8whI+cHpbR51X8VHUk/+PNgAXnGcq7195qWlOksdXrY3oAQ5c4mhislfUDO\nZKzln2xwdOXlmPeYm7FEGYFuhBw+XYrF5vCpqeMqgwbcl7ZnGax+HnpNgPP/5vUwf3rSSqv3pPmZ\nSXtpxV7M1eRVmkIJWyCoTFA0umpDkuCa16BVL/hyGhQcdO/SalS0T4ljf5Tb8YWTyrIcqZzlVdMT\ntM5eyfYejyquBX4GaABLtmSToNMwvFuafy+IpJ1VEBFBWhOjcl9SEQZusz7GIs1VdDo4V7FBMTU+\n/7ldrqGBdF9BWh0EbU/thMV3QXp/GPO6cgH3QkWfWc2LUlyMGkmqVO50Z9L8S3QLaQ1BU6feGl2B\noI1TJCJUaqUPtbwiKOuUZmjSmbTquGQ5uktH+Fr3DzpJ2UyzPsw9B8/zeb2sjtlq5/sduYzq2cp/\nS7tI2VkFGVHubIJUTkmv2JnLPZ9oKG/TjcmHX0f6YITSV5XaOcKrDB67ThYTo5Y4p4Vn3R2AlgkB\nBmmlBTB/EmgNMOnTmk9s1Sg22dCqVeg0NZ+LVCqJeG2FybqrNJrgZ7kzPTnWo0erkNYQNFq2LVQa\nwItOQFImW0qvw2QdXOUQVzY5JCX/Zu2UCe6Pr4Wv7oEJH4NKRee0BH7cnYfF5kDr4bPe1MgpNHG5\naiOvxbzFWQxcb3mG3XI7pAAfIH/cfYqSclvgP8veExpcUFYd8VvUxBnZoxUzRnXj78cGsLD7m2A6\nC7NGKN5pjYTdJ410TkvwedEMyHXAboWFt4LxFEz6TJHcqIUipxent6EEg05DSbkSnBWbbcTGqP2+\nyAtpDUGTwoO0wqPWtxmj+r3GoSHNJne8WHF22fMt/PYKoGTS7A6ZowWloXvfhoIs84jhe96LeZV9\ncgbjyp9jt6z0BQf6ALkkK4e0BF0N3+WmQESDNEmSRkmStFeSpAOSJM3wsF8nSdIC5/51kiS1D/8q\nGz93XdSRCQMzeWxjAisvXADJbRT15z/edHvWNWR25RTX6vGmj1GTqNeQ508m7btH4ejvivBiZk1f\nOU8Um33bPBn0mko6aYFZQglpDUGTwoO0Qpxk4VFNTf2rkGeTh96r9KP+9C/Yt4JOzgnP/U295Gmz\nwDf38VfbPFbIQ5hoeYp8FA27QB8gC8ss/LIvj7F9070KdTdmIlbulCRJDbwFXA6cADZIkvSNLMu7\nKh12B3BWluVOkiRNAv4DTAz/ahs3kiTx/LheHMov5cHvClh695ec8/t0+OHvSt/V1a9CTD088CJI\nntHM6ZJyn/1oLvwStN3wAWycA+c/CH38/1WszebJoNO4XQm8GbH7QkhrCJoMXiQU0qWCKt+HJZss\nSUo/6um98OWddJr6I5JE0+5LKzsDC25RHmQvepTy5FtJ/WG/b1kOH3y9JQerXW4yNlDViWRP2mDg\ngCzLhwAkSfocGAtUDtLGAs84/74IeFOSJEmWG0F6J8rQalS8Obk/V73+G/cs2MvXf51NXMv/ws8z\noWC/0oI10yAAACAASURBVCib4IeAYJSx+6TS1Htu69rFLVsm6jnlyxrq8K+w/FHoPBJGPB3QOopr\nkfZIqJJJswVkCSUQNCm8SCuY41qRERtb52CgzsTEwsRP4f2L0X95C12Sn2q6mbTT+5UBtKITMH4W\n9J7AOGBc/zZ1Op3RbOWN1Qfo3zaZHn48aDdGIlnuzAAqf9JOOLd5PEaWZRtQBNQoSkuSdJckSRsl\nSdqYn58fouU2flol6XltUj8O5Jfw9yW7kC9+TBEaPLUT3r8UsjdHeokBU2EHVfsHPC1RR563TNqZ\nw0ofWkonuO4Dr95y3ig2+w68DLqKwYEik9XvyU6BoMnhRVohbvRzrJkxnMMvXMWaGcPDm1lObgM3\nzIWCA/ybtzmQWxS+944WDv2iWDyZi2HK0qA07L/100FOl5Tz9DU9AhcZbyQ0isEBWZbfl2V5oCzL\nA1u0aBHp5TRoLuicyt9GdOGrrGzmrz8O3cfCHT8oQcmHo2H7okgvMSB25RSTnqT3S6C2ZaKePKOZ\nGolaczHMv1Hpz7txPugDf6IrriXwMug0FTppZqvIpAkE3ohWaYUOF8LIfzHAtIZRZz7B7mhCBZ9N\nH8En4yGhNUxbBW2H1vuUxwrKmPP7Ycb3z6BPm+T6r7GBEsnH9Wygcg4007nN0zEnJEnSAElAAYKQ\ncv/wTmw8eoZnlu6kd2YSPTN6wbSflEzSl3dA3i649ElQRX+Mv/tk7UMDLtISdFjtMmfLrDSPdwZ1\nDjssngan98EtiyElcONmWZZrDbzidZoqtlCB9qQJBE2KaJVWGHIPR7b/wYPZX3Bq01W0HHRtpFcU\nWhx2WPkU/PkmdLoMrp8D+uBYYv17+W7UKolHR9a02WtKRPIuuwHoLElSB0mStMAk4Jtqx3wDTHH+\n/XpgtehHCz0qlcT/JvYlJV7LvZ9uVtTyDS3g1q8VK4/fXlFEHM3FkV6qT8xWOwfzS/waGgAvgrar\nnoN938Po/0DHS+q0DpPVjtUu+/TidPWkORyyszQqyp0CQYNDkjg74kW2OTqQ8sN9kL8v0isKHeUl\n8PlNSoA25B64cUHQArQ/Dxbw/c5c7r3kHFolNcyhtWARsSDN2WN2H7AC2A0slGV5pyRJz0mSNMZ5\n2GwgRZKkA8DDQA2ZDkFoSDHoeHNyf3IKTUz/YqtSAtRolbLC6Jdg/w8w+wo4cyjSS/XKvlNGHDJ+\nZ9IqrKGcQdrWBbDmfzDgNhh0Z53X4bZ5qmW6U5bhdGk5docsMmkCQQPlnPRU7rY8jJUY58NsI+xP\nKzwOc0Yp94ErX1YeYtXBebC0O2Se+3YXGcmxtEzUh97qK8qJaL1KluXlsix3kWX5HFmW/+Xc9pQs\ny984/26WZfkGWZY7ybI82DUJKggPA9o1Y8bobvyw6xSzfz+sbJQkGHKXUvoznoRZw5WG0SgkkKEB\nUEzWAWV44MRG+OZ+aH8hXPlSQBYm1amwhPKtkwaQU6gEiL6ybgKBwE+2LYRXe8IzycrXbTW11IJN\noj4GR2I6c9KfhbOHFY9Phz3k7xs2TmxUrvuFR+GmL2DwtKCefuHG4+w+Wcxl56bx9Dc7w2P1FcVE\nf1ORIKLccUEHRvVoxQvf7WHT0TMVOzpeAnf9BPFpijXK+llRJ3y7K6eYeK2ats3j/Dre5TpQevqo\n8gSc0EqZ2ArABNgTvszVXRh0riBNEekUgwMCQT3x4EzA0gfCEqh1SjPwQ+k5MOoF2L8CVj0b8vcM\nCzu+hI+uUqZr71gJnUYE9fTFZisvr9jLoPbNWLnrFCZr1eDWZfXVlBBBmsAnkiTx4g29yWgWy18/\nzaKgpJJERfOOcOePSsPo8kfg278pStNRwu6TRrq1TkTlp0q1TqOmVaydK7Y9DJZSmLwA4utvQ+Ly\n4vRVwkzQVwvSRLlTIKgfHpwJsJqU7SGmc1oCB/JKkAfdCQPvgDWvwdbPQ/6+IUOW4ef/wKLbIb0f\nTFsNacFt6Dearfzfwq2cKbPw1NU9OFnkWbMypFZfUYgI0gS1kqiP4e2b+nOmzMLfFmypOlquT1Rk\nKS54SBnD/ngclJ6O2FpdyLLsnOysXcTWjcPOq+o3aG3ar0wppZ0blLVUlDt9ZdKUfa5ypxgcEAjq\niRdnAq/bg8g5aQZKLXYl0Bj9H6Vt4psHlFJhQ8NqVibcf/439J6kDJDFpwb1LXZkF3HNG7+zek8e\nT17VnV6ZSV4tvUJu9RVliCBN4Bc90pN4dkwPftt/mjdXH6i6U6WGy56B8R9A9iZF+DZ3RySW6ebE\nWRPGchvdW/s5bSTL8P0MzrOt533DPdBlpM/Dl2Rl+93QWlRWe7kzXqeI44pMmkAQJJIyA9seRDo7\nPTwP5JUo7RIT5kFia6WNoqgB9VSV5MHca2D7FzD8H3Dtu6DRBe30sizzydqjjH/nD8xWB5/fNZQ7\nLugAwPSRXYmNqSoaHharryhDBGkCv5k0qA3j+2Xwv1X7+H2/h2xZ7xvgtuXgsCqTn7uXhn+RTnbm\nKEMDfmfS1r4D69/n55SJzLVe5vPQJVnZPL54u98NrfnOEnGCDzHbBFcmrUj0pAkEQcGLMwEjngr5\nW9cwWo9rDjd+DpYy+PxG5Wu0c2oXzBoBuduV3tyLHqnXAFV1jGYr98/P4sklOzivYwrLH7yQQe2b\nu/eP65fBzPG9yEiORQIykmOZOb5Xk/MoFkGawG8kSeL5a3vSOc3Ag59nkeupZyBjANz1s1IqXHCz\n0scQgYGC3SeLkSTo2sqPIG33t7DiCTj3GjZ0/hv5xnIcPtTCX1qx1++G1v+t3MdbPx0E4JKXfvYa\nyBmq9aT5CugEAoEfRNCZICVeS7O4mKpG62nnKpZyJ7fBknvA4Qj5OurM/pXKg7a9HG5bBj3GBfX0\nrvLmdztyeXRUVz6cOqhCQLwS4/plRM7qK0oQQZogIOK0Gt6+qT8mq53752/GavdwoUloBVOXQZ8b\nlT6GL6YojfhhZNfJYjqkxhOnrSXYObEJvrxTCS6vfZ+0xDhsDpkzZd4HILw1rlbfPuvXQ/xv1X73\n974ybq5y5+kSC3FaNTFq8dEUCOpN7wnw0A54plD5GiaXAkmS6JRm4ECeseqOrqPgiudh19ew8h9h\nWUtAyDKsfVcxSW/eXhkQyBgQxNNXLW/OnzaUey/p5PdwV1NE3AkEAdMpLYGZ43ux4chZXvY2Dh2j\nh3HvKBek3Uth9kgoPBa2NfplB3X6AMyfCIY0pRShjaspaOsBb42rrSspY58uKeeF7/fUOMZbxk2n\nUaPVKB9H0Y8mEDR8OqUlsD+vpKYX8Hl/hcF3K0r9a9+NzOI8YbfCsofh+8egy2i47fug9u8ZzVYe\n+HwLTy7ZwdCOKSx74AIGd2he+wubOCJIE9SJsX0zuGlIW9779RArd53yfJAkwbD7YfIXSoD2/qVw\n9I+Qr63YbOXEWZNvEdvC4zBvrPLkePOXiu0VkJZYSdDWC9NHdkWvqfnRSUvUU26zY7LYuWPuRq8G\ny94ycQlOrTQx2SkQNHw6pRkoLLNSUFotKy9JMGomdLsavp8R0d5dN6az8Ml1sHEOnP8gTPwEdIag\nnX5XTjFj3lzDsm05TB/ZlY+mDiLFELwBhMaMuBsI6sw/ru7O1hOF/N/CLSx74ELaeBON7XwZTFsF\n8ychzx3D4cHPsCfjOgpKyikotVBQYqF1sp57L+kUlHXtOKHYsPTw5tlZkqdIhZQbYepSSO3s3uXR\nv7Ma4/plsDfXyDu/KL1mGcmxDGzXjK+35jB1zgbidWq2nSikeZzWY9nUWybOoNdQUGoRmTSBoBFQ\necIztXpAolLD+Fkwb4zSbjHlW2gzKOD3WJKVzUsr9pJTaCI9OZbpI7sG3rdVcFApb549CmPfhn43\nBbwOb8iyzGfrj/Hs0l00i4th/rShDOlYf+3JpoQI0gR1Rh+j5u3JA7jqjd+499PNfHHPeeirjUy7\nSe3M8eu+5eTsyQxe+3d+t/3Mc7ZbsKFBH6PCbHVwff9MdyarPmQdLwSgb5vkmjtNhfDxeGUM/tYl\n0LpPld0tDK5yp/dMGkBqgnLcpicvcz8RXtKtBdO/2IbNIfPsmB4kxcbw+OLtVYYMfI2Qx2tdmTQR\npAkEDZ3KE55DPQUm2jilzWL25UrbxR0rIeUcwL/gyzVl7rq+uHpeAf8DtcO/woJbQFIp+mftz6/j\nv7YmJeU2nli8nW+25nBh51Rendi3ZrAqqBVR7hTUi7YpcbxyQx+2Zxdx32deJj6BTUfPMHb2Du62\nP8qBzrdzq2Yluzq9w/4nBvLF3cMAWHv4jMfXBkrWsbN0bBFPcly1aSFLqfLEmL8HJn0CbYfWeK1W\noyIlXkuuj0wawL5cIynx2iop+2v7ZfLJnUN48fre/H97dx4fdX3ncfz1SUIIEEgIhCMkkQCKjXI2\nHNb1qEjFaoV60Nrarhfq1qPtdrW2tmy7u62s7vay1dVV8GrxqsVb68Xa2qIcgQgoh6AQjoQrIQFC\nru/+8ZtAgJkwk8xkfpN5Px+PPJKZ+T1mvuSnv3x+n+/n+/n+4+eGRryEvGWFZx+t7BRJeIOzMuid\nkcbqQDugoHr1h68/4/386HTY9XHYLX4iWWUe1JK53pZ+mQO9mY4oBmirt+7lS/f8lRcD05uPXDVR\nAVo76a+BdNgXThnEjy74DHe9toZz/nshN50zgmv+oYjuaV5W7YUVW/ne0yvIy8pg7pWnMSz3fCg7\njfTnb4aHplA883F6d09j0YZdXDQmr0Njcc5RuqmKs0cOOPKF+n1eI8nyxXDZw95WViEMH5B5aHP2\nUNZW1nDiwGNrNiYP63fEXfOMcUPCvqs9XJOmTJpIojMzPntCX5Z8cpybz37D4RsLvBKMeefzTP0P\nOdCQe8QhLcFX62tJuKvMj9HU6LUcev9+GDEVLn0IMsJs+h2GJ97fxOznV2l6M0qUSZOouPaMYbzx\n3bM4fUR/7np1Def98h3e+qiC3729npvnlzI2P5s/fet0huUGApvRM+HqV8E1kzpvGjflrmDRhl0d\nHsfm3QfYta+ecYWtpjr3boN553up/Yt+C8XT23yPMflZrN62l/rG4H2MnHOsr6jlpIERbDkVhpZM\nWls7E4hI4pgwNId1lbXsPnrxQCsLSrdw+iM7mbrn++ysrec3B+/gFNt4zHFHB1/t2jbpQBX84TIv\nQJt8o7c/cZQCtMamZv71uZXc/uwHTCrK4aVbzggZoEWyY0uyU5AmUVPYryf/+80SHrl6IikpxtUP\nL+Hu19bw5XFDeOzaifQ9ullh3jiv8W3eWK7f+TO+uucBKqtqg7112Eo37wE4HKRtXwkPnuu12/jq\n/LCKYkfnZ1Pf2Mzaipqgr2+rrqPmYCMnRjtIa8mkaeGASJcwKdBiYnGIbFrrqc11Lp9LDv6YA2Qw\nP/1njLe1Rxx7dPAV8bZJO9d518KN78BF98C0n3sLGKKgen8DV85bzCN//5RZZxTxcBvTm5Hu2JLs\nFKRJ1J11Ui6vfvtM/vVLxcy+sJhfzBxzaOrzGJkD4JvPs6v4H7ku7SXs95d0aIP20k1V9OiWysiB\nvWHdGzB3GrgmuPoVr5FkGMbkewHeivKqoK+3BG8jY5RJUwsOka5hVH4W6WkpvB+i3vbourJP3SAu\nOzibXa43j6XfyedSvD2QgwVfEdW8rloAD5wNB3Z7CwTGfzNa/0Q+3lHLjHvf5b2Nu7jr0tHccUEx\nqW00p+1wLV2S0V8Dn4vKEus4SE9L4arTi8I7OC2d7Et/zR0/7cVPdj4I958FX3kMhoyP+HNLN+1h\ndH4Wacvmwcu3woBiL6WfFf7vrCCnB317dqNsczVfn3Ts6y1B2klBatI6orcyaSKxV/YUvPlvUF3u\nNWudMjtmOxF0T0tlXEF2yExasPqxrfRnZv1snuwxh0e7zWFet6+Q+8UfBr3uH7fmtakBXp8Ni+6F\n/AlePW4UG9S+s3YHN/5hGempKcyfNZmSocdvTtvuWrokpUyajyVTWjg1xdhedDE39bjTa/Y4dxos\nfSSifT/rGprYurWc71XfCS/9M281jmJq1Q9YsCGysZgZo/Kz28ik1ZLbu/uxq0c7KFMLB0Riq+wp\neOEWqN4MOO/7C7d4z8fIpKIcVm6ppvZg4zGvhaofS8/OY9j33yVt1MXMapzPjBX/5AWVkdi7FR6+\n0AvQJl4PV74c1QDtlQ+2cfXDixmS3YMFN54eVoAG7aylS2IK0nws2dLCk4f147Xdg9lx+atee4wX\nboEHzoK1fw4rWCtf9Ede7nYbY/f9lbsaZjKr4Xusq6Zdge2Y/CzWVdZyoL7pmNfWVdREPYsG0EuZ\nNJHYevPfoOGojE3DAe/5GJlQlEOzg6Wf7jnmtTbryjKyvA3ZZ9wHW0vhvtPD252gsR6WPQb3nwnb\nP4BLHoIv3gVp0bupfKlsGzfNL2V0fhZP33Ba6EbmQURcS5fkFKT5WLKlhVtaV/y9IgW+8Sfv4tSy\nGumhqfDx28GDtbpqWPAtRrw5ix0um+n1/8G9TTNowrsQRBLYtqw6uuet9TQ1O+5buP6I15ubHesq\no7+yE6BkaA5nnNifYbm9ov7eIkLobFSkWaoIjC/sS2qKsThIXdpx68rMYOzX4Ia/QN+h8OQV8PRV\nsPJZ2HfUaviDNfC3e+DXY+D5m6DPELjubRh1aVT/PS+WbeWWJ0oZV5DNo9dMoneEN5WR9o9MdqpJ\n87G87B5sCRKQddW0cHFenyP7pY39Goy6DEofh3fu9voI5Y2DHjnQ3AjNTd733Rtg/y5eyfkGt2yd\nSkOQ/6zDCWyP7uANcN/CjxmWm3noArKl6gD765tiEqQV9e/FY9cEKYITkejIyg9MdQZ5PkZ6dU/j\n1Lw+IRcPhNVLsd9wb0eChT+HxQ/Bqme95weNgmFnQ0qa15y2rhqGngHT74HhU7wgL4qeX7GV7z65\nnPGF2cy7auKhEo1IRdI/Mtkpk+ZjyZYWTk0xJhblHNkvLbUblFwFNy+D8+/yLkZ11dAY2LYprTsU\nTIRrXuff932ZtG7Bl32HE9gGm15uaHZHZOFitWhARDrBlNnQ7ahrQbce3vMxNLEoh+XlVdQ1HFs+\nEba0dDj3J3DbRrjmDTjnR5CRDe/dD3/9FRSdBbPegitf9Jp1RzlAe275Fr7zRCmfLezLwx0I0CQy\n+i37WMudRiKu7myvycP68eZHlVTurTtyH89uGTDpeu8riO3VdWytfpMZY/N4bVVF2PtlthbO9PLa\nCq+P24gB0c2kJeoqXpGE0rKKs5NWd7aYMDSH//3LRsrKq5lYFF6BfUipad5m7AUT4MxboX4/1Nd6\n7Yxi5IVABm3C0BzmXjnhUP2sxJ5+0z6XbGnhQ3VpG3YxfWz4/+7lgSa23/zcUM4eOaBdAU+o6eVB\nrYLFdRU1DOqTEdVdAaKyUbKIhGf0zJgHZXDkjdfAwDVk8Se7Ox6kHS29p/cVI698sI3vPLmckhNy\nmHfVBHqmK2zoTPpt+4iyKa3r0nZHFKSVbqoiPTWFU/L6ML6wb7t+b7eeN/KYmjQ4MlBaW1nDSYOi\nm0VraxVvsp1/ka7g6Buv7XvrMLyM1I2fHxHfwUXgz6u2e9v6FWQzVwFaXKgmzSeSqSdaW1rq0t6L\ncB/P0k1VFOf1Cb2zQRiOXnU0OHD327ITQFOzY11FLScNiG49WrKt4hXp6oLdeDlgzfYaGpuC7wns\nN299VMGNf1jGKUOyePiqCapBixP91n1C2ZTDWurSKvbWHZomaEtjUzNlW6q4fGJhhz/76Onls+9+\nm7JAU9vNu/dzsLE56is7k20Vr0hXF+oGywEfbqthVH50NjUPV6SzNP+3dgc3PLaMkwf14dGrJwZv\ns9GJOzckM2XSfELZlMNa6tIWhZlN+2h7DXUNzYwr7Bv1sYzOz6asvBo4vLLzxCiv7Ey2VbwiXV1b\nN1jvh9giKlYimaVxzjH//U3MenQJIwZk8tg1E4PX38Zh54ZkpSDNJ7RVxmGt69LCUbrZy3SNK8iO\n+lhG52exrbqOypo61lV6KztPjHImTc0dRbqWUDde/Xql8/7GyEo5OircnWv21jVw0/xSfvDsB0wq\nyuHxayeF3vouDjs3JCtNd/pEsKL1ZM2mRFqXVrppD/0zu5PfN/oB7ZhA4Fe2uZq1FTUMye4Rk9qM\nZFvFK9KVhWqf9Jd1O3l7TSXOOSzKfcxCCWeWZvnmKm6ev4ytVXXcNm0kN5w5nJSUNsYXh50bkpWC\nNJ9Ixp5obTltuFeXtmFHLcNy255eXL6pinGF2TG56J2S14cUg7LyKtZsj82enSLS9QS78TrY2MQf\nl5Xz8Y7aqPdaDKWtmtfGpmYe+utG7n5tDQP7ZPDU9afx2RPCKBuJw84NyUpBmo8om3LYRWPzmPPK\nR/zhvU386MLikMft2VfPhp37uLQkNheHnulpnDSwN6Wbq9iwYx9nnZQbk88Rka5vYpFXb/u3j3d1\nWpAWbJYmIy2Fzw3vx+f/eyGbdx9g2imD+M9LRpPVM8z+j1NmezVorac8O7hzg1pQBacgTXxpQO8M\nzjt1EE8vLedfzhtJRrfgrTVeX10BeB29Y2V0fhZ/XLaFpmYX9Xo0EUkeQ/v1ZFCfDGY/t4r7/28D\nYwqyGJOfzdiCbE4ZkhWzUgrwZmm2VB2gT6Cl0NNLyxlTkM2PLyhmavHAyGYiorxzgxp6h6YgTXzr\nikkn8FLZNl5YsZXLSgqOeb2p2XHvwvWckteHknBS9O00Oj+bp5Z4tRaa7hSR9jIzHr92EgvXVLJ8\ncxUryqt4+YPth14vzOnJyYN6c/LgPnxmUG9OHNibwpyepKdFvsZv38FG1lXWsraihnUVNYwYkElN\nXQN76xo5fUQ/bjx7BBV76/jpC6u5/rGlkWevorhzg1pQhaYgrStL8D42k4flMGJAJo+/tylokPZi\n2VY+2bWf/7lifEyLcMfke4sHzGBElBvZikhyGTEg84jryK7ag6wor2L11r18uL2Gj7bt5Y0PK2h2\n3uspBvl9e1LUvxdF/XvRPzOd+sZmDjY2U9fQxMHGZvbXN7Fnfz1V+xsOfa892HjoM9LTUhiRm8nU\n4kFcMbmQcYV9WVC6hR/+aaUvsldqQRVaXII0M8sBngSGAp8AM51ze4Ic1wR8EHi4yTl3UWeNMeG1\n9LFpqRlo6WMDCROomRlfn1TIT19Yzcot1Zw65HADyOZmx2/fWs/Igb35QvGgmI5jzfa9ADgHU3/x\njmolRCRq+mV255yTB3LOyQMPPVfX0MTaihrWV9byyc59bNy1n407a1n66R5qDzZiBt3TUuielkr3\ntBR6pKeS3TOdfpnpDM/tRXbPdHJ7d2d4biYnDcykMKcnaalHZuP8lL1SQ+/Q4pVJux140zk3x8xu\nDzz+fpDjDjjnxnbu0LqItvrYJEiQBnDx+HzuenUNjy/6lDmXjD70/GurtrOuspbfXD6u7aXiHbSg\ndAs/fm7VoceqlRCRWMvolsro/GxG5x/Z+9E5R0OTo1uqdXj2wE/ZK7WgCi1ezWynA48Efn4EmBGn\ncXRdXaSPTVaPblw0Jo/nlm9lb10D4F2o7nlrPcP69+KCUYNj+vnhNoIUEYk1MyM9LSUq5R1+aqCu\nht6hxStIG+ic2xb4eTswMMRxGWa2xMwWmVnIQM7Mrgsct2THjh1RH2xCCtWvJgH72Fwx+QQONDTx\n7FIvwHzzw0pWb9vLtz4/gtQYZtHAX3ebIiLR4rft6GaMG8K7t5/DxjkX8O7t5yhAC4hZkGZmb5jZ\nyiBf01sf55xzePvOBnOCc64E+BrwKzMbHuwg59wDzrkS51xJbq76WAHeIoFuR90RdbCPTbyMys9i\nTH4Wj7+3ycuivb2egpweTB+bF/PP9tPdpohItCh7lRhiVpPmnDs31GtmVmFmg51z28xsMFAZ4j22\nBL5vMLOFwDjg41iMt8uJQh8bPzUX/PrkE7jtmTL+689rWLG5ip9/eRTdUmOfCFathIh0VWqg7n/m\nJbI6+UPN7gZ2tVo4kOOcu+2oY/oC+51zB82sP/B3YLpzbnVb711SUuKWLFkSs7Eni6ObC4IXnMTr\nTutAfROTfv4Ge+saGZyVwcJbz6Z7WmqnBJJ+ClZFRKRrMbOlgVnDY8Rrdecc4Ckzuwb4FJgJYGYl\nwA3OuWuBzwD3m1kz3rTsnOMFaBI9flqeDdAjPZVLP1vA3Hc3csNZww8FaJ3RpVp3myIiEg9xCdKc\nc7uAKUGeXwJcG/j5b8CoTh6aBPixYP6fzh5OZvdUvjLBa2zrt0BSREQkmuK1ulN8zo8F87m9u/PP\nXzi8j6cfA0kREZFoUZAmQflteXYwvgsky56CX54KP8n2vpc9FZ9xiIhIl6AgTYJKhOXZvgokW7bh\nqt4MuMPbcClQExGRdorL6s5Y0urOGPPZpu2+WXn5y1MDAdpRsgrguys7fzwiIpIQ/Li6UxJRBJu2\nd1bw5JuVl11kGy4REfEPTXdK+NratL2VltYYW6oO4DjcGmNB6ZbOG2tn60LbcImIiD8oSJPwhZkt\nSspNybvQNlwiIuIPCtIkfGFmi5KyNcbomfCl33g1aJj3/Uu/iWu9noiIJDbVpEn4psw+siYNgmaL\n8rJ7sCVIQNblNyUfPVNBmYiIRI0yaRK+MLNFvmqNISIikqCUSZPIhJEtallt6YvWGCIiIglKQZrE\nhG9aY4iIiCQoTXeKiIiI+JAyaeI7vtlFQEREJI4UpImvtDTCbemz1tIIF1CgJiIiSUXTneIrSdkI\nV0REJAhl0sRXkrIRrogkBZVySKQUpEmHRfPCk7SNcEWkS1Mph7SHpjulQ6K9mboa4YpIV6RSDmkP\nBWnSIdG+8MwYN4Q7Lx7FkOweGDAkuwd3XjxKd5oiktBUyiHtoelO6ZBYXHjUCFdEuhqVckh7KJMm\nHRLqAqMLj4jIYSrlkPZQkCYdoguPiMjxqZRD2kPTndIh2kxdRCQ8HSnlUPuO5KQgTTpMNWQiIrGj\nnX9SFAAABWFJREFU9h3JS9OdIiIiPqb2HclLQZqIiIiPqX1H8lKQJiIi4mNaRZ+8FKSJiIj4mFbR\nJy8tHBAREfExraJPXgrSREREfE6r6JOTpjtFREREfEhBmoiIiIgPKUgTERER8SEFaSIiIiI+FJcg\nzcwuM7NVZtZsZiVtHDfNzNaY2Xozu70zxygiIiIST/HKpK0ELgbeCXWAmaUCvwPOB4qBy82suHOG\nJyIiIhJfcWnB4Zz7EMDM2jpsIrDeObchcOwTwHRgdcwHKCIiIhJnfq5JGwJsbvW4PPCciIiISJcX\ns0yamb0BDAry0h3Oueei/FnXAdcBFBYWRvOtRUREROIiZkGac+7cDr7FFqCg1eP8wHPBPusB4AGA\nkpIS18HPFREREYk7P093LgZONLMiM0sHvgo8H+cxiYiIiHQKc67zE09m9mXgHiAXqAKWO+fOM7M8\n4EHn3BcDx30R+BWQCsx1zv0sjPfeAXwas8FLi/7AzngPQsKic5U4dK4Sh85V4vD7uTrBOZcb7IW4\nBGmS+MxsiXMuZI878Q+dq8Shc5U4dK4SRyKfKz9Pd4qIiIgkLQVpIiIiIj6kIE3a64F4D0DCpnOV\nOHSuEofOVeJI2HOlmjQRERERH1ImTURERMSHFKRJu5jZ3Wb2kZmVmdmfzCw73mOSI5nZNDNbY2br\nzez2eI9HgjOzAjN728xWm9kqM/t2vMckbTOzVDMrNbMX4z0WCc3Mss3smcDfqg/N7LR4jylSCtKk\nvV4HTnXOjQbWAj+I83ikFTNLBX4HnA8UA5ebWXF8RyUhNALfc84VA5OBG3WufO/bwIfxHoQc16+B\nV51zJwNjSMBzpiBN2sU592fnXGPg4SK8bbvEPyYC651zG5xz9cATwPQ4j0mCcM5tc84tC/xcg/eH\nZEh8RyWhmFk+cAHwYLzHIqGZWRZwJvAQgHOu3jlXFd9RRU5BmkTD1cAr8R6EHGEIsLnV43L0h9/3\nzGwoMA54L74jkTb8CrgNaI73QKRNRcAOYF5gavpBM+sV70FFSkGahGRmb5jZyiBf01sdcwfedM3v\n4zdSkcRnZpnAH4HvOOf2xns8ciwzuxCodM4tjfdY5LjSgPHAfc65ccA+IOFqc9PiPQDxL+fcuW29\nbmZXAhcCU5x6ufjNFqCg1eP8wHPiQ2bWDS9A+71z7tl4j0dCOh24KLCvdAbQx8wed85dEedxybHK\ngXLnXEtW+hkSMEhTJk3axcym4aX8L3LO7Y/3eOQYi4ETzazIzNKBrwLPx3lMEoSZGV7dzIfOuV/E\nezwSmnPuB865fOfcULz/p95SgOZPzrntwGYzGxl4agqwOo5Dahdl0qS9fgt0B173/sawyDl3Q3yH\nJC2cc41mdhPwGpAKzHXOrYrzsCS404FvAB+Y2fLAcz90zr0cxzGJdAU3A78P3KhuAK6K83giph0H\nRERERHxI050iIiIiPqQgTURERMSHFKSJiIiI+JCCNBEREREfUpAmIiIi4kMK0kREQjCzAjPbaGY5\ngcd9A4+HxndkIpIMFKSJiITgnNsM3AfMCTw1B3jAOfdJ3AYlIklDfdJERNoQ2LJpKTAXmAWMdc41\nxHdUIpIMtOOAiEgbnHMNZnYr8CrwBQVoItJZNN0pInJ85wPbgFPjPRARSR4K0kRE2mBmY4GpwGTg\nu2Y2OM5DEpEkoSBNRCQEMzO8hQPfcc5tAu4G/iu+oxKRZKEgTUQktFnAJufc64HH9wKfMbOz4jgm\nEUkSWt0pIiIi4kPKpImIiIj4kII0ERERER9SkCYiIiLiQwrSRERERHxIQZqIiIiIDylIExEREfEh\nBWkiIiIiPqQgTURERMSH/h9x941iIQTIhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting results\n",
    "y_pred_dnn_re= deep_nn_reg.predict(X)\n",
    "plotting()\n",
    "# plt.plot(X,y_pred_base) # plot regression line\n",
    "# plt.plot(X,y_pred_nn) # plot regression line\n",
    "plt.plot(X,y_pred_dnn) \n",
    "plt.plot(X,y_pred_dnn_re)\n",
    "plt.legend(['baseline model','Neural Network','Deep Neural Network','Deep Neural Network with Regularization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rlhse_gixFqY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPDsk4FixFkD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "em1WoQSCw6nS"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03-DL-NEURAL-NETWORK-AND-REGULARIZATION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
